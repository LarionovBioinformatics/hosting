[["index.html", "I forgot the title Section 1 My great Math LaTex 1.1 Rendering to PDF and EPUB 1.2 Sharing on social media", " I forgot the title Of course, its Me 2021-04-16 Section 1 My great Math LaTex This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\[{{a^2 + b^2} \\over { \\sum{y} \\iff \\int{z \\over d^2}}} \\ne c^2\\]. https://www.math.uci.edu/~xiangwen/pdf/LaTeX-Math-Symbols.pdf 1.1 Rendering to PDF and EPUB Although PDF and EPUB rendering is a default feature of bookdown, it is suppressed in this example to illustrate interactive features of Plotly 1.2 Sharing on social media I do not like these media ! I kept it here just to illustrate this feature of bookdown "],["r-markdown.html", "Section 2 R-Markdown 2.1 2nd level header 2.2 Not only R … 2.3 Back to R", " Section 2 R-Markdown 2.1 2nd level header 2.1.1 3rd level header etc 2.2 Not only R … 2.2.1 Python Just use chunk with option “python” Requires Reticulate and Python (of course :) DOES NOT WORK with bookdown (and gives a very obscure error mesage !) print(&quot;hello world&quot;) a=5 print(a) 2.2.2 Bash Just use chunk with option “bash” echo BlaBlaBla ## BlaBlaBla 2.3 Back to R 2.3.1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: data(&quot;cars&quot;) summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 2.3.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["a-simple-analysis.html", "Section 3 A simple analysis 3.1 In-built datasets 3.2 Explore data 3.3 T-test 3.4 Liear model", " Section 3 A simple analysis library(ggplot2) 3.1 In-built datasets # Inbuilt dataset data(&quot;mtcars&quot;) # Explore data class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 dim(mtcars) ## [1] 32 11 3.2 Explore data mpg vs hp cosiderig vs as covariate 3.2.1 Base R plottig plot(mpg~hp, data=mtcars, pch=vs, main=&quot;My title&quot;) legend(&quot;topright&quot;, c(&quot;vs=0&quot;,&quot;vs=1&quot;), pch=c(0,1)) 3.2.2 ggplot style ggplot(data=mtcars,aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point() + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.3 T-test plot(mpg~vs, data=mtcars) boxplot(mpg~vs, data=mtcars) t.test(mpg~vs, data=mtcars) ## ## Welch Two Sample t-test ## ## data: mpg by vs ## t = -4.6671, df = 22.716, p-value = 0.0001098 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.462508 -4.418445 ## sample estimates: ## mean in group 0 mean in group 1 ## 16.61667 24.55714 3.4 Liear model 3.4.1 mpg ~ hp my_model_1 &lt;- lm(mpg~hp, data=mtcars) class(my_model_1) ## [1] &quot;lm&quot; summary(my_model_1) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 plot(mpg~hp, data=mtcars) abline(my_model_1) plot(my_model_1) 3.4.2 mpg ~ vs + hp my_model_2 &lt;- lm(mpg ~ vs + hp, data=mtcars) summary(my_model_2) ## ## Call: ## lm(formula = mpg ~ vs + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7131 -2.3336 -0.1332 1.9055 7.9055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.96300 2.89069 9.328 3.13e-10 *** ## vs 2.57622 1.96966 1.308 0.201163 ## hp -0.05453 0.01448 -3.766 0.000752 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.818 on 29 degrees of freedom ## Multiple R-squared: 0.6246, Adjusted R-squared: 0.5987 ## F-statistic: 24.12 on 2 and 29 DF, p-value: 6.768e-07 plot(my_model_2) 3.4.3 mpg ~ vs * hp my_model_3 &lt;- lm(mpg ~ vs * hp, data=mtcars) summary(my_model_3) ## ## Call: ## lm(formula = mpg ~ vs * hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5821 -1.7710 -0.3612 1.5969 9.2646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.49637 2.73893 8.944 1.07e-09 *** ## vs 14.50418 4.58160 3.166 0.00371 ** ## hp -0.04153 0.01379 -3.011 0.00547 ** ## vs:hp -0.11657 0.04130 -2.822 0.00868 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.428 on 28 degrees of freedom ## Multiple R-squared: 0.7077, Adjusted R-squared: 0.6764 ## F-statistic: 22.6 on 3 and 28 DF, p-value: 1.227e-07 plot(my_model_3) "],["data-types.html", "Section 4 Data types 4.1 Vectors 4.2 Numeric matrices 4.3 Recycling in element-wise operations 4.4 Factors: categorical variale 4.5 Lists 4.6 Dataframe: list of vectors/factors", " Section 4 Data types Be careful with changing folders in bookdown !! setwd(&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021&quot;) 4.1 Vectors 4.1.1 Numeric vectors a &lt;- 1:5 class(a) ## [1] &quot;integer&quot; a ## [1] 1 2 3 4 5 b &lt;- seq(from=10, to=50, by=10) class(b) ## [1] &quot;numeric&quot; b ## [1] 10 20 30 40 50 # Vectorised operation: element-wise summation a + b ## [1] 11 22 33 44 55 4.1.2 Vector elements may have names Names is a property attributes(a) ## NULL names(a) &lt;- c(&quot;e1&quot;,&quot;e2&quot;,&quot;e3&quot;,&quot;e4&quot;,&quot;e5&quot;) a ## e1 e2 e3 e4 e5 ## 1 2 3 4 5 attributes(a) ## $names ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; a[c(1,2,4)] ## e1 e2 e4 ## 1 2 4 a[c(T,T,F,T,F)] ## e1 e2 e4 ## 1 2 4 a[c(&quot;e1&quot;,&quot;e2&quot;,&quot;e4&quot;)] ## e1 e2 e4 ## 1 2 4 4.2 Numeric matrices m1 &lt;- matrix(1:25, nrow=5) m1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 m2 &lt;- matrix(rep(10,25), nrow=5) m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 10 10 10 10 ## [2,] 10 10 10 10 10 ## [3,] 10 10 10 10 10 ## [4,] 10 10 10 10 10 ## [5,] 10 10 10 10 10 t(m1) # Transposition ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 ## [5,] 21 22 23 24 25 m1 + m2 # Element-wise summation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 16 21 26 31 ## [2,] 12 17 22 27 32 ## [3,] 13 18 23 28 33 ## [4,] 14 19 24 29 34 ## [5,] 15 20 25 30 35 m1 * m2 # Element-wise multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 60 110 160 210 ## [2,] 20 70 120 170 220 ## [3,] 30 80 130 180 230 ## [4,] 40 90 140 190 240 ## [5,] 50 100 150 200 250 m1 %*% m2 # Matrix multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 550 550 550 550 550 ## [2,] 600 600 600 600 600 ## [3,] 650 650 650 650 650 ## [4,] 700 700 700 700 700 ## [5,] 750 750 750 750 750 4.2.1 Matrices may have rownames and colnames rownames(m1) &lt;- c(&quot;s1&quot;,&quot;s2&quot;,&quot;s3&quot;,&quot;s4&quot;,&quot;s5&quot;) colnames(m1) &lt;- c(&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;g5&quot;) m1 ## g1 g2 g3 g4 g5 ## s1 1 6 11 16 21 ## s2 2 7 12 17 22 ## s3 3 8 13 18 23 ## s4 4 9 14 19 24 ## s5 5 10 15 20 25 m1[c(&quot;s2&quot;,&quot;s3&quot;),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 m1[c(F,T,T,F,F),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 4.3 Recycling in element-wise operations Shorter vector is recycled along the longer oject Matrices are processed as vectors in column-after-column way c &lt;- 1:3 c ## [1] 1 2 3 b + c ## Warning in b + c: longer object length is not a multiple of shorter object ## length ## [1] 11 22 33 41 52 c + m2 ## Warning in c + m2: longer object length is not a multiple of shorter object ## length ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 13 12 11 13 ## [2,] 12 11 13 12 11 ## [3,] 13 12 11 13 12 ## [4,] 11 13 12 11 13 ## [5,] 12 11 13 12 11 a + m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 m2 + a ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 rm(a,b,c,m1,m2) 4.4 Factors: categorical variale Categorical != character v &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) class(v) ## [1] &quot;character&quot; v ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; f &lt;- factor(v) class(f) ## [1] &quot;factor&quot; f ## [1] A B C ## Levels: A B C f &lt;- factor(v, levels=c(&quot;C&quot;,&quot;B&quot;,&quot;A&quot;)) f ## [1] A B C ## Levels: C B A f &lt;- factor(v, levels=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;)) f ## [1] &lt;NA&gt; B C ## Levels: B C D f &lt;- factor(v, levels=LETTERS) f ## [1] A B C ## Levels: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z f &lt;- factor(v, levels=LETTERS, ordered = T) f ## [1] A B C ## 26 Levels: A &lt; B &lt; C &lt; D &lt; E &lt; F &lt; G &lt; H &lt; I &lt; J &lt; K &lt; L &lt; M &lt; N &lt; O &lt; ... &lt; Z # Potential confusion factor(1:5, levels=c(4,5,3,2,1), ordered=T) ## [1] 1 2 3 4 5 ## Levels: 4 &lt; 5 &lt; 3 &lt; 2 &lt; 1 rm(v,f) 4.5 Lists Collection of ojects of different types my.list &lt;- list(c(1,2),&quot;A&quot;,T ) my.list ## [[1]] ## [1] 1 2 ## ## [[2]] ## [1] &quot;A&quot; ## ## [[3]] ## [1] TRUE my.list[1] ## [[1]] ## [1] 1 2 my.list[[1]][1] ## [1] 1 my.list[[1]][2] ## [1] 2 my.list[2][1] ## [[1]] ## [1] &quot;A&quot; my.list &lt;- list(lel1=c(1,2), lel2=&quot;A&quot;, lel3=T ) my.list ## $lel1 ## [1] 1 2 ## ## $lel2 ## [1] &quot;A&quot; ## ## $lel3 ## [1] TRUE my.list$lel1 ## [1] 1 2 my.list$lel1[2] ## [1] 2 rm(my.list) 4.6 Dataframe: list of vectors/factors #data(&quot;mtcars&quot;) class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 #data(&quot;mpg&quot;) #class(mpg) #str(mpg) my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA)) class(my.df) ## [1] &quot;data.frame&quot; my.df ## first_name second_name age income ## 1 AL Smith 15 100 ## 2 AL Larionov 23 200 ## 3 BS Bob 1 NA str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: Factor w/ 3 levels &quot;Bob&quot;,&quot;Larionov&quot;,..: 3 2 1 ## $ age : num 15 23 1 ## $ income : num 100 200 NA my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA), stringsAsFactors=F) str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: chr &quot;Smith&quot; &quot;Larionov&quot; &quot;Bob&quot; ## $ age : num 15 23 1 ## $ income : num 100 200 NA "],["plotly.html", "Section 5 Plotly 5.1 Reference 5.2 Load library 5.3 Get some data 5.4 Plotly magic", " Section 5 Plotly 5.1 Reference https://plotly-r.com/index.html 5.2 Load library library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 5.3 Get some data # Inbuilt dataset #data(&quot;mtcars&quot;) mtcars &lt;- data.frame(mtcars, make=rownames(mtcars), stringsAsFactors = F) # Explore data dim(mtcars) ## [1] 32 12 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 12 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... ## $ make: chr &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... 5.4 Plotly magic d &lt;- ggplot(data=mtcars, aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point(aes(text=make)) + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## Warning: Ignoring unknown aesthetics: text d ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplotly(d, tooltip=&quot;text&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # A base R plot ... plot(mpg~hp, data=mtcars) "],["distributionns.html", "Section 6 Distributionns 6.1 Function curve() 6.2 Normal distribution 6.3 Gamma 6.4 Beta", " Section 6 Distributionns 6.1 Function curve() curve(sin(x),0,2*pi) curve(cos(x),from=0,to=2*pi,add=T,col=&quot;red&quot;,lty=2) 6.2 Normal distribution dnorm(-1.96, mean=0, sd=1) ## [1] 0.05844094 dnorm(-1.96, mean=10, sd=7) ## [1] 0.01324074 curve(dnorm(x),-3,3) curve(dnorm(x, 10, 3),0,20) curve(pnorm(x, 10, 3),0,20) curve(dnorm(x),-4,4) abline(v=qnorm(0.025), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.025, lower.tail = F), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.005), col=&quot;blue&quot;, lty=2) abline(v=qnorm(0.005, lower.tail = F), col=&quot;blue&quot;, lty=2) curve(qnorm(x)) rnorm(5) ## [1] -1.307595 1.618792 -1.518170 -1.025914 -0.648565 hist(rnorm(1000),freq = F,ylim=c(0,0.5)) curve(dnorm(x),-4,4,add=T,col=&quot;blue&quot;) dh &lt;- c(rnorm(1000,75,50),rnorm(1000,180,15)) hist(dh,100,200) 6.3 Gamma curve(dgamma(x,2,6)) curve(dgamma(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;g(2,6)&quot;,&quot;g(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dgamma(x,3,7),0,2) abline(v=qgamma(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qgamma(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rgamma(1000,3,7),freq = F, add=T, border=&quot;green&quot;) 6.4 Beta curve(dbeta(x,2,6)) curve(dbeta(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;b(2,6)&quot;,&quot;b(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dbeta(x,3,7)) abline(v=qbeta(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qbeta(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rbeta(1000,3,7),freq = F, add=T, border=&quot;green&quot;) "],["package.html", "Section 7 Package 7.1 User-defined fnction 7.2 Recursive functions are allowed 7.3 Lets add some data 7.4 Prepare template for package source folder", " Section 7 Package 7.1 User-defined fnction mff &lt;- function(a,b){ return(a+b) } mff(3,5) ## [1] 8 #mff(3,&quot;Good&quot;) #mff(&quot;a&quot;,&quot;b&quot;) 7.2 Recursive functions are allowed my_sum &lt;- function(n){ result &lt;- 0 for(i in 1:n){ result &lt;- result + i } return(result) } my_sum(10) ## [1] 55 my_rsum &lt;- function(n){ if(n&gt;1){ return(n+my_rsum(n-1)) } if(n==1){ return(n) } } my_rsum(10) ## [1] 55 7.3 Lets add some data my_great_dataset &lt;- data.frame(names=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), values=c(1,2,3,4)) 7.4 Prepare template for package source folder ls() getwd() package.skeleton(name=&quot;ab&quot;,list=ls()) A bush chunk to build: cd /Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021 R CMD build ab R CMD check ab_1.0.tar.gz Follow the instructions for minimal required documentation, i.e.: Titles (&amp; Descriptions?) in each object Rd Examples (in the package Rd) Then install, load and enjoy ! #install.packages(...) library(ab) data(my_great_dataset) "],["data-reading-writing.html", "Section 8 Data reading / writing 8.1 A simple simulated dataset 8.2 Saving and reading", " Section 8 Data reading / writing library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 8.1 A simple simulated dataset x &lt;- runif(1000, 0, 100) hist(x) f &lt;- factor(c(rep(&quot;A&quot;,500),rep(&quot;B&quot;,500))) head(f) ## [1] A A A A A A ## Levels: A B tail(f) ## [1] B B B B B B ## Levels: A B y1 &lt;- 0.5 * x[1:500] + rnorm(500,0,25) y2 &lt;- -0.5 * x[501:1000] + rnorm(500,0,25) y &lt;- c(y1,y2+50) sim_data.df &lt;- data.frame(y,x,f) str(sim_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -2.6 -19.1 40.7 14.1 30.6 ... ## $ x: num 48.5 20.2 47.5 44.3 60.1 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... plot(y~x, pch=as.integer(f)) ggplot(data=sim_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 8.2 Saving and reading … at last ! 8.2.1 RData save.image(file=&quot;results1.RData&quot;) rm(list=ls()) load(&quot;results1.RData&quot;) save(&quot;sim_data.df&quot;,file=&quot;results2.RData&quot;) rm(list=ls()) load(&quot;results2.RData&quot;) 8.2.2 Text files write.table(sim_data.df,file=&quot;results.tsv&quot;, quote=F,sep=&quot;\\t&quot;,row.names = F) rm(list=ls()) my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -2.6 -19.1 40.7 14.1 30.6 ... ## $ x: num 48.5 20.2 47.5 44.3 60.1 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-76.493 Min. : 0.06282 A:500 ## 1st Qu.: 6.056 1st Qu.:27.93117 B:500 ## Median : 25.692 Median :53.83435 ## Mean : 24.937 Mean :52.26297 ## 3rd Qu.: 44.360 3rd Qu.:77.64114 ## Max. :118.265 Max. :99.64583 "],["data-exploring-modifying.html", "Section 9 Data exploring / modifying 9.1 Read data 9.2 Apply (summary) functions over margin 9.3 Missed values 9.4 Sorting, filtering and summarising 9.5 Dplyr", " Section 9 Data exploring / modifying library(ggplot2) library(dplyr) 9.1 Read data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -2.6 -19.1 40.7 14.1 30.6 ... ## $ x: num 48.5 20.2 47.5 44.3 60.1 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-76.493 Min. : 0.06282 A:500 ## 1st Qu.: 6.056 1st Qu.:27.93117 B:500 ## Median : 25.692 Median :53.83435 ## Mean : 24.937 Mean :52.26297 ## 3rd Qu.: 44.360 3rd Qu.:77.64114 ## Max. :118.265 Max. :99.64583 9.2 Apply (summary) functions over margin This is base R data(&quot;mtcars&quot;) dim(mtcars) ## [1] 32 11 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... apply(mtcars,2,mean) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 apply(my_data.df[,c(1,2)],2,mean) ## y x ## 24.93726 52.26297 9.3 Missed values Missed values could be NA or NULL There are special values like Inf and -Inf (e.g. produced by division by zero) that are “Not a Number” or NaN NaN is NOT NA The functions to chack such values include is.na, is.nan, is.finite etc x &lt;- c(1,2,5,NA,NA) mean(x,na.rm=T) ## [1] 2.666667 sum(x,na.rm=T) ## [1] 8 is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE rm(x) # Division by zero is NOT NA 1/0 ## [1] Inf 9.4 Sorting, filtering and summarising 9.4.1 Base R Base R can do everything that dplyr does, but sometime the dplyr does is slightly easier. # Count/Tabulate table(my_data.df$f,useNA=&quot;always&quot;) ## ## A B &lt;NA&gt; ## 500 500 0 # Filtering my_data.df[my_data.df$x &gt;99,] ## y x f ## 50 87.25953 99.02441 A ## 220 36.02797 99.03070 A ## 414 63.36380 99.30329 A ## 660 -42.05427 99.20795 B ## 792 23.33271 99.64583 B # Ordering != sorting sort(my_data.df$x)[1:5] ## [1] 0.06281936 0.09801767 0.34217769 0.36712326 0.45816218 order(my_data.df$x)[1:5] ## [1] 195 248 695 511 897 head(my_data.df[order(my_data.df$x),]) ## y x f ## 195 -34.624382 0.06281936 A ## 248 7.726435 0.09801767 A ## 695 58.616329 0.34217769 B ## 511 8.220187 0.36712326 B ## 897 118.265019 0.45816218 B ## 388 9.517190 0.46599540 A 9.4.2 Tables to tests In base R contingency tables are naturally connected to tests table(mtcars$vs) ## ## 0 1 ## 18 14 table(mtcars$am) ## ## 0 1 ## 19 13 table(mtcars$vs,mtcars$am) ## ## 0 1 ## 0 12 6 ## 1 7 7 fisher.test(table(mtcars$vs,mtcars$am)) # Chi Square etc ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(mtcars$vs, mtcars$am) ## p-value = 0.4727 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3825342 10.5916087 ## sample estimates: ## odds ratio ## 1.956055 x.mx &lt;- matrix(c(37,11,14,24),nrow=2) x.mx ## [,1] [,2] ## [1,] 37 14 ## [2,] 11 24 fisher.test(x.mx) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x.mx ## p-value = 0.0003348 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.047814 16.532095 ## sample estimates: ## odds ratio ## 5.63382 #table(mtcars$vs,mtcars$am,mtcars$gear) 9.5 Dplyr 9.5.1 Tidy Data concept Dplyr, ggplot and some other packages are part of the wider concept of Tidy Data : https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html I personally do not like this concept because I consider it as an additional unnecessary layer above what is already available in base R. On the other hand, I cant deny the convenience of Dplyr and ggplot. In any way, it is not possible to ignore the Tidy Data packages because they became so ubiquitous nowadays. 9.5.2 Traps in TidyData One of the stupidest thing in Tidy Data is that it refuses using Row Names, considering them somehow “untidy.” This means that many dplyr functions just silently drop the row names. Why on Earth row names are less tidy than column names?? Another possible trap is that dplyr functions may change the row order. So, the user needs either explicitly arrange the rows all the time, or, at least, pay attention to the row order when using dplyr. 9.5.3 Dplyr is BIG Here we just mention some functions to illustrate how dplyr may be used. In fact it is MUCH bigger than these small examples. See the cheat-sheet, for the beginning: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf 9.5.4 Piping Piping is a technique borrowed by dplyr from magrittr package: https://uc-r.github.io/pipe https://cran.r-project.org/web/packages/magrittr/index.html Key combination shift + ctr + M x %&gt;% f(y,z,...) is equivalent to f(x,y,z,...) 9.5.5 Dplyr examples # Summarising my_data.df %&gt;% group_by(f) %&gt;% summarise(count=n(), x_mean=mean(x), y_mean=mean(y)) ## # A tibble: 2 x 4 ## f count x_mean y_mean ## * &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 500 53.7 26.7 ## 2 B 500 50.9 23.1 # Sorting my_data.df %&gt;% arrange(x) %&gt;% head ## y x f ## 1 -34.624382 0.06281936 A ## 2 7.726435 0.09801767 A ## 3 58.616329 0.34217769 B ## 4 8.220187 0.36712326 B ## 5 118.265019 0.45816218 B ## 6 9.517190 0.46599540 A # Filtering rows my_data.df %&gt;% filter(x&gt;75) %&gt;% arrange(x) %&gt;% head ## y x f ## 1 6.504354 75.13432 A ## 2 -20.888095 75.14072 B ## 3 61.317872 75.16275 A ## 4 31.999576 75.37514 A ## 5 41.422868 75.39647 A ## 6 72.672175 75.94906 A # Selecting columns my_data.df %&gt;% select(y,x) %&gt;% head ## y x ## 1 -2.601017 48.45296 ## 2 -19.080882 20.17205 ## 3 40.684370 47.51981 ## 4 14.132072 44.33171 ## 5 30.603842 60.14332 ## 6 39.805518 94.58283 # Piping can be done outside of the dplyr too mtcars %&gt;% head ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 "],["regression.html", "Section 10 Regression 10.1 Note 10.2 Load data 10.3 Explore data 10.4 Impute if needed 10.5 Linear modelling 10.6 lm could be evaluated by ANOVA 10.7 Predict 10.8 Update 10.9 Crooked things yet to explore …", " Section 10 Regression 10.1 Note Because of the shoterned time we only discussed examples with lm(); interface to glm() works in a very similar way - look in the references that I e-mailed to you earlier. 10.2 Load data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) 10.3 Explore data dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -2.6 -19.1 40.7 14.1 30.6 ... ## $ x: num 48.5 20.2 47.5 44.3 60.1 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; head(my_data.df) ## y x f ## 1 -2.601017 48.45296 A ## 2 -19.080882 20.17205 A ## 3 40.684370 47.51981 A ## 4 14.132072 44.33171 A ## 5 30.603842 60.14332 A ## 6 39.805518 94.58283 A summary(my_data.df) ## y x f ## Min. :-76.493 Min. : 0.06282 A:500 ## 1st Qu.: 6.056 1st Qu.:27.93117 B:500 ## Median : 25.692 Median :53.83435 ## Mean : 24.937 Mean :52.26297 ## 3rd Qu.: 44.360 3rd Qu.:77.64114 ## Max. :118.265 Max. :99.64583 any(is.na(my_data.df)) ## [1] FALSE ggplot(data=my_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 10.4 Impute if needed x &lt;- c(1,2,5,NA,NA) is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE mean_x &lt;- mean(x, na.rm = T) mean_x -&gt; x[is.na(x)] x ## [1] 1.000000 2.000000 5.000000 2.666667 2.666667 rm(x) 10.5 Linear modelling 10.5.1 Single term model_1 &lt;- lm(y~x, data=my_data.df) summary(model_1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -101.476 -19.094 0.934 19.528 94.194 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.06291 1.88081 12.794 &lt;2e-16 *** ## x 0.01673 0.03163 0.529 0.597 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.36 on 998 degrees of freedom ## Multiple R-squared: 0.0002802, Adjusted R-squared: -0.0007215 ## F-statistic: 0.2797 on 1 and 998 DF, p-value: 0.597 plot(model_1) plot(y~x,data=my_data.df) abline(model_1, col=&quot;red&quot;) 10.5.2 Multiple terms without interaction model_2 &lt;- lm(y~x+f, data=my_data.df) #equivalent: note using dot for ALL terms model_2_dot &lt;- lm(y~., data=my_data.df) summary(model_2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.252 -18.959 0.964 19.444 95.818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.01050 2.11764 12.283 &lt;2e-16 *** ## x 0.01361 0.03162 0.431 0.6669 ## fB -3.56959 1.79341 -1.990 0.0468 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.32 on 997 degrees of freedom ## Multiple R-squared: 0.004237, Adjusted R-squared: 0.002239 ## F-statistic: 2.121 on 2 and 997 DF, p-value: 0.1204 summary(model_2_dot) ## ## Call: ## lm(formula = y ~ ., data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.252 -18.959 0.964 19.444 95.818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.01050 2.11764 12.283 &lt;2e-16 *** ## x 0.01361 0.03162 0.431 0.6669 ## fB -3.56959 1.79341 -1.990 0.0468 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.32 on 997 degrees of freedom ## Multiple R-squared: 0.004237, Adjusted R-squared: 0.002239 ## F-statistic: 2.121 on 2 and 997 DF, p-value: 0.1204 plot(model_2) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_2, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_2, col = &quot;blue&quot;, lwd = 3): only using the first two of 3 ## regression coefficients 10.5.3 With interaction model_3 &lt;- lm(y~x*f, data=my_data.df) summary(model_3) ## ## Call: ## lm(formula = y ~ x * f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.904 -15.541 -0.708 15.961 73.907 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.70302 2.40343 -0.293 0.77 ## x 0.51139 0.03973 12.870 &lt;2e-16 *** ## fB 47.02536 3.29519 14.271 &lt;2e-16 *** ## x:fB -0.96733 0.05539 -17.464 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24.79 on 996 degrees of freedom ## Multiple R-squared: 0.2377, Adjusted R-squared: 0.2354 ## F-statistic: 103.5 on 3 and 996 DF, p-value: &lt; 2.2e-16 plot(model_3) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_3, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_3, col = &quot;blue&quot;, lwd = 3): only using the first two of 4 ## regression coefficients 10.6 lm could be evaluated by ANOVA https://stats.stackexchange.com/questions/115304/interpreting-output-from-anova-when-using-lm-as-input Anova output may be more clear when categorical variables have more than 3 levels (we discussed it during the practical session) Note that there are at least two slightly different functions for ANOVA in R: anova() and aov() anova(model_1) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 225 225.04 0.2797 0.597 ## Residuals 998 802885 804.49 anova(model_2) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 225 225.0 0.2806 0.59645 ## f 1 3178 3177.7 3.9616 0.04682 * ## Residuals 997 799707 802.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model_3) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 225 225 0.3661 0.5453 ## f 1 3178 3178 5.1695 0.0232 * ## x:f 1 187470 187470 304.9811 &lt;2e-16 *** ## Residuals 996 612236 615 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.1 Anova could be used to compare models This can be used in the following way: First a “null” model could be calculated with all the “confounders”/“covariates” Then a predictor of interest is added to the model and models are compared by ANOVA If the model is improved significantly, then the predictor is important anova(model_1,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 802885 ## 2 996 612236 2 190648 155.08 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model_1,model_2,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x + f ## Model 3: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 802885 ## 2 997 799707 1 3178 5.1695 0.0232 * ## 3 996 612236 1 187470 304.9811 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.2 A bit of OOP about anova(lm) # What other methods could be applied to lm object? class(model_1) ## [1] &quot;lm&quot; methods(class=&quot;lm&quot;) ## [1] add1 alias anova case.names coerce ## [6] confint cooks.distance deviance dfbeta dfbetas ## [11] drop1 dummy.coef effects extractAIC family ## [16] formula fortify hatvalues influence initialize ## [21] kappa labels logLik model.frame model.matrix ## [26] nobs plot predict print proj ## [31] qqnorm qr residuals rstandard rstudent ## [36] show simulate slotsFromS3 summary variable.names ## [41] vcov ## see &#39;?methods&#39; for accessing help and source code # What other classes coul be evaluated by ANOVA? methods(anova) ## [1] anova.gam* anova.glm* anova.glmlist* anova.gls* anova.lm* ## [6] anova.lme* anova.lmlist* anova.loess* anova.mlm* anova.nls* ## see &#39;?methods&#39; for accessing help and source code #?aov #?anova #?anova.lm 10.7 Predict new_data.df &lt;- data.frame(x=c(10,20), f=factor(c(&quot;A&quot;,&quot;B&quot;))) new_data.df ## x f ## 1 10 A ## 2 20 B predict(model_1,new_data.df) ## 1 2 ## 24.23021 24.39751 predict(model_2,new_data.df) ## 1 2 ## 26.14665 22.71321 10.8 Update It is very rarely used, but occasionally could be seen in help and tutorials Note using dots on the left and right of ~ in the formula m1 = lm(y~x, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -101.476 -19.094 0.934 19.528 94.194 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.06291 1.88081 12.794 &lt;2e-16 *** ## x 0.01673 0.03163 0.529 0.597 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.36 on 998 degrees of freedom ## Multiple R-squared: 0.0002802, Adjusted R-squared: -0.0007215 ## F-statistic: 0.2797 on 1 and 998 DF, p-value: 0.597 m2 = update(m1,.~.+f, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.252 -18.959 0.964 19.444 95.818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.01050 2.11764 12.283 &lt;2e-16 *** ## x 0.01361 0.03162 0.431 0.6669 ## fB -3.56959 1.79341 -1.990 0.0468 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.32 on 997 degrees of freedom ## Multiple R-squared: 0.004237, Adjusted R-squared: 0.002239 ## F-statistic: 2.121 on 2 and 997 DF, p-value: 0.1204 10.9 Crooked things yet to explore … We could not discuss these because we were short of time (and I have limited understanding of them ?) 10.9.1 Intercept, design matrix See my PowerPoint slides for students with biological background Intercept could be set to zero by two ways: y ~ 0 + x y ~ x - 1 Something strange happens without intercept in the model. This something could be undertood by staring at “design matrix” m1 &lt;- lm(y~x+f, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.252 -18.959 0.964 19.444 95.818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.01050 2.11764 12.283 &lt;2e-16 *** ## x 0.01361 0.03162 0.431 0.6669 ## fB -3.56959 1.79341 -1.990 0.0468 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.32 on 997 degrees of freedom ## Multiple R-squared: 0.004237, Adjusted R-squared: 0.002239 ## F-statistic: 2.121 on 2 and 997 DF, p-value: 0.1204 m2 &lt;- lm(y~x+f-1, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f - 1, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.252 -18.959 0.964 19.444 95.818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.01361 0.03162 0.431 0.667 ## fA 26.01050 2.11764 12.283 &lt;2e-16 *** ## fB 22.44091 2.04721 10.962 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.32 on 997 degrees of freedom ## Multiple R-squared: 0.4388, Adjusted R-squared: 0.4371 ## F-statistic: 259.8 on 3 and 997 DF, p-value: &lt; 2.2e-16 m3 &lt;- lm(y~0+x+f, data=my_data.df) summary(m3) ## ## Call: ## lm(formula = y ~ 0 + x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.252 -18.959 0.964 19.444 95.818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.01361 0.03162 0.431 0.667 ## fA 26.01050 2.11764 12.283 &lt;2e-16 *** ## fB 22.44091 2.04721 10.962 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.32 on 997 degrees of freedom ## Multiple R-squared: 0.4388, Adjusted R-squared: 0.4371 ## F-statistic: 259.8 on 3 and 997 DF, p-value: &lt; 2.2e-16 data(&quot;mtcars&quot;) other_data &lt;- mtcars[1:10,c(&quot;vs&quot;,&quot;am&quot;,&quot;gear&quot;)] other_data ## vs am gear ## Mazda RX4 0 1 4 ## Mazda RX4 Wag 0 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 0 0 3 ## Valiant 1 0 3 ## Duster 360 0 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 model.matrix(~am+gear, data=other_data) ## (Intercept) am gear ## Mazda RX4 1 1 4 ## Mazda RX4 Wag 1 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 1 0 3 ## Valiant 1 0 3 ## Duster 360 1 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 model.matrix(~0+am+gear, data=other_data) ## am gear ## Mazda RX4 1 4 ## Mazda RX4 Wag 1 4 ## Datsun 710 1 4 ## Hornet 4 Drive 0 3 ## Hornet Sportabout 0 3 ## Valiant 0 3 ## Duster 360 0 3 ## Merc 240D 0 4 ## Merc 230 0 4 ## Merc 280 0 4 ## attr(,&quot;assign&quot;) ## [1] 1 2 10.9.2 Contrasts Also, there is some cryptic way allowing to specify what sub-groups to include/exclude in the comparison: this is something to do with “contrasts” https://rcompanion.org/rcompanion/h_01.html https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r contrasts(my_data.df$f) ## B ## A 0 ## B 1 contrasts(as.factor(other_data$gear)) ## 4 ## 3 0 ## 4 1 10.9.3 Type I, II and III sums ANOVA and linear models are, in fact equivalent to each other (even at the level of implementation). Both rely on calculating some sum of squares. There are 3 ways of calculating these sums: Type I, Type II and Type III Google about these … https://rcompanion.org/rcompanion/d_04.html "],["real-life-lm-example.html", "Section 11 Real life lm() example 11.1 Summary 11.2 Start section 11.3 Read data 11.4 Explore and prepare data 11.5 Explore in a univariate manner 11.6 Model with all variables 11.7 Drop1 11.8 Optimal model 11.9 Predict", " Section 11 Real life lm() example 11.1 Summary This is only the first orientation with the dataset. This example: Reads data Explores missingnes and does simplistic imputation Looks at univariate regression/anova for each variable: all look highly significant Multivariate model with all variables: suggests dropping owners drop1 suggests keeping only make, engine.size and mileage Importantly, the example does NOT analyze the normality of residuals in the models etc. It does NOT considers interactions. Further directions may also include: transformation of variables using generalized models using add1 estimating accuracy of prediction etc. 11.2 Start section 11.3 Read data data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) rm(data_file) 11.4 Explore and prepare data Data clean-up and preparation is a separate large realm in data science. Here we just check that data look well structured, check the data size, types of variables, missingness. 11.4.1 Overall size and structure dim(data.df) ## [1] 3780 6 str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... 11.4.2 Missingness count.na.udf &lt;- function(x)(sum(is.na(x))) count_na &lt;- apply(data.df,2,count.na.udf) count_na ## mileage manufactured.year engine.size owners ## 0 0 2 1968 ## price make ## 0 0 count_na/nrow(data.df) ## mileage manufactured.year engine.size owners ## 0.0000000000 0.0000000000 0.0005291005 0.5206349206 ## price make ## 0.0000000000 0.0000000000 rm(count.na.udf,count_na) 11.4.3 Impute missed Its a simplistic procedure, with its pros and contras. Strictly speaking, it may not be necessary if lm() has its own imputation in place. However, it may not be imediately clear what exactly this default implicit procedure is … So, it may be useful to deal with missed data explicitly to be sure how they were treated. On a technical point: I recollect that there could be a specialized function for applying mean in place of missed data. mean_engine_size &lt;- mean(data.df$engine.size, na.rm=T) mean_engine_size -&gt; data.df$engine.size[is.na(data.df$engine.size)] mean_owners &lt;- mean(data.df$owners, na.rm=T) mean_owners -&gt; data.df$owners[is.na(data.df$owners)] any(is.na(data.df)) ## [1] FALSE rm(mean_owners, mean_engine_size) 11.4.4 Calculate age Our previous knowledge in the field suggests that age may be a better representation for manufactured.year Should manufactured.year be removed from analysis after making age? May including both variables confuse the model (because of collinearuty). data.df &lt;- data.frame(data.df,age=2021-data.df$manufactured.year) str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 7 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : num 2.46 3 6 2.46 1 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... ## $ age : num 6 9 20 11 8 17 18 9 8 9 ... 11.5 Explore in a univariate manner 11.5.1 Anova for categorical variable makes data.df %&gt;% group_by(make) %&gt;% summarise(count=n(), mean_price=mean(price), mean_mileage=mean(mileage), mean_age=mean(age), mean_price=mean(price), mean_owners=mean(owners), mean_engine=mean(engine.size)) %&gt;% arrange(desc(count)) ## # A tibble: 43 x 7 ## make count mean_price mean_mileage mean_age mean_owners mean_engine ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vauxhall 715 6188. 44018. 6.09 2.36 1.42 ## 2 Ford 286 6502 52934. 7.21 2.46 1.34 ## 3 Nissan 269 8673. 39307. 5.90 2.31 1.33 ## 4 Hyundai 250 8521 31668. 4.35 2.44 1.30 ## 5 Volkswagen 208 5318. 74174. 10.4 3.03 1.51 ## 6 Toyota 199 6305. 48149. 8.34 2.10 1.55 ## 7 Peugeot 193 6488. 43657. 6.38 2.41 1.39 ## 8 Citroen 165 4882. 53608. 8.18 2.40 1.44 ## 9 Renault 164 6506. 41006. 6.68 2.37 1.34 ## 10 SEAT 156 6342. 53444. 6.63 2.25 1.40 ## # … with 33 more rows boxplot(price~make, data=data.df) summary(aov(price~make, data=data.df)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## make 42 1.171e+10 278725988 21.38 &lt;2e-16 *** ## Residuals 3737 4.871e+10 13035655 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.5.2 Univariate lm() for numeric variables There is a way of doing this in one step … 11.5.2.1 mileage mileage.lm &lt;- lm(price~mileage, data=data.df) summary(mileage.lm) ## ## Call: ## lm(formula = price ~ mileage, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6878.8 -1971.3 -661.8 1175.8 29729.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.780e+03 9.264e+01 105.57 &lt;2e-16 *** ## mileage -5.909e-02 1.469e-03 -40.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3346 on 3778 degrees of freedom ## Multiple R-squared: 0.2999, Adjusted R-squared: 0.2997 ## F-statistic: 1618 on 1 and 3778 DF, p-value: &lt; 2.2e-16 mileage.lm.p=coefficients(summary(mileage.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~mileage, data=data.df, main=paste(&quot;Univariate mileage p =&quot;,round(mileage.lm.p,5))) abline(mileage.lm, col=&quot;red&quot;, lwd=3) rm(mileage.lm,mileage.lm.p) 11.5.2.2 age age.lm &lt;- lm(price~age, data=data.df) summary(age.lm) ## ## Call: ## lm(formula = price ~ age, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4784 -1868 -717 898 36767 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11038.23 95.84 115.18 &lt;2e-16 *** ## age -584.32 11.21 -52.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3050 on 3778 degrees of freedom ## Multiple R-squared: 0.4183, Adjusted R-squared: 0.4182 ## F-statistic: 2717 on 1 and 3778 DF, p-value: &lt; 2.2e-16 age.lm.p=coefficients(summary(age.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~age, data=data.df, main=paste(&quot;Univariate age p =&quot;,round(age.lm.p,5))) abline(age.lm, col=&quot;red&quot;, lwd=3) rm(age.lm,age.lm.p) 11.5.2.3 owners owners.lm &lt;- lm(price~owners, data=data.df) summary(owners.lm) ## ## Call: ## lm(formula = price ~ owners, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7583.5 -2683.5 -343.3 1862.9 31164.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9651.66 161.15 59.89 &lt;2e-16 *** ## owners -1173.18 60.43 -19.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3813 on 3778 degrees of freedom ## Multiple R-squared: 0.09071, Adjusted R-squared: 0.09047 ## F-statistic: 376.9 on 1 and 3778 DF, p-value: &lt; 2.2e-16 owners.lm.p=coefficients(summary(owners.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~owners, data=data.df, main=paste(&quot;Univariate owners p =&quot;,round(owners.lm.p,5))) abline(owners.lm, col=&quot;red&quot;, lwd=3) rm(owners.lm,owners.lm.p) 11.5.2.4 engine.size engine.size.lm &lt;- lm(price~engine.size, data=data.df) summary(engine.size.lm) ## ## Call: ## lm(formula = price ~ engine.size, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7582.8 -2853.8 -75.1 2179.3 26531.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4682.0 231.5 20.224 &lt;2e-16 *** ## engine.size 1393.1 148.8 9.362 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3954 on 3778 degrees of freedom ## Multiple R-squared: 0.02267, Adjusted R-squared: 0.02241 ## F-statistic: 87.65 on 1 and 3778 DF, p-value: &lt; 2.2e-16 engine.size.lm.p=coefficients(summary(engine.size.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~engine.size, data=data.df, main=paste(&quot;Univariate engine.size p =&quot;,round(engine.size.lm.p,5))) abline(engine.size.lm, col=&quot;red&quot;, lwd=3) rm(engine.size.lm,engine.size.lm.p) 11.6 Model with all variables It seems that owners are not very informative: could it be because we imputed half of them full.m &lt;- lm(price~., data=data.df) #summary(full.m) anova(full.m) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.8708 &lt;2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.3510 &lt;2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.5751 &lt;2e-16 *** ## owners 1 1.0695e+06 1.0695e+06 0.2467 0.6195 ## make 42 6.9638e+09 1.6580e+08 38.2404 &lt;2e-16 *** ## Residuals 3733 1.6186e+10 4.3359e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.7 Drop1 I think that drop1 suggests to keep only make, engine.size, mileage https://stats.stackexchange.com/questions/4639/interpreting-the-drop1-output-in-r drop1.m &lt;- drop1(full.m, test=&quot;F&quot;) #drop1.m drop1.m[order(drop1.m$`Pr(&gt;F)`),] ## Single term deletions ## ## Model: ## price ~ mileage + manufactured.year + engine.size + owners + ## make + age ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## make 42 6963803785 2.3150e+10 59083 38.2404 &lt;2e-16 *** ## engine.size 1 3549033541 1.9735e+10 58562 818.5316 &lt;2e-16 *** ## mileage 1 1900987496 1.8087e+10 58232 438.4344 &lt;2e-16 *** ## owners 1 4803073 1.6191e+10 57813 1.1078 0.2926 ## &lt;none&gt; 1.6186e+10 57814 ## manufactured.year 0 0 1.6186e+10 57814 ## age 0 0 1.6186e+10 57814 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 rm(full.m,drop1.m) 11.8 Optimal model optimal.m &lt;- lm(price~make + engine.size + mileage, data=data.df) anova(optimal.m) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## make 42 1.1706e+10 2.7873e+08 37.793 &lt; 2.2e-16 *** ## engine.size 1 1.1660e+08 1.1660e+08 15.810 7.136e-05 *** ## mileage 1 2.1052e+10 2.1052e+10 2854.492 &lt; 2.2e-16 *** ## Residuals 3735 2.7546e+10 7.3750e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.9 Predict new_data.df &lt;- list(make=c(&quot;Citroen&quot;,&quot;Audi&quot;,&quot;Mercedes-Benz&quot;), engine.size=c(1,1.5,3), mileage=c(70000,50000,10000)) predicted_price &lt;- predict(optimal.m,newdata = new_data.df) new_data.df &lt;- data.frame(new_data.df, predicted_price) new_data.df ## make engine.size mileage predicted_price ## 1 Citroen 1.0 70000 2653.858 ## 2 Audi 1.5 50000 10086.041 ## 3 Mercedes-Benz 3.0 10000 15371.881 rm(predicted_price) "],["stepwise-selection-of-predictors-in-lm.html", "Section 12 Stepwise selection of predictors in lm() 12.1 Prepare data 12.2 The full model 12.3 Stepwise selection 12.4 A sidetrack about seed", " Section 12 Stepwise selection of predictors in lm() 12.1 Prepare data 12.1.1 Load data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) rm(data_file) str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... head(data.df) ## mileage manufactured.year engine.size owners price make ## 1 32000 2015 1.6 NA 7495 Citroen ## 2 69000 2012 2.0 3 7000 Audi ## 3 95300 2001 3.2 6 8000 Mercedes-Benz ## 4 85962 2010 2.7 NA 7490 Audi ## 5 64000 2013 1.8 1 7000 Audi ## 6 47053 2004 3.0 4 7995 BMW dim(data.df) ## [1] 3780 6 12.1.2 Impute missed count.na.udf &lt;- function(x)(sum(is.na(x))) count_na &lt;- apply(data.df,2,count.na.udf) count_na ## mileage manufactured.year engine.size owners ## 0 0 2 1968 ## price make ## 0 0 count_na/nrow(data.df) ## mileage manufactured.year engine.size owners ## 0.0000000000 0.0000000000 0.0005291005 0.5206349206 ## price make ## 0.0000000000 0.0000000000 mean_engine_size &lt;- mean(data.df$engine.size, na.rm=T) mean_engine_size -&gt; data.df$engine.size[is.na(data.df$engine.size)] mean_owners &lt;- mean(data.df$owners, na.rm=T) mean_owners -&gt; data.df$owners[is.na(data.df$owners)] any(is.na(data.df)) ## [1] FALSE rm(count.na.udf,count_na,mean_owners, mean_engine_size) 12.2 The full model This is a full model : it uses all the data lm.full &lt;- lm(price~.,data=data.df) summary(lm.full) ## ## Call: ## lm(formula = price ~ ., data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11040.4 -1175.1 -309.0 748.6 20590.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.147e+06 2.352e+04 -48.737 &lt; 2e-16 *** ## mileage -2.808e-02 1.341e-03 -20.939 &lt; 2e-16 *** ## manufactured.year 5.717e+02 1.163e+01 49.176 &lt; 2e-16 *** ## engine.size 3.004e+03 1.050e+02 28.610 &lt; 2e-16 *** ## owners -4.082e+01 3.878e+01 -1.053 0.292638 ## makeAlfa Romeo -2.015e+03 8.290e+02 -2.430 0.015129 * ## makeAston Martin 2.576e+04 1.652e+03 15.590 &lt; 2e-16 *** ## makeAudi 1.853e+03 7.364e+02 2.517 0.011887 * ## makeBMW 1.434e+03 7.283e+02 1.969 0.049021 * ## makeChevrolet -2.511e+03 9.055e+02 -2.773 0.005576 ** ## makeChrysler -3.748e+03 9.886e+02 -3.791 0.000152 *** ## makeCitroen -2.097e+03 7.151e+02 -2.933 0.003380 ** ## makeDacia -2.873e+03 7.613e+02 -3.774 0.000163 *** ## makeDaihatsu -7.264e+01 2.197e+03 -0.033 0.973627 ## makeDodge -1.493e+03 1.257e+03 -1.188 0.234965 ## makeDS AUTOMOBILES -2.429e+03 9.203e+02 -2.639 0.008341 ** ## makeFiat -1.868e+03 7.163e+02 -2.608 0.009156 ** ## makeFord -7.337e+02 7.067e+02 -1.038 0.299247 ## makeHonda -5.571e+01 7.642e+02 -0.073 0.941890 ## makeHyundai -8.206e+02 7.082e+02 -1.159 0.246658 ## makeInfiniti 5.677e+02 1.390e+03 0.408 0.683041 ## makeIsuzu -1.261e+03 2.202e+03 -0.573 0.566998 ## makeJaguar -1.572e+03 8.844e+02 -1.778 0.075508 . ## makeJeep -1.057e+03 1.260e+03 -0.839 0.401694 ## makeKia -9.860e+02 7.296e+02 -1.351 0.176644 ## makeLand Rover 4.365e+02 1.258e+03 0.347 0.728734 ## makeLexus 2.837e+03 8.404e+02 3.376 0.000744 *** ## makeMazda -1.161e+02 7.339e+02 -0.158 0.874260 ## makeMercedes-Benz 1.450e+03 7.356e+02 1.971 0.048833 * ## makeMG -1.746e+03 9.395e+02 -1.859 0.063108 . ## makeMINI 1.445e+02 7.190e+02 0.201 0.840741 ## makeMitsubishi -1.081e+03 8.086e+02 -1.337 0.181427 ## makeNissan 3.130e+02 7.077e+02 0.442 0.658330 ## makePeugeot -1.647e+03 7.119e+02 -2.313 0.020761 * ## makePorsche 1.289e+04 9.951e+02 12.948 &lt; 2e-16 *** ## makeRenault -1.382e+03 7.150e+02 -1.933 0.053264 . ## makeSaab -2.424e+03 9.070e+02 -2.673 0.007547 ** ## makeSEAT -1.419e+03 7.160e+02 -1.982 0.047509 * ## makeSKODA -1.344e+03 7.607e+02 -1.767 0.077371 . ## makeSmart -1.129e+03 8.612e+02 -1.311 0.190009 ## makeSsangYong -3.613e+03 2.200e+03 -1.643 0.100560 ## makeSubaru -2.250e+03 1.018e+03 -2.210 0.027170 * ## makeSuzuki -1.701e+03 7.513e+02 -2.264 0.023656 * ## makeToyota -1.070e+03 7.134e+02 -1.500 0.133588 ## makeVauxhall -2.188e+03 7.004e+02 -3.124 0.001796 ** ## makeVolkswagen -1.189e+01 7.106e+02 -0.017 0.986651 ## makeVolvo -7.560e+02 7.654e+02 -0.988 0.323307 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2082 on 3733 degrees of freedom ## Multiple R-squared: 0.7321, Adjusted R-squared: 0.7288 ## F-statistic: 221.8 on 46 and 3733 DF, p-value: &lt; 2.2e-16 #coefficients(summary(lm.full)) anova(lm.full) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.8708 &lt;2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.3510 &lt;2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.5751 &lt;2e-16 *** ## owners 1 1.0695e+06 1.0695e+06 0.2467 0.6195 ## make 42 6.9638e+09 1.6580e+08 38.2404 &lt;2e-16 *** ## Residuals 3733 1.6186e+10 4.3359e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 formula(lm.full) ## price ~ mileage + manufactured.year + engine.size + owners + ## make 12.3 Stepwise selection base R drop1, add1, step http://rstudio-pubs-static.s3.amazonaws.com/2899_a9129debf6bd47d2a0501de9c0dc583d.html “advanced” solutions MASS::stepAIC leaps package rms::fastbw olsrr package 12.3.1 backward step.bk &lt;- step(lm.full, direction = &quot;backward&quot;, trace = 1) ## Start: AIC=57814.29 ## price ~ mileage + manufactured.year + engine.size + owners + ## make ## ## Df Sum of Sq RSS AIC ## - owners 1 4.8031e+06 1.6191e+10 57813 ## &lt;none&gt; 1.6186e+10 57814 ## - mileage 1 1.9010e+09 1.8087e+10 58232 ## - engine.size 1 3.5490e+09 1.9735e+10 58562 ## - make 42 6.9638e+09 2.3150e+10 59083 ## - manufactured.year 1 1.0485e+10 2.6671e+10 59700 ## ## Step: AIC=57813.41 ## price ~ mileage + manufactured.year + engine.size + make ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.6191e+10 57813 ## - mileage 1 1.9644e+09 1.8155e+10 58244 ## - engine.size 1 3.5511e+09 1.9742e+10 58561 ## - make 42 6.9601e+09 2.3151e+10 59081 ## - manufactured.year 1 1.1355e+10 2.7546e+10 59820 formula(step.bk) ## price ~ mileage + manufactured.year + engine.size + make anova(step.bk) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.750 &lt; 2.2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.296 &lt; 2.2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.503 &lt; 2.2e-16 *** ## make 42 6.9601e+09 1.6572e+08 38.219 &lt; 2.2e-16 *** ## Residuals 3734 1.6191e+10 4.3360e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 step.bk$anova ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 3733 16185742445 57814.29 ## 2 - owners 1 4803073 3734 16190545517 57813.41 12.3.2 forward Requires null model: for data and intercept scope: for predictors to add lm.null &lt;- lm(price~1,data=data.df) step.fw &lt;- step(lm.null, scope= ~owners+mileage+engine.size+make+manufactured.year, direction = &quot;forward&quot;, trace = 1) ## Start: AIC=62701.31 ## price ~ 1 ## ## Df Sum of Sq RSS AIC ## + manufactured.year 1 2.5275e+10 3.5146e+10 60655 ## + mileage 1 1.8119e+10 4.2302e+10 61356 ## + make 42 1.1706e+10 4.8714e+10 61971 ## + owners 1 5.4810e+09 5.4940e+10 62344 ## + engine.size 1 1.3700e+09 5.9051e+10 62617 ## &lt;none&gt; 6.0421e+10 62701 ## ## Step: AIC=60655.17 ## price ~ manufactured.year ## ## Df Sum of Sq RSS AIC ## + make 42 1.4026e+10 2.1120e+10 58814 ## + engine.size 1 9.6948e+09 2.5451e+10 59437 ## + mileage 1 1.0921e+09 3.4053e+10 60538 ## &lt;none&gt; 3.5146e+10 60655 ## + owners 1 1.4655e+07 3.5131e+10 60656 ## ## Step: AIC=58814.04 ## price ~ manufactured.year + make ## ## Df Sum of Sq RSS AIC ## + engine.size 1 2964711644 1.8155e+10 58244 ## + mileage 1 1378008193 1.9742e+10 58561 ## + owners 1 59227052 2.1060e+10 58805 ## &lt;none&gt; 2.1120e+10 58814 ## ## Step: AIC=58244.27 ## price ~ manufactured.year + make + engine.size ## ## Df Sum of Sq RSS AIC ## + mileage 1 1964368341 1.6191e+10 57813 ## + owners 1 68183917 1.8087e+10 58232 ## &lt;none&gt; 1.8155e+10 58244 ## ## Step: AIC=57813.41 ## price ~ manufactured.year + make + engine.size + mileage ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.6191e+10 57813 ## + owners 1 4803073 1.6186e+10 57814 formula(step.fw) ## price ~ manufactured.year + make + engine.size + mileage anova(step.fw) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## manufactured.year 1 2.5275e+10 2.5275e+10 5829.185 &lt; 2.2e-16 *** ## make 42 1.4026e+10 3.3395e+08 77.018 &lt; 2.2e-16 *** ## engine.size 1 2.9647e+09 2.9647e+09 683.747 &lt; 2.2e-16 *** ## mileage 1 1.9644e+09 1.9644e+09 453.039 &lt; 2.2e-16 *** ## Residuals 3734 1.6191e+10 4.3360e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 step.bk$anova ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 3733 16185742445 57814.29 ## 2 - owners 1 4803073 3734 16190545517 57813.41 12.4 A sidetrack about seed will be used in caret package x &lt;- 1:100 set.seed(42) sample(x,5) ## [1] 49 65 25 74 18 rm(x) "],["regression-trees.html", "Section 13 Regression trees 13.1 Summary 13.2 Start section 13.3 Prepare data 13.4 Simple tree model 13.5 Full tree without pruning 13.6 A model with manual settings control 13.7 Selecting optimal hyperparameters through enumeration 13.8 Bootstrap aggregating (bagging) 13.9 Bagging with caret 13.10 Confusion plots 13.11 Final section", " Section 13 Regression trees 13.1 Summary http://uc-r.github.io/regression_trees Also you may see for general R intro example: http://uc-r.github.io/ 13.2 Start section Sys.time() ## [1] &quot;2021-04-16 18:39:16 BST&quot; rm(list=ls()) graphics.off() gc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 2326991 124.3 4571884 244.2 NA 4571884 244.2 ## Vcells 4263262 32.6 10146329 77.5 16384 8388608 64.0 #install.packages(&quot;rpart.plot&quot;) #install.packages(&quot;AmesHousing&quot;) library(rsample) # data splitting library(dplyr) # data wrangling library(rpart) # performing regression trees library(rpart.plot) # plotting regression trees library(ipred) # bagging library(caret) # bagging ## Loading required package: lattice library(AmesHousing) # data to play with set.seed(123) 13.3 Prepare data 13.3.1 Data data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) data.df &lt;- unique(data.df) str(data.df) ## &#39;data.frame&#39;: 1600 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... head(data.df) ## mileage manufactured.year engine.size owners price make ## 1 32000 2015 1.6 NA 7495 Citroen ## 2 69000 2012 2.0 3 7000 Audi ## 3 95300 2001 3.2 6 8000 Mercedes-Benz ## 4 85962 2010 2.7 NA 7490 Audi ## 5 64000 2013 1.8 1 7000 Audi ## 6 47053 2004 3.0 4 7995 BMW dim(data.df) ## [1] 1600 6 rm(data_file) 13.3.2 Split data_split &lt;- initial_split(data.df) names(data_split) ## [1] &quot;data&quot; &quot;in_id&quot; &quot;out_id&quot; &quot;id&quot; data_train &lt;- training(data_split) data_test &lt;- testing(data_split) 13.4 Simple tree model Uses default pruning settings m1 &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot; ) m1 ## n= 1200 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 1200 25233610000 7217.132 ## 2) manufactured.year&lt; 2012.5 360 4569798000 3340.072 ## 4) engine.size&lt; 3.1 350 1440377000 3008.423 ## 8) manufactured.year&lt; 2010.5 261 719566900 2484.115 * ## 9) manufactured.year&gt;=2010.5 89 438652500 4546.000 * ## 5) engine.size&gt;=3.1 10 1743531000 14947.800 * ## 3) manufactured.year&gt;=2012.5 840 12933280000 8878.729 ## 6) engine.size&lt; 2.45 829 8580714000 8635.245 ## 12) manufactured.year&lt; 2016.5 390 3016570000 7224.836 ## 24) make=Abarth,Alfa Romeo,Chevrolet,Chrysler,Citroen,Dacia,DS AUTOMOBILES,Fiat,Ford,Honda,Hyundai,Infiniti,Kia,Mazda,MINI,Mitsubishi,Nissan,Peugeot,Renault,SEAT,SKODA,Smart,Suzuki,Toyota,Vauxhall,Volkswagen,Volvo 356 1369669000 6784.680 ## 48) make=Alfa Romeo,Chevrolet,Chrysler,Citroen,Dacia,DS AUTOMOBILES,Fiat,Ford,Hyundai,Kia,Mitsubishi,Peugeot,Renault,SEAT,SKODA,Smart,Suzuki,Toyota,Vauxhall 259 631373900 6244.714 * ## 49) make=Abarth,Honda,Infiniti,Mazda,MINI,Nissan,Volkswagen,Volvo 97 461148500 8226.443 * ## 25) make=Audi,BMW,Jaguar,Lexus,Mercedes-Benz,Porsche 34 855767900 11833.530 ## 50) mileage&gt;=40446.5 27 193347700 10402.590 * ## 51) mileage&lt; 40446.5 7 393894700 17352.860 * ## 13) manufactured.year&gt;=2016.5 439 4099121000 9888.228 ## 26) engine.size&lt; 1.45 262 1154730000 8583.332 * ## 27) engine.size&gt;=1.45 177 1837909000 11819.770 ## 54) make=Citroen,Dacia,DS AUTOMOBILES,Fiat,MG,MINI,Peugeot,Renault,Suzuki,Vauxhall,Volvo 71 307640100 9776.577 * ## 55) make=Audi,BMW,Ford,Honda,Hyundai,Infiniti,Jeep,Kia,Lexus,Mazda,Mercedes-Benz,Nissan,SEAT,Toyota,Volkswagen 106 1035339000 13188.320 * ## 7) engine.size&gt;=2.45 11 599548200 27228.550 * rpart.plot(m1) plotcp(m1) 13.5 Full tree without pruning m2 &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(cp = 0, xval = 10) ) plotcp(m2) abline(v = 7, lty = &quot;dashed&quot;) abline(v = 20, lty = &quot;dashed&quot;, col=&quot;red&quot;) 13.6 A model with manual settings control m3 &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(minsplit = 10, maxdepth = 12, xval = 10) ) m3$cptable ## CP nsplit rel error xerror xstd ## 1 0.30635851 0 1.0000000 1.0026040 0.09546510 ## 2 0.14873099 1 0.6936415 0.7131700 0.10101577 ## 3 0.05805843 2 0.5449105 0.5948951 0.08586096 ## 4 0.05492239 3 0.4868521 0.5677906 0.07959552 ## 5 0.04384951 4 0.4319297 0.5439939 0.07924383 ## 6 0.03135232 5 0.3880802 0.4631415 0.06097607 ## 7 0.01961391 6 0.3567278 0.4104493 0.04707860 ## 8 0.01118181 7 0.3371139 0.3870748 0.04676690 ## 9 0.01098324 8 0.3259321 0.3843668 0.04663158 ## 10 0.01064158 9 0.3149489 0.3802017 0.04662678 ## 11 0.01000000 10 0.3043073 0.3786965 0.04662779 plotcp(m3) 13.7 Selecting optimal hyperparameters through enumeration 13.7.1 Make the hyperparameters grid hyper_grid &lt;- expand.grid( minsplit = seq(5, 20, 1), maxdepth = seq(8, 15, 1) ) head(hyper_grid) ## minsplit maxdepth ## 1 5 8 ## 2 6 8 ## 3 7 8 ## 4 8 8 ## 5 9 8 ## 6 10 8 tail(hyper_grid) ## minsplit maxdepth ## 123 15 15 ## 124 16 15 ## 125 17 15 ## 126 18 15 ## 127 19 15 ## 128 20 15 dim(hyper_grid) ## [1] 128 2 13.7.2 # Calculate models for all combinations in the hyperparameters grid models &lt;- list() for (i in 1:nrow(hyper_grid)) { # get minsplit, maxdepth values at row i minsplit &lt;- hyper_grid$minsplit[i] maxdepth &lt;- hyper_grid$maxdepth[i] # train a model and store in the list models[[i]] &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(minsplit = minsplit, maxdepth = maxdepth) ) } 13.7.3 Select hyperparameters with lowest x-validation error # function to get optimal cp get_cp &lt;- function(x) { min &lt;- which.min(x$cptable[, &quot;xerror&quot;]) cp &lt;- x$cptable[min, &quot;CP&quot;] } # function to get minimum error get_min_error &lt;- function(x) { min &lt;- which.min(x$cptable[, &quot;xerror&quot;]) xerror &lt;- x$cptable[min, &quot;xerror&quot;] } # Add cp and error to hyperparameters grid hyper_grid %&gt;% mutate( cp = purrr::map_dbl(models, get_cp), error = purrr::map_dbl(models, get_min_error) ) %&gt;% arrange(error) %&gt;% top_n(-5, wt = error) ## minsplit maxdepth cp error ## 1 12 9 0.01 0.3630854 ## 2 6 8 0.01 0.3651868 ## 3 9 8 0.01 0.3665252 ## 4 9 14 0.01 0.3668898 ## 5 16 8 0.01 0.3671934 13.7.4 The optimal tree optimal_tree &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(minsplit = 12, maxdepth = 9, cp = 0.01) ) rpart.plot(optimal_tree) 13.7.5 Prediction RMSE is the error estimate = how (on average) the predicted value is different from the actual value in the test set pred &lt;- predict(optimal_tree, newdata = data_test) RMSE(pred = pred, obs = data_test$price) ## [1] 2524.683 hist(data.df$price) quantile(data.df$price) ## 0% 25% 50% 75% 100% ## 250 3999 6850 9200 34999 13.8 Bootstrap aggregating (bagging) 13.8.1 bagged trees model with default settings Note Out-Of-Bag x-validation RMSE in the model summary # make bootstrapping reproducible set.seed(123) # train bagged model bagged_m1 &lt;- bagging( formula = price ~ ., data = data_train, coob = TRUE ) bagged_m1 ## ## Bagging regression trees with 25 bootstrap replications ## ## Call: bagging.data.frame(formula = price ~ ., data = data_train, coob = TRUE) ## ## Out-of-bag estimate of root mean squared error: 3251.254 13.8.2 Selecting optimal bag size # assess 10-50 bagged trees ntree &lt;- 10:150 # create empty vector to store OOB RMSE values rmse &lt;- vector(mode = &quot;numeric&quot;, length = length(ntree)) x &lt;- Sys.time() for (i in seq_along(ntree)) { # reproducibility set.seed(123) # perform bagged model model &lt;- bagging( formula = price ~ ., data = data_train, coob = TRUE, nbagg = ntree[i]) # get OOB error rmse[i] &lt;- model$err } y &lt;- Sys.time() y-x ## Time difference of 34.79162 secs plot(ntree, rmse, type = &#39;l&#39;, lwd = 2) abline(v = 60, col = &quot;red&quot;, lty = &quot;dashed&quot;) 13.9 Bagging with caret https://topepo.github.io/caret https://topepo.github.io/caret/available-models.html to be done: requires # Specify 10-fold cross validation ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10) # CV bagged model x &lt;- Sys.time() bagged_cv &lt;- train( price ~ ., data = na.omit(data_train), method = &quot;treebag&quot;, trControl = ctrl, importance = TRUE, ) y &lt;- Sys.time() y-x ## Time difference of 3.312546 secs bagged_cv ## Bagged CART ## ## 561 samples ## 5 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 504, 506, 505, 505, 505, 505, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3044.858 0.658359 1909.488 # plot most important variables plot(varImp(bagged_cv), 20) pred &lt;- predict(bagged_cv, data_test) RMSE(pred, data_test$price) ## Warning in pred - obs: longer object length is not a multiple of shorter object ## length ## [1] 5330.005 13.10 Confusion plots plot(na.omit(data_train)$price, predict(bagged_cv, na.omit(data_train)), main=&quot;Training set&quot;) abline(a=0,b=1,col=&quot;red&quot;,lty=2) R2 &lt;- cor(na.omit(data_train)$price, predict(bagged_cv, na.omit(data_train)))^2 legend(&quot;bottomright&quot;, legend=paste(&quot;R^2 =&quot;,round(R2,2)), bty=&quot;n&quot;) plot(na.omit(data_test)$price, predict(bagged_cv, na.omit(data_test)), main=&quot;Test set&quot;) abline(a=0,b=1,col=&quot;red&quot;,lty=2) R2 &lt;- cor(na.omit(data_test)$price, predict(bagged_cv, na.omit(data_test)))^2 legend(&quot;bottomright&quot;, legend=paste(&quot;R^2 =&quot;,round(R2,2)), bty=&quot;n&quot;) 13.11 Final section sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] AmesHousing_0.0.4 caret_6.0-86 lattice_0.20-41 ipred_0.9-11 ## [5] rpart.plot_3.0.9 rpart_4.1-15 rsample_0.0.9 dplyr_1.0.4 ## [9] plotly_4.9.3 ggplot2_3.3.3 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 tidyr_1.1.2 jsonlite_1.7.2 ## [4] viridisLite_0.3.0 splines_3.6.2 foreach_1.5.1 ## [7] prodlim_2019.11.13 assertthat_0.2.1 stats4_3.6.2 ## [10] highr_0.8 yaml_2.2.1 globals_0.14.0 ## [13] pillar_1.4.7 glue_1.4.2 reticulate_1.18 ## [16] pROC_1.17.0.1 digest_0.6.27 colorspace_2.0-0 ## [19] recipes_0.1.15 htmltools_0.5.1.1 Matrix_1.3-2 ## [22] plyr_1.8.6 timeDate_3043.102 pkgconfig_2.0.3 ## [25] listenv_0.8.0 bookdown_0.21 purrr_0.3.4 ## [28] scales_1.1.1 gower_0.2.2 lava_1.6.9 ## [31] tibble_3.0.6 mgcv_1.8-33 generics_0.1.0 ## [34] farver_2.0.3 ellipsis_0.3.1 withr_2.4.1 ## [37] furrr_0.2.2 nnet_7.3-15 lazyeval_0.2.2 ## [40] cli_2.3.0 survival_3.2-7 magrittr_2.0.1 ## [43] crayon_1.4.1 evaluate_0.14 future_1.21.0 ## [46] fansi_0.4.2 parallelly_1.23.0 nlme_3.1-152 ## [49] MASS_7.3-53.1 class_7.3-18 tools_3.6.2 ## [52] data.table_1.13.6 lifecycle_1.0.0 stringr_1.4.0 ## [55] munsell_0.5.0 e1071_1.7-4 compiler_3.6.2 ## [58] rlang_0.4.10 grid_3.6.2 iterators_1.0.13 ## [61] rstudioapi_0.13 htmlwidgets_1.5.3 crosstalk_1.1.1 ## [64] labeling_0.4.2 rmarkdown_2.6 ModelMetrics_1.2.2.2 ## [67] gtable_0.3.0 codetools_0.2-18 DBI_1.1.1 ## [70] reshape2_1.4.4 R6_2.5.0 lubridate_1.7.9.2 ## [73] knitr_1.31 utf8_1.1.4 stringi_1.5.3 ## [76] parallel_3.6.2 Rcpp_1.0.6 vctrs_0.3.6 ## [79] tidyselect_1.1.0 xfun_0.21 "]]
