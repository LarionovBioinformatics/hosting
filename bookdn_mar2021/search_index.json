[["index.html", "I forgot the title Section 1 My great Math LaTex 1.1 Rendering to PDF and EPUB 1.2 Sharing on social media", " I forgot the title Of course, its Me 2021-03-24 Section 1 My great Math LaTex This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\[{{a^2 + b^2} \\over { \\sum{y} \\iff \\int{z \\over d^2}}} \\ne c^2\\]. https://www.math.uci.edu/~xiangwen/pdf/LaTeX-Math-Symbols.pdf 1.1 Rendering to PDF and EPUB Although PDF and EPUB rendering is a default feature of bookdown, it is suppressed in this example to illustrate interactive features of Plotly 1.2 Sharing on social media I do not like these media ! I kept it here just to illustrate this feature of bookdown "],["r-markdown.html", "Section 2 R-Markdown 2.1 2nd level header 2.2 Not only R … 2.3 Back to R", " Section 2 R-Markdown 2.1 2nd level header 2.1.1 3rd level header etc 2.2 Not only R … 2.2.1 Python Just use chunk with option “python” Requires Reticulate and Python (of course :) DOES NOT WORK with bookdown (and gives a very obscure error mesage !) print(&quot;hello world&quot;) a=5 print(a) 2.2.2 Bash Just use chunk with option “bash” echo BlaBlaBla ## BlaBlaBla 2.3 Back to R 2.3.1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: data(&quot;cars&quot;) summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 2.3.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["a-simple-analysis.html", "Section 3 A simple analysis 3.1 In-built datasets 3.2 Explore data 3.3 T-test 3.4 Liear model", " Section 3 A simple analysis library(ggplot2) 3.1 In-built datasets # Inbuilt dataset data(&quot;mtcars&quot;) # Explore data class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 dim(mtcars) ## [1] 32 11 3.2 Explore data mpg vs hp cosiderig vs as covariate 3.2.1 Base R plottig plot(mpg~hp, data=mtcars, pch=vs, main=&quot;My title&quot;) legend(&quot;topright&quot;, c(&quot;vs=0&quot;,&quot;vs=1&quot;), pch=c(0,1)) 3.2.2 ggplot style ggplot(data=mtcars,aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point() + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.3 T-test plot(mpg~vs, data=mtcars) boxplot(mpg~vs, data=mtcars) t.test(mpg~vs, data=mtcars) ## ## Welch Two Sample t-test ## ## data: mpg by vs ## t = -4.6671, df = 22.716, p-value = 0.0001098 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.462508 -4.418445 ## sample estimates: ## mean in group 0 mean in group 1 ## 16.61667 24.55714 3.4 Liear model 3.4.1 mpg ~ hp my_model_1 &lt;- lm(mpg~hp, data=mtcars) class(my_model_1) ## [1] &quot;lm&quot; summary(my_model_1) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 plot(mpg~hp, data=mtcars) abline(my_model_1) plot(my_model_1) 3.4.2 mpg ~ vs + hp my_model_2 &lt;- lm(mpg ~ vs + hp, data=mtcars) summary(my_model_2) ## ## Call: ## lm(formula = mpg ~ vs + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7131 -2.3336 -0.1332 1.9055 7.9055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.96300 2.89069 9.328 3.13e-10 *** ## vs 2.57622 1.96966 1.308 0.201163 ## hp -0.05453 0.01448 -3.766 0.000752 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.818 on 29 degrees of freedom ## Multiple R-squared: 0.6246, Adjusted R-squared: 0.5987 ## F-statistic: 24.12 on 2 and 29 DF, p-value: 6.768e-07 plot(my_model_2) 3.4.3 mpg ~ vs * hp my_model_3 &lt;- lm(mpg ~ vs * hp, data=mtcars) summary(my_model_3) ## ## Call: ## lm(formula = mpg ~ vs * hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5821 -1.7710 -0.3612 1.5969 9.2646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.49637 2.73893 8.944 1.07e-09 *** ## vs 14.50418 4.58160 3.166 0.00371 ** ## hp -0.04153 0.01379 -3.011 0.00547 ** ## vs:hp -0.11657 0.04130 -2.822 0.00868 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.428 on 28 degrees of freedom ## Multiple R-squared: 0.7077, Adjusted R-squared: 0.6764 ## F-statistic: 22.6 on 3 and 28 DF, p-value: 1.227e-07 plot(my_model_3) "],["data-types.html", "Section 4 Data types 4.1 Vectors 4.2 Numeric matrices 4.3 Recycling in element-wise operations 4.4 Factors: categorical variale 4.5 Lists 4.6 Dataframe: list of vectors/factors", " Section 4 Data types Be careful with changing folders in bookdown !! setwd(&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021&quot;) 4.1 Vectors 4.1.1 Numeric vectors a &lt;- 1:5 class(a) ## [1] &quot;integer&quot; a ## [1] 1 2 3 4 5 b &lt;- seq(from=10, to=50, by=10) class(b) ## [1] &quot;numeric&quot; b ## [1] 10 20 30 40 50 # Vectorised operation: element-wise summation a + b ## [1] 11 22 33 44 55 4.1.2 Vector elements may have names Names is a property attributes(a) ## NULL names(a) &lt;- c(&quot;e1&quot;,&quot;e2&quot;,&quot;e3&quot;,&quot;e4&quot;,&quot;e5&quot;) a ## e1 e2 e3 e4 e5 ## 1 2 3 4 5 attributes(a) ## $names ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; a[c(1,2,4)] ## e1 e2 e4 ## 1 2 4 a[c(T,T,F,T,F)] ## e1 e2 e4 ## 1 2 4 a[c(&quot;e1&quot;,&quot;e2&quot;,&quot;e4&quot;)] ## e1 e2 e4 ## 1 2 4 4.2 Numeric matrices m1 &lt;- matrix(1:25, nrow=5) m1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 m2 &lt;- matrix(rep(10,25), nrow=5) m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 10 10 10 10 ## [2,] 10 10 10 10 10 ## [3,] 10 10 10 10 10 ## [4,] 10 10 10 10 10 ## [5,] 10 10 10 10 10 t(m1) # Transposition ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 ## [5,] 21 22 23 24 25 m1 + m2 # Element-wise summation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 16 21 26 31 ## [2,] 12 17 22 27 32 ## [3,] 13 18 23 28 33 ## [4,] 14 19 24 29 34 ## [5,] 15 20 25 30 35 m1 * m2 # Element-wise multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 60 110 160 210 ## [2,] 20 70 120 170 220 ## [3,] 30 80 130 180 230 ## [4,] 40 90 140 190 240 ## [5,] 50 100 150 200 250 m1 %*% m2 # Matrix multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 550 550 550 550 550 ## [2,] 600 600 600 600 600 ## [3,] 650 650 650 650 650 ## [4,] 700 700 700 700 700 ## [5,] 750 750 750 750 750 4.2.1 Matrices may have rownames and colnames rownames(m1) &lt;- c(&quot;s1&quot;,&quot;s2&quot;,&quot;s3&quot;,&quot;s4&quot;,&quot;s5&quot;) colnames(m1) &lt;- c(&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;g5&quot;) m1 ## g1 g2 g3 g4 g5 ## s1 1 6 11 16 21 ## s2 2 7 12 17 22 ## s3 3 8 13 18 23 ## s4 4 9 14 19 24 ## s5 5 10 15 20 25 m1[c(&quot;s2&quot;,&quot;s3&quot;),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 m1[c(F,T,T,F,F),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 4.3 Recycling in element-wise operations Shorter vector is recycled along the longer oject Matrices are processed as vectors in column-after-column way c &lt;- 1:3 c ## [1] 1 2 3 b + c ## Warning in b + c: longer object length is not a multiple of shorter object ## length ## [1] 11 22 33 41 52 c + m2 ## Warning in c + m2: longer object length is not a multiple of shorter object ## length ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 13 12 11 13 ## [2,] 12 11 13 12 11 ## [3,] 13 12 11 13 12 ## [4,] 11 13 12 11 13 ## [5,] 12 11 13 12 11 a + m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 m2 + a ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 rm(a,b,c,m1,m2) 4.4 Factors: categorical variale Categorical != character v &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) class(v) ## [1] &quot;character&quot; v ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; f &lt;- factor(v) class(f) ## [1] &quot;factor&quot; f ## [1] A B C ## Levels: A B C f &lt;- factor(v, levels=c(&quot;C&quot;,&quot;B&quot;,&quot;A&quot;)) f ## [1] A B C ## Levels: C B A f &lt;- factor(v, levels=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;)) f ## [1] &lt;NA&gt; B C ## Levels: B C D f &lt;- factor(v, levels=LETTERS) f ## [1] A B C ## Levels: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z f &lt;- factor(v, levels=LETTERS, ordered = T) f ## [1] A B C ## 26 Levels: A &lt; B &lt; C &lt; D &lt; E &lt; F &lt; G &lt; H &lt; I &lt; J &lt; K &lt; L &lt; M &lt; N &lt; O &lt; ... &lt; Z # Potential confusion factor(1:5, levels=c(4,5,3,2,1), ordered=T) ## [1] 1 2 3 4 5 ## Levels: 4 &lt; 5 &lt; 3 &lt; 2 &lt; 1 rm(v,f) 4.5 Lists Collection of ojects of different types my.list &lt;- list(c(1,2),&quot;A&quot;,T ) my.list ## [[1]] ## [1] 1 2 ## ## [[2]] ## [1] &quot;A&quot; ## ## [[3]] ## [1] TRUE my.list[1] ## [[1]] ## [1] 1 2 my.list[[1]][1] ## [1] 1 my.list[[1]][2] ## [1] 2 my.list[2][1] ## [[1]] ## [1] &quot;A&quot; my.list &lt;- list(lel1=c(1,2), lel2=&quot;A&quot;, lel3=T ) my.list ## $lel1 ## [1] 1 2 ## ## $lel2 ## [1] &quot;A&quot; ## ## $lel3 ## [1] TRUE my.list$lel1 ## [1] 1 2 my.list$lel1[2] ## [1] 2 rm(my.list) 4.6 Dataframe: list of vectors/factors #data(&quot;mtcars&quot;) class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 #data(&quot;mpg&quot;) #class(mpg) #str(mpg) my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA)) class(my.df) ## [1] &quot;data.frame&quot; my.df ## first_name second_name age income ## 1 AL Smith 15 100 ## 2 AL Larionov 23 200 ## 3 BS Bob 1 NA str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: Factor w/ 3 levels &quot;Bob&quot;,&quot;Larionov&quot;,..: 3 2 1 ## $ age : num 15 23 1 ## $ income : num 100 200 NA my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA), stringsAsFactors=F) str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: chr &quot;Smith&quot; &quot;Larionov&quot; &quot;Bob&quot; ## $ age : num 15 23 1 ## $ income : num 100 200 NA "],["plotly.html", "Section 5 Plotly 5.1 Reference 5.2 Load library 5.3 Get some data 5.4 Plotly magic", " Section 5 Plotly 5.1 Reference https://plotly-r.com/index.html 5.2 Load library library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 5.3 Get some data # Inbuilt dataset #data(&quot;mtcars&quot;) mtcars &lt;- data.frame(mtcars, make=rownames(mtcars), stringsAsFactors = F) # Explore data dim(mtcars) ## [1] 32 12 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 12 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... ## $ make: chr &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... 5.4 Plotly magic d &lt;- ggplot(data=mtcars, aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point(aes(text=make)) + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## Warning: Ignoring unknown aesthetics: text d ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplotly(d, tooltip=&quot;text&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # A base R plot ... plot(mpg~hp, data=mtcars) "],["distributionns.html", "Section 6 Distributionns 6.1 Function curve() 6.2 Normal distribution 6.3 Gamma 6.4 Beta", " Section 6 Distributionns 6.1 Function curve() curve(sin(x),0,2*pi) curve(cos(x),from=0,to=2*pi,add=T,col=&quot;red&quot;,lty=2) 6.2 Normal distribution dnorm(-1.96, mean=0, sd=1) ## [1] 0.05844094 dnorm(-1.96, mean=10, sd=7) ## [1] 0.01324074 curve(dnorm(x),-3,3) curve(dnorm(x, 10, 3),0,20) curve(pnorm(x, 10, 3),0,20) curve(dnorm(x),-4,4) abline(v=qnorm(0.025), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.025, lower.tail = F), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.005), col=&quot;blue&quot;, lty=2) abline(v=qnorm(0.005, lower.tail = F), col=&quot;blue&quot;, lty=2) curve(qnorm(x)) rnorm(5) ## [1] 0.3127052 -1.7464490 -0.2896767 -0.2313764 -1.5151107 hist(rnorm(1000),freq = F,ylim=c(0,0.5)) curve(dnorm(x),-4,4,add=T,col=&quot;blue&quot;) dh &lt;- c(rnorm(1000,75,50),rnorm(1000,180,15)) hist(dh,100,200) 6.3 Gamma curve(dgamma(x,2,6)) curve(dgamma(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;g(2,6)&quot;,&quot;g(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dgamma(x,3,7),0,2) abline(v=qgamma(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qgamma(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rgamma(1000,3,7),freq = F, add=T, border=&quot;green&quot;) 6.4 Beta curve(dbeta(x,2,6)) curve(dbeta(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;b(2,6)&quot;,&quot;b(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dbeta(x,3,7)) abline(v=qbeta(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qbeta(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rbeta(1000,3,7),freq = F, add=T, border=&quot;green&quot;) "],["package.html", "Section 7 Package 7.1 User-defined fnction 7.2 Recursive functions are allowed 7.3 Lets add some data 7.4 Prepare template for package source folder", " Section 7 Package 7.1 User-defined fnction mff &lt;- function(a,b){ return(a+b) } mff(3,5) ## [1] 8 #mff(3,&quot;Good&quot;) #mff(&quot;a&quot;,&quot;b&quot;) 7.2 Recursive functions are allowed my_sum &lt;- function(n){ result &lt;- 0 for(i in 1:n){ result &lt;- result + i } return(result) } my_sum(10) ## [1] 55 my_rsum &lt;- function(n){ if(n&gt;1){ return(n+my_rsum(n-1)) } if(n==1){ return(n) } } my_rsum(10) ## [1] 55 7.3 Lets add some data my_great_dataset &lt;- data.frame(names=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), values=c(1,2,3,4)) 7.4 Prepare template for package source folder ls() getwd() package.skeleton(name=&quot;ab&quot;,list=ls()) A bush chunk to build: cd /Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021 R CMD build ab R CMD check ab_1.0.tar.gz Follow the instructions for minimal required documentation, i.e.: Titles (&amp; Descriptions?) in each object Rd Examples (in the package Rd) Then install, load and enjoy ! #install.packages(...) library(ab) data(my_great_dataset) "],["data-reading-writing.html", "Section 8 Data reading / writing 8.1 A simple simulated dataset 8.2 Saving and reading", " Section 8 Data reading / writing library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 8.1 A simple simulated dataset x &lt;- runif(1000, 0, 100) hist(x) f &lt;- factor(c(rep(&quot;A&quot;,500),rep(&quot;B&quot;,500))) head(f) ## [1] A A A A A A ## Levels: A B tail(f) ## [1] B B B B B B ## Levels: A B y1 &lt;- 0.5 * x[1:500] + rnorm(500,0,25) y2 &lt;- -0.5 * x[501:1000] + rnorm(500,0,25) y &lt;- c(y1,y2+50) sim_data.df &lt;- data.frame(y,x,f) str(sim_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -36.57 2.15 42.44 70.87 88.03 ... ## $ x: num 27.7 45.4 81.7 64.4 55.4 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... plot(y~x, pch=as.integer(f)) ggplot(data=sim_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 8.2 Saving and reading … at last ! 8.2.1 RData save.image(file=&quot;results1.RData&quot;) rm(list=ls()) load(&quot;results1.RData&quot;) save(&quot;sim_data.df&quot;,file=&quot;results2.RData&quot;) rm(list=ls()) load(&quot;results2.RData&quot;) 8.2.2 Text files write.table(sim_data.df,file=&quot;results.tsv&quot;, quote=F,sep=&quot;\\t&quot;,row.names = F) rm(list=ls()) my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -36.57 2.15 42.44 70.87 88.03 ... ## $ x: num 27.7 45.4 81.7 64.4 55.4 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-82.526 Min. : 0.2636 A:500 ## 1st Qu.: 5.312 1st Qu.:24.2604 B:500 ## Median : 24.084 Median :50.5506 ## Mean : 24.223 Mean :49.9268 ## 3rd Qu.: 43.284 3rd Qu.:74.5549 ## Max. :117.011 Max. :99.7659 "],["data-exploring-modifying.html", "Section 9 Data exploring / modifying 9.1 Read data 9.2 Apply (summary) functions over margin 9.3 Missed values 9.4 Sorting, filtering and summarising 9.5 Dplyr", " Section 9 Data exploring / modifying library(ggplot2) library(dplyr) 9.1 Read data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -36.57 2.15 42.44 70.87 88.03 ... ## $ x: num 27.7 45.4 81.7 64.4 55.4 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-82.526 Min. : 0.2636 A:500 ## 1st Qu.: 5.312 1st Qu.:24.2604 B:500 ## Median : 24.084 Median :50.5506 ## Mean : 24.223 Mean :49.9268 ## 3rd Qu.: 43.284 3rd Qu.:74.5549 ## Max. :117.011 Max. :99.7659 9.2 Apply (summary) functions over margin This is base R data(&quot;mtcars&quot;) dim(mtcars) ## [1] 32 11 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... apply(mtcars,2,mean) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 apply(my_data.df[,c(1,2)],2,mean) ## y x ## 24.22335 49.92681 9.3 Missed values Missed values could be NA or NULL There are special values like Inf and -Inf (e.g. produced by division by zero) that are “Not a Number” or NaN NaN is NOT NA The functions to chack such values include is.na, is.nan, is.finite etc x &lt;- c(1,2,5,NA,NA) mean(x,na.rm=T) ## [1] 2.666667 sum(x,na.rm=T) ## [1] 8 is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE rm(x) # Division by zero is NOT NA 1/0 ## [1] Inf 9.4 Sorting, filtering and summarising 9.4.1 Base R Base R can do everything that dplyr does, but sometime the dplyr does is slightly easier. # Count/Tabulate table(my_data.df$f,useNA=&quot;always&quot;) ## ## A B &lt;NA&gt; ## 500 500 0 # Filtering my_data.df[my_data.df$x &gt;99,] ## y x f ## 29 59.108631 99.22415 A ## 205 -10.178437 99.11799 A ## 583 8.606938 99.63798 B ## 612 -2.040589 99.35501 B ## 615 8.904664 99.44556 B ## 655 3.271351 99.05113 B ## 706 -1.584454 99.42078 B ## 726 11.243354 99.14803 B ## 753 -26.320111 99.76585 B ## 993 -35.589642 99.40753 B # Ordering != sorting sort(my_data.df$x)[1:5] ## [1] 0.2635849 0.2772705 0.3116284 0.4554120 0.5474435 order(my_data.df$x)[1:5] ## [1] 119 128 662 643 469 head(my_data.df[order(my_data.df$x),]) ## y x f ## 119 15.177678 0.2635849 A ## 128 38.354724 0.2772705 A ## 662 26.150160 0.3116284 B ## 643 5.717829 0.4554120 B ## 469 54.586483 0.5474435 A ## 788 27.018809 0.6150180 B 9.4.2 Tables to tests In base R contingency tables are naturally connected to tests table(mtcars$vs) ## ## 0 1 ## 18 14 table(mtcars$am) ## ## 0 1 ## 19 13 table(mtcars$vs,mtcars$am) ## ## 0 1 ## 0 12 6 ## 1 7 7 fisher.test(table(mtcars$vs,mtcars$am)) # Chi Square etc ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(mtcars$vs, mtcars$am) ## p-value = 0.4727 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3825342 10.5916087 ## sample estimates: ## odds ratio ## 1.956055 x.mx &lt;- matrix(c(37,11,14,24),nrow=2) x.mx ## [,1] [,2] ## [1,] 37 14 ## [2,] 11 24 fisher.test(x.mx) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x.mx ## p-value = 0.0003348 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.047814 16.532095 ## sample estimates: ## odds ratio ## 5.63382 #table(mtcars$vs,mtcars$am,mtcars$gear) 9.5 Dplyr 9.5.1 Tidy Data concept Dplyr, ggplot and some other packages are part of the wider concept of Tidy Data : https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html I personally do not like this concept because I consider it as an additional unnecessary layer above what is already available in base R. On the other hand, I cant deny the convenience of Dplyr and ggplot. In any way, it is not possible to ignore the Tidy Data packages because they became so ubiquitous nowadays. 9.5.2 Traps in TidyData One of the stupidest thing in Tidy Data is that it refuses using Row Names, considering them somehow “untidy.” This means that many dplyr functions just silently drop the row names. Why on Earth row names are less tidy than column names?? Another possible trap is that dplyr functions may change the row order. So, the user needs either explicitly arrange the rows all the time, or, at least, pay attention to the row order when using dplyr. 9.5.3 Dplyr is BIG Here we just mention some functions to illustrate how dplyr may be used. In fact it is MUCH bigger than these small examples. See the cheat-sheet, for the beginning: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf 9.5.4 Piping Piping is a technique borrowed by dplyr from magrittr package: https://uc-r.github.io/pipe https://cran.r-project.org/web/packages/magrittr/index.html Key combination shift + ctr + M x %&gt;% f(y,z,...) is equivalent to f(x,y,z,...) 9.5.5 Dplyr examples # Summarising my_data.df %&gt;% group_by(f) %&gt;% summarise(count=n(), x_mean=mean(x), y_mean=mean(y)) ## # A tibble: 2 x 4 ## f count x_mean y_mean ## * &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 500 48.8 23.8 ## 2 B 500 51.0 24.7 # Sorting my_data.df %&gt;% arrange(x) %&gt;% head ## y x f ## 1 15.177678 0.2635849 A ## 2 38.354724 0.2772705 A ## 3 26.150160 0.3116284 B ## 4 5.717829 0.4554120 B ## 5 54.586483 0.5474435 A ## 6 27.018809 0.6150180 B # Filtering rows my_data.df %&gt;% filter(x&gt;75) %&gt;% arrange(x) %&gt;% head ## y x f ## 1 37.531548 75.14321 A ## 2 2.961144 75.35843 B ## 3 47.599322 75.36817 B ## 4 25.535710 75.43332 B ## 5 40.379314 75.54195 B ## 6 2.878849 75.70843 B # Selecting columns my_data.df %&gt;% select(y,x) %&gt;% head ## y x ## 1 -36.567660 27.67588 ## 2 2.149815 45.40984 ## 3 42.442918 81.67726 ## 4 70.871773 64.44782 ## 5 88.032996 55.38734 ## 6 42.314210 93.96441 # Piping can be done outside of the dplyr too mtcars %&gt;% head ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 "],["regression.html", "Section 10 Regression 10.1 Note 10.2 Load data 10.3 Explore data 10.4 Impute if needed 10.5 Linear modelling 10.6 lm could be evaluated by ANOVA 10.7 Predict 10.8 Update 10.9 Crooked things yet to explore …", " Section 10 Regression 10.1 Note Because of the shoterned time we only discussed examples with lm(); interface to glm() works in a very similar way - look in the references that I e-mailed to you earlier. 10.2 Load data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) 10.3 Explore data dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -36.57 2.15 42.44 70.87 88.03 ... ## $ x: num 27.7 45.4 81.7 64.4 55.4 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; head(my_data.df) ## y x f ## 1 -36.567660 27.67588 A ## 2 2.149815 45.40984 A ## 3 42.442918 81.67726 A ## 4 70.871773 64.44782 A ## 5 88.032996 55.38734 A ## 6 42.314210 93.96441 A summary(my_data.df) ## y x f ## Min. :-82.526 Min. : 0.2636 A:500 ## 1st Qu.: 5.312 1st Qu.:24.2604 B:500 ## Median : 24.084 Median :50.5506 ## Mean : 24.223 Mean :49.9268 ## 3rd Qu.: 43.284 3rd Qu.:74.5549 ## Max. :117.011 Max. :99.7659 any(is.na(my_data.df)) ## [1] FALSE ggplot(data=my_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 10.4 Impute if needed x &lt;- c(1,2,5,NA,NA) is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE mean_x &lt;- mean(x, na.rm = T) mean_x -&gt; x[is.na(x)] x ## [1] 1.000000 2.000000 5.000000 2.666667 2.666667 rm(x) 10.5 Linear modelling 10.5.1 Single term model_1 &lt;- lm(y~x, data=my_data.df) summary(model_1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.859 -18.877 -0.155 19.047 92.704 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.366788 1.822824 13.368 &lt;2e-16 *** ## x -0.002873 0.031576 -0.091 0.928 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.94 on 998 degrees of freedom ## Multiple R-squared: 8.296e-06, Adjusted R-squared: -0.0009937 ## F-statistic: 0.008279 on 1 and 998 DF, p-value: 0.9275 plot(model_1) plot(y~x,data=my_data.df) abline(model_1, col=&quot;red&quot;) 10.5.2 Multiple terms without interaction model_2 &lt;- lm(y~x+f, data=my_data.df) #equivalent: note using dot for ALL terms model_2_dot &lt;- lm(y~., data=my_data.df) summary(model_2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.424 -19.038 -0.155 19.018 92.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.938959 2.014110 11.89 &lt;2e-16 *** ## x -0.003483 0.031611 -0.11 0.912 ## fB 0.916567 1.832264 0.50 0.617 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.95 on 997 degrees of freedom ## Multiple R-squared: 0.0002592, Adjusted R-squared: -0.001746 ## F-statistic: 0.1293 on 2 and 997 DF, p-value: 0.8788 summary(model_2_dot) ## ## Call: ## lm(formula = y ~ ., data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.424 -19.038 -0.155 19.018 92.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.938959 2.014110 11.89 &lt;2e-16 *** ## x -0.003483 0.031611 -0.11 0.912 ## fB 0.916567 1.832264 0.50 0.617 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.95 on 997 degrees of freedom ## Multiple R-squared: 0.0002592, Adjusted R-squared: -0.001746 ## F-statistic: 0.1293 on 2 and 997 DF, p-value: 0.8788 plot(model_2) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_2, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_2, col = &quot;blue&quot;, lwd = 3): only using the first two of 3 ## regression coefficients 10.5.3 With interaction model_3 &lt;- lm(y~x*f, data=my_data.df) summary(model_3) ## ## Call: ## lm(formula = y ~ x * f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -87.806 -16.348 -0.071 16.529 81.139 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.62263 2.24074 -0.278 0.781 ## x 0.49974 0.03964 12.606 &lt;2e-16 *** ## fB 49.61632 3.18525 15.577 &lt;2e-16 *** ## x:fB -0.97610 0.05521 -17.679 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.27 on 996 degrees of freedom ## Multiple R-squared: 0.2391, Adjusted R-squared: 0.2368 ## F-statistic: 104.3 on 3 and 996 DF, p-value: &lt; 2.2e-16 plot(model_3) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_3, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_3, col = &quot;blue&quot;, lwd = 3): only using the first two of 4 ## regression coefficients 10.6 lm could be evaluated by ANOVA https://stats.stackexchange.com/questions/115304/interpreting-output-from-anova-when-using-lm-as-input Anova output may be more clear when categorical variables have more than 3 levels (we discussed it during the practical session) Note that there are at least two slightly different functions for ANOVA in R: anova() and aov() anova(model_1) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 7 6.93 0.0083 0.9275 ## Residuals 998 835745 837.42 anova(model_2) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 7 6.93 0.0083 0.9275 ## f 1 210 209.71 0.2502 0.6170 ## Residuals 997 835535 838.05 anova(model_3) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 7 7 0.0109 0.9170 ## f 1 210 210 0.3284 0.5667 ## x:f 1 199573 199573 312.5577 &lt;2e-16 *** ## Residuals 996 635962 639 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.1 Anova could be used to compare models This can be used in the following way: First a “null” model could be calculated with all the “confounders”/“covariates” Then a predictor of interest is added to the model and models are compared by ANOVA If the model is improved significantly, then the predictor is important anova(model_1,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 835745 ## 2 996 635962 2 199783 156.44 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model_1,model_2,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x + f ## Model 3: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 835745 ## 2 997 835535 1 210 0.3284 0.5667 ## 3 996 635962 1 199573 312.5577 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.2 A bit of OOP about anova(lm) # What other methods could be applied to lm object? class(model_1) ## [1] &quot;lm&quot; methods(class=&quot;lm&quot;) ## [1] add1 alias anova case.names coerce ## [6] confint cooks.distance deviance dfbeta dfbetas ## [11] drop1 dummy.coef effects extractAIC family ## [16] formula fortify hatvalues influence initialize ## [21] kappa labels logLik model.frame model.matrix ## [26] nobs plot predict print proj ## [31] qqnorm qr residuals rstandard rstudent ## [36] show simulate slotsFromS3 summary variable.names ## [41] vcov ## see &#39;?methods&#39; for accessing help and source code # What other classes coul be evaluated by ANOVA? methods(anova) ## [1] anova.gam* anova.glm* anova.glmlist* anova.gls* anova.lm* ## [6] anova.lme* anova.lmlist* anova.loess* anova.mlm* anova.nls* ## see &#39;?methods&#39; for accessing help and source code #?aov #?anova #?anova.lm 10.7 Predict new_data.df &lt;- data.frame(x=c(10,20), f=factor(c(&quot;A&quot;,&quot;B&quot;))) new_data.df ## x f ## 1 10 A ## 2 20 B predict(model_1,new_data.df) ## 1 2 ## 24.33806 24.30933 predict(model_2,new_data.df) ## 1 2 ## 23.90413 24.78587 10.8 Update It is very rarely used, but occasionally could be seen in help and tutorials Note using dots on the left and right of ~ in the formula m1 = lm(y~x, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.859 -18.877 -0.155 19.047 92.704 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.366788 1.822824 13.368 &lt;2e-16 *** ## x -0.002873 0.031576 -0.091 0.928 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.94 on 998 degrees of freedom ## Multiple R-squared: 8.296e-06, Adjusted R-squared: -0.0009937 ## F-statistic: 0.008279 on 1 and 998 DF, p-value: 0.9275 m2 = update(m1,.~.+f, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.424 -19.038 -0.155 19.018 92.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.938959 2.014110 11.89 &lt;2e-16 *** ## x -0.003483 0.031611 -0.11 0.912 ## fB 0.916567 1.832264 0.50 0.617 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.95 on 997 degrees of freedom ## Multiple R-squared: 0.0002592, Adjusted R-squared: -0.001746 ## F-statistic: 0.1293 on 2 and 997 DF, p-value: 0.8788 10.9 Crooked things yet to explore … We could not discuss these because we were short of time (and I have limited understanding of them ?) 10.9.1 Intercept, design matrix See my PowerPoint slides for students with biological background Intercept could be set to zero by two ways: y ~ 0 + x y ~ x - 1 Something strange happens without intercept in the model. This something could be undertood by staring at “design matrix” m1 &lt;- lm(y~x+f, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.424 -19.038 -0.155 19.018 92.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.938959 2.014110 11.89 &lt;2e-16 *** ## x -0.003483 0.031611 -0.11 0.912 ## fB 0.916567 1.832264 0.50 0.617 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.95 on 997 degrees of freedom ## Multiple R-squared: 0.0002592, Adjusted R-squared: -0.001746 ## F-statistic: 0.1293 on 2 and 997 DF, p-value: 0.8788 m2 &lt;- lm(y~x+f-1, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f - 1, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.424 -19.038 -0.155 19.018 92.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x -0.003483 0.031611 -0.11 0.912 ## fA 23.938959 2.014110 11.89 &lt;2e-16 *** ## fB 24.855526 2.068753 12.02 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.95 on 997 degrees of freedom ## Multiple R-squared: 0.4126, Adjusted R-squared: 0.4109 ## F-statistic: 233.5 on 3 and 997 DF, p-value: &lt; 2.2e-16 m3 &lt;- lm(y~0+x+f, data=my_data.df) summary(m3) ## ## Call: ## lm(formula = y ~ 0 + x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.424 -19.038 -0.155 19.018 92.228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x -0.003483 0.031611 -0.11 0.912 ## fA 23.938959 2.014110 11.89 &lt;2e-16 *** ## fB 24.855526 2.068753 12.02 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.95 on 997 degrees of freedom ## Multiple R-squared: 0.4126, Adjusted R-squared: 0.4109 ## F-statistic: 233.5 on 3 and 997 DF, p-value: &lt; 2.2e-16 data(&quot;mtcars&quot;) other_data &lt;- mtcars[1:10,c(&quot;vs&quot;,&quot;am&quot;,&quot;gear&quot;)] other_data ## vs am gear ## Mazda RX4 0 1 4 ## Mazda RX4 Wag 0 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 0 0 3 ## Valiant 1 0 3 ## Duster 360 0 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 model.matrix(~am+gear, data=other_data) ## (Intercept) am gear ## Mazda RX4 1 1 4 ## Mazda RX4 Wag 1 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 1 0 3 ## Valiant 1 0 3 ## Duster 360 1 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 model.matrix(~0+am+gear, data=other_data) ## am gear ## Mazda RX4 1 4 ## Mazda RX4 Wag 1 4 ## Datsun 710 1 4 ## Hornet 4 Drive 0 3 ## Hornet Sportabout 0 3 ## Valiant 0 3 ## Duster 360 0 3 ## Merc 240D 0 4 ## Merc 230 0 4 ## Merc 280 0 4 ## attr(,&quot;assign&quot;) ## [1] 1 2 10.9.2 Contrasts Also, there is some cryptic way allowing to specify what sub-groups to include/exclude in the comparison: this is something to do with “contrasts” https://rcompanion.org/rcompanion/h_01.html https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r contrasts(my_data.df$f) ## B ## A 0 ## B 1 contrasts(as.factor(other_data$gear)) ## 4 ## 3 0 ## 4 1 10.9.3 Type I, II and III sums ANOVA and linear models are, in fact equivalent to each other (even at the level of implementation). Both rely on calculating some sum of squares. There are 3 ways of calculating these sums: Type I, Type II and Type III Google about these … https://rcompanion.org/rcompanion/d_04.html "],["real-life-lm-example.html", "Section 11 Real life lm() example 11.1 Summary 11.2 Start section 11.3 Read data 11.4 Explore and prepare data 11.5 Explore in a univariate manner 11.6 Model with all variables 11.7 Drop1 11.8 Optimal model 11.9 Predict 11.10 End section", " Section 11 Real life lm() example 11.1 Summary This is only the first orientation with the dataset. This example: Reads data Explores missingnes and does simplistic imputation Looks at univariate regression/anova for each variable: all look highly significant Multivariate model with all variables: suggests dropping owners drop1 suggests keeping only make, engine.size and mileage Importantly, the example does NOT analyze the normality of residuals in the models etc. It does NOT considers interactions. Further directions may also include: transformation of variables using generalized models using add1 estimating accuracy of prediction etc. 11.2 Start section 11.3 Read data data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) rm(data_file) 11.4 Explore and prepare data Data clean-up and preparation is a separate large realm in data science. Here we just check that data look well structured, check the data size, types of variables, missingness. 11.4.1 Overall size and structure dim(data.df) ## [1] 3780 6 str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... 11.4.2 Missingness count.na.udf &lt;- function(x)(sum(is.na(x))) count_na &lt;- apply(data.df,2,count.na.udf) count_na ## mileage manufactured.year engine.size owners ## 0 0 2 1968 ## price make ## 0 0 count_na/nrow(data.df) ## mileage manufactured.year engine.size owners ## 0.0000000000 0.0000000000 0.0005291005 0.5206349206 ## price make ## 0.0000000000 0.0000000000 rm(count.na.udf,count_na) 11.4.3 Impute missed Its a simplistic procedure, with its pros and contras. Strictly speaking, it may not be necessary if lm() has its own imputation in place. However, it may not be imediately clear what exactly this default implicit procedure is … So, it may be useful to deal with missed data explicitly to be sure how they were treated. On a technical point: I recollect that there could be a specialized function for applying mean in place of missed data. mean_engine_size &lt;- mean(data.df$engine.size, na.rm=T) mean_engine_size -&gt; data.df$engine.size[is.na(data.df$engine.size)] mean_owners &lt;- mean(data.df$owners, na.rm=T) mean_owners -&gt; data.df$owners[is.na(data.df$owners)] any(is.na(data.df)) ## [1] FALSE rm(mean_owners, mean_engine_size) 11.4.4 Calculate age Our previous knowledge in the field suggests that age may be a better representation for manufactured.year Should manufactured.year be removed from analysis after making age? May including both variables confuse the model (because of collinearuty). data.df &lt;- data.frame(data.df,age=2021-data.df$manufactured.year) str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 7 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : num 2.46 3 6 2.46 1 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... ## $ age : num 6 9 20 11 8 17 18 9 8 9 ... 11.5 Explore in a univariate manner 11.5.1 Anova for categorical variable makes data.df %&gt;% group_by(make) %&gt;% summarise(count=n(), mean_price=mean(price), mean_mileage=mean(mileage), mean_age=mean(age), mean_price=mean(price), mean_owners=mean(owners), mean_engine=mean(engine.size)) %&gt;% arrange(desc(count)) ## # A tibble: 43 x 7 ## make count mean_price mean_mileage mean_age mean_owners mean_engine ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vauxhall 715 6188. 44018. 6.09 2.36 1.42 ## 2 Ford 286 6502 52934. 7.21 2.46 1.34 ## 3 Nissan 269 8673. 39307. 5.90 2.31 1.33 ## 4 Hyundai 250 8521 31668. 4.35 2.44 1.30 ## 5 Volkswagen 208 5318. 74174. 10.4 3.03 1.51 ## 6 Toyota 199 6305. 48149. 8.34 2.10 1.55 ## 7 Peugeot 193 6488. 43657. 6.38 2.41 1.39 ## 8 Citroen 165 4882. 53608. 8.18 2.40 1.44 ## 9 Renault 164 6506. 41006. 6.68 2.37 1.34 ## 10 SEAT 156 6342. 53444. 6.63 2.25 1.40 ## # … with 33 more rows boxplot(price~make, data=data.df) summary(aov(price~make, data=data.df)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## make 42 1.171e+10 278725988 21.38 &lt;2e-16 *** ## Residuals 3737 4.871e+10 13035655 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.5.2 Univariate lm() for numeric variables There is a way of doing this in one step … 11.5.2.1 mileage mileage.lm &lt;- lm(price~mileage, data=data.df) summary(mileage.lm) ## ## Call: ## lm(formula = price ~ mileage, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6878.8 -1971.3 -661.8 1175.8 29729.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.780e+03 9.264e+01 105.57 &lt;2e-16 *** ## mileage -5.909e-02 1.469e-03 -40.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3346 on 3778 degrees of freedom ## Multiple R-squared: 0.2999, Adjusted R-squared: 0.2997 ## F-statistic: 1618 on 1 and 3778 DF, p-value: &lt; 2.2e-16 mileage.lm.p=coefficients(summary(mileage.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~mileage, data=data.df, main=paste(&quot;Univariate mileage p =&quot;,round(mileage.lm.p,5))) abline(mileage.lm, col=&quot;red&quot;, lwd=3) rm(mileage.lm,mileage.lm.p) 11.5.2.2 age age.lm &lt;- lm(price~age, data=data.df) summary(age.lm) ## ## Call: ## lm(formula = price ~ age, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4784 -1868 -717 898 36767 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11038.23 95.84 115.18 &lt;2e-16 *** ## age -584.32 11.21 -52.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3050 on 3778 degrees of freedom ## Multiple R-squared: 0.4183, Adjusted R-squared: 0.4182 ## F-statistic: 2717 on 1 and 3778 DF, p-value: &lt; 2.2e-16 age.lm.p=coefficients(summary(age.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~age, data=data.df, main=paste(&quot;Univariate age p =&quot;,round(age.lm.p,5))) abline(age.lm, col=&quot;red&quot;, lwd=3) rm(age.lm,age.lm.p) 11.5.2.3 owners owners.lm &lt;- lm(price~owners, data=data.df) summary(owners.lm) ## ## Call: ## lm(formula = price ~ owners, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7583.5 -2683.5 -343.3 1862.9 31164.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9651.66 161.15 59.89 &lt;2e-16 *** ## owners -1173.18 60.43 -19.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3813 on 3778 degrees of freedom ## Multiple R-squared: 0.09071, Adjusted R-squared: 0.09047 ## F-statistic: 376.9 on 1 and 3778 DF, p-value: &lt; 2.2e-16 owners.lm.p=coefficients(summary(owners.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~owners, data=data.df, main=paste(&quot;Univariate owners p =&quot;,round(owners.lm.p,5))) abline(owners.lm, col=&quot;red&quot;, lwd=3) rm(owners.lm,owners.lm.p) 11.5.2.4 engine.size engine.size.lm &lt;- lm(price~engine.size, data=data.df) summary(engine.size.lm) ## ## Call: ## lm(formula = price ~ engine.size, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7582.8 -2853.8 -75.1 2179.3 26531.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4682.0 231.5 20.224 &lt;2e-16 *** ## engine.size 1393.1 148.8 9.362 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3954 on 3778 degrees of freedom ## Multiple R-squared: 0.02267, Adjusted R-squared: 0.02241 ## F-statistic: 87.65 on 1 and 3778 DF, p-value: &lt; 2.2e-16 engine.size.lm.p=coefficients(summary(engine.size.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~engine.size, data=data.df, main=paste(&quot;Univariate engine.size p =&quot;,round(engine.size.lm.p,5))) abline(engine.size.lm, col=&quot;red&quot;, lwd=3) rm(engine.size.lm,engine.size.lm.p) 11.6 Model with all variables It seems that owners are not very informative: could it be because we imputed half of them full.m &lt;- lm(price~., data=data.df) #summary(full.m) anova(full.m) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.8708 &lt;2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.3510 &lt;2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.5751 &lt;2e-16 *** ## owners 1 1.0695e+06 1.0695e+06 0.2467 0.6195 ## make 42 6.9638e+09 1.6580e+08 38.2404 &lt;2e-16 *** ## Residuals 3733 1.6186e+10 4.3359e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.7 Drop1 I think that drop1 suggests to keep only make, engine.size, mileage https://stats.stackexchange.com/questions/4639/interpreting-the-drop1-output-in-r drop1.m &lt;- drop1(full.m, test=&quot;F&quot;) #drop1.m drop1.m[order(drop1.m$`Pr(&gt;F)`),] ## Single term deletions ## ## Model: ## price ~ mileage + manufactured.year + engine.size + owners + ## make + age ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## make 42 6963803785 2.3150e+10 59083 38.2404 &lt;2e-16 *** ## engine.size 1 3549033541 1.9735e+10 58562 818.5316 &lt;2e-16 *** ## mileage 1 1900987496 1.8087e+10 58232 438.4344 &lt;2e-16 *** ## owners 1 4803073 1.6191e+10 57813 1.1078 0.2926 ## &lt;none&gt; 1.6186e+10 57814 ## manufactured.year 0 0 1.6186e+10 57814 ## age 0 0 1.6186e+10 57814 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 rm(full.m,drop1.m) 11.8 Optimal model optimal.m &lt;- lm(price~make + engine.size + mileage, data=data.df) anova(optimal.m) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## make 42 1.1706e+10 2.7873e+08 37.793 &lt; 2.2e-16 *** ## engine.size 1 1.1660e+08 1.1660e+08 15.810 7.136e-05 *** ## mileage 1 2.1052e+10 2.1052e+10 2854.492 &lt; 2.2e-16 *** ## Residuals 3735 2.7546e+10 7.3750e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.9 Predict new_data.df &lt;- list(make=c(&quot;Citroen&quot;,&quot;Audi&quot;,&quot;Mercedes-Benz&quot;), engine.size=c(1,1.5,3), mileage=c(70000,50000,10000)) predicted_price &lt;- predict(optimal.m,newdata = new_data.df) new_data.df &lt;- data.frame(new_data.df, predicted_price) new_data.df ## make engine.size mileage predicted_price ## 1 Citroen 1.0 70000 2653.858 ## 2 Audi 1.5 50000 10086.041 ## 3 Mercedes-Benz 3.0 10000 15371.881 rm(predicted_price) 11.10 End section #ls() sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] dplyr_1.0.4 plotly_4.9.3 ggplot2_3.3.3 ## ## loaded via a namespace (and not attached): ## [1] reticulate_1.18 tidyselect_1.1.0 xfun_0.21 purrr_0.3.4 ## [5] splines_3.6.2 lattice_0.20-41 colorspace_2.0-0 vctrs_0.3.6 ## [9] generics_0.1.0 htmltools_0.5.1.1 viridisLite_0.3.0 yaml_2.2.1 ## [13] mgcv_1.8-33 utf8_1.1.4 rlang_0.4.10 pillar_1.4.7 ## [17] glue_1.4.2 withr_2.4.1 DBI_1.1.1 lifecycle_1.0.0 ## [21] stringr_1.4.0 munsell_0.5.0 gtable_0.3.0 htmlwidgets_1.5.3 ## [25] evaluate_0.14 labeling_0.4.2 knitr_1.31 crosstalk_1.1.1 ## [29] fansi_0.4.2 highr_0.8 Rcpp_1.0.6 scales_1.1.1 ## [33] jsonlite_1.7.2 farver_2.0.3 digest_0.6.27 stringi_1.5.3 ## [37] bookdown_0.21 grid_3.6.2 cli_2.3.0 tools_3.6.2 ## [41] magrittr_2.0.1 lazyeval_0.2.2 tibble_3.0.6 crayon_1.4.1 ## [45] tidyr_1.1.2 pkgconfig_2.0.3 ellipsis_0.3.1 Matrix_1.3-2 ## [49] data.table_1.13.6 assertthat_0.2.1 rmarkdown_2.6 httr_1.4.2 ## [53] rstudioapi_0.13 R6_2.5.0 nlme_3.1-152 compiler_3.6.2 "]]
