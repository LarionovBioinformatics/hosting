[["index.html", "I forgot the title Section 1 My great Math LaTex 1.1 Rendering to PDF and EPUB 1.2 Sharing on social media", " I forgot the title Of course, its Me 2021-03-23 Section 1 My great Math LaTex This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\[{{a^2 + b^2} \\over { \\sum{y} \\iff \\int{z \\over d^2}}} \\ne c^2\\]. https://www.math.uci.edu/~xiangwen/pdf/LaTeX-Math-Symbols.pdf 1.1 Rendering to PDF and EPUB Although PDF and EPUB rendering is a default feature of bookdown, it is suppressed in this example to illustrate interactive features of Plotly 1.2 Sharing on social media I do not like these media ! I kept it here just to illustrate this feature of bookdown "],["r-markdown.html", "Section 2 R-Markdown 2.1 2nd level header 2.2 Not only R … 2.3 Back to R", " Section 2 R-Markdown 2.1 2nd level header 2.1.1 3rd level header etc 2.2 Not only R … 2.2.1 Python Just use chunk with option “python” Requires Reticulate and Python (of course :) DOES NOT WORK with bookdown (and gives a very obscure error mesage !) print(&quot;hello world&quot;) a=5 print(a) 2.2.2 Bash Just use chunk with option “bash” echo BlaBlaBla ## BlaBlaBla 2.3 Back to R 2.3.1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: data(&quot;cars&quot;) summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 2.3.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["a-simple-analysis.html", "Section 3 A simple analysis 3.1 In-built datasets 3.2 Explore data 3.3 T-test 3.4 Liear model", " Section 3 A simple analysis library(ggplot2) 3.1 In-built datasets # Inbuilt dataset data(&quot;mtcars&quot;) # Explore data class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 dim(mtcars) ## [1] 32 11 3.2 Explore data mpg vs hp cosiderig vs as covariate 3.2.1 Base R plottig plot(mpg~hp, data=mtcars, pch=vs, main=&quot;My title&quot;) legend(&quot;topright&quot;, c(&quot;vs=0&quot;,&quot;vs=1&quot;), pch=c(0,1)) 3.2.2 ggplot style ggplot(data=mtcars,aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point() + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.3 T-test plot(mpg~vs, data=mtcars) boxplot(mpg~vs, data=mtcars) t.test(mpg~vs, data=mtcars) ## ## Welch Two Sample t-test ## ## data: mpg by vs ## t = -4.6671, df = 22.716, p-value = 0.0001098 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.462508 -4.418445 ## sample estimates: ## mean in group 0 mean in group 1 ## 16.61667 24.55714 3.4 Liear model 3.4.1 mpg ~ hp my_model_1 &lt;- lm(mpg~hp, data=mtcars) class(my_model_1) ## [1] &quot;lm&quot; summary(my_model_1) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 plot(mpg~hp, data=mtcars) abline(my_model_1) plot(my_model_1) 3.4.2 mpg ~ vs + hp my_model_2 &lt;- lm(mpg ~ vs + hp, data=mtcars) summary(my_model_2) ## ## Call: ## lm(formula = mpg ~ vs + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7131 -2.3336 -0.1332 1.9055 7.9055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.96300 2.89069 9.328 3.13e-10 *** ## vs 2.57622 1.96966 1.308 0.201163 ## hp -0.05453 0.01448 -3.766 0.000752 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.818 on 29 degrees of freedom ## Multiple R-squared: 0.6246, Adjusted R-squared: 0.5987 ## F-statistic: 24.12 on 2 and 29 DF, p-value: 6.768e-07 plot(my_model_2) 3.4.3 mpg ~ vs * hp my_model_3 &lt;- lm(mpg ~ vs * hp, data=mtcars) summary(my_model_3) ## ## Call: ## lm(formula = mpg ~ vs * hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5821 -1.7710 -0.3612 1.5969 9.2646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.49637 2.73893 8.944 1.07e-09 *** ## vs 14.50418 4.58160 3.166 0.00371 ** ## hp -0.04153 0.01379 -3.011 0.00547 ** ## vs:hp -0.11657 0.04130 -2.822 0.00868 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.428 on 28 degrees of freedom ## Multiple R-squared: 0.7077, Adjusted R-squared: 0.6764 ## F-statistic: 22.6 on 3 and 28 DF, p-value: 1.227e-07 plot(my_model_3) "],["data-types.html", "Section 4 Data types 4.1 Vectors 4.2 Numeric matrices 4.3 Recycling in element-wise operations 4.4 Factors: categorical variale 4.5 Lists 4.6 Dataframe: list of vectors/factors", " Section 4 Data types Be careful with changing folders in bookdown !! setwd(&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021&quot;) 4.1 Vectors 4.1.1 Numeric vectors a &lt;- 1:5 class(a) ## [1] &quot;integer&quot; a ## [1] 1 2 3 4 5 b &lt;- seq(from=10, to=50, by=10) class(b) ## [1] &quot;numeric&quot; b ## [1] 10 20 30 40 50 # Vectorised operation: element-wise summation a + b ## [1] 11 22 33 44 55 4.1.2 Vector elements may have names Names is a property attributes(a) ## NULL names(a) &lt;- c(&quot;e1&quot;,&quot;e2&quot;,&quot;e3&quot;,&quot;e4&quot;,&quot;e5&quot;) a ## e1 e2 e3 e4 e5 ## 1 2 3 4 5 attributes(a) ## $names ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; a[c(1,2,4)] ## e1 e2 e4 ## 1 2 4 a[c(T,T,F,T,F)] ## e1 e2 e4 ## 1 2 4 a[c(&quot;e1&quot;,&quot;e2&quot;,&quot;e4&quot;)] ## e1 e2 e4 ## 1 2 4 4.2 Numeric matrices m1 &lt;- matrix(1:25, nrow=5) m1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 m2 &lt;- matrix(rep(10,25), nrow=5) m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 10 10 10 10 ## [2,] 10 10 10 10 10 ## [3,] 10 10 10 10 10 ## [4,] 10 10 10 10 10 ## [5,] 10 10 10 10 10 t(m1) # Transposition ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 ## [5,] 21 22 23 24 25 m1 + m2 # Element-wise summation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 16 21 26 31 ## [2,] 12 17 22 27 32 ## [3,] 13 18 23 28 33 ## [4,] 14 19 24 29 34 ## [5,] 15 20 25 30 35 m1 * m2 # Element-wise multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 60 110 160 210 ## [2,] 20 70 120 170 220 ## [3,] 30 80 130 180 230 ## [4,] 40 90 140 190 240 ## [5,] 50 100 150 200 250 m1 %*% m2 # Matrix multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 550 550 550 550 550 ## [2,] 600 600 600 600 600 ## [3,] 650 650 650 650 650 ## [4,] 700 700 700 700 700 ## [5,] 750 750 750 750 750 4.2.1 Matrices may have rownames and colnames rownames(m1) &lt;- c(&quot;s1&quot;,&quot;s2&quot;,&quot;s3&quot;,&quot;s4&quot;,&quot;s5&quot;) colnames(m1) &lt;- c(&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;g5&quot;) m1 ## g1 g2 g3 g4 g5 ## s1 1 6 11 16 21 ## s2 2 7 12 17 22 ## s3 3 8 13 18 23 ## s4 4 9 14 19 24 ## s5 5 10 15 20 25 m1[c(&quot;s2&quot;,&quot;s3&quot;),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 m1[c(F,T,T,F,F),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 4.3 Recycling in element-wise operations Shorter vector is recycled along the longer oject Matrices are processed as vectors in column-after-column way c &lt;- 1:3 c ## [1] 1 2 3 b + c ## Warning in b + c: longer object length is not a multiple of shorter object ## length ## [1] 11 22 33 41 52 c + m2 ## Warning in c + m2: longer object length is not a multiple of shorter object ## length ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 13 12 11 13 ## [2,] 12 11 13 12 11 ## [3,] 13 12 11 13 12 ## [4,] 11 13 12 11 13 ## [5,] 12 11 13 12 11 a + m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 m2 + a ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 rm(a,b,c,m1,m2) 4.4 Factors: categorical variale Categorical != character v &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) class(v) ## [1] &quot;character&quot; v ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; f &lt;- factor(v) class(f) ## [1] &quot;factor&quot; f ## [1] A B C ## Levels: A B C f &lt;- factor(v, levels=c(&quot;C&quot;,&quot;B&quot;,&quot;A&quot;)) f ## [1] A B C ## Levels: C B A f &lt;- factor(v, levels=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;)) f ## [1] &lt;NA&gt; B C ## Levels: B C D f &lt;- factor(v, levels=LETTERS) f ## [1] A B C ## Levels: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z f &lt;- factor(v, levels=LETTERS, ordered = T) f ## [1] A B C ## 26 Levels: A &lt; B &lt; C &lt; D &lt; E &lt; F &lt; G &lt; H &lt; I &lt; J &lt; K &lt; L &lt; M &lt; N &lt; O &lt; ... &lt; Z # Potential confusion factor(1:5, levels=c(4,5,3,2,1), ordered=T) ## [1] 1 2 3 4 5 ## Levels: 4 &lt; 5 &lt; 3 &lt; 2 &lt; 1 rm(v,f) 4.5 Lists Collection of ojects of different types my.list &lt;- list(c(1,2),&quot;A&quot;,T ) my.list ## [[1]] ## [1] 1 2 ## ## [[2]] ## [1] &quot;A&quot; ## ## [[3]] ## [1] TRUE my.list[1] ## [[1]] ## [1] 1 2 my.list[[1]][1] ## [1] 1 my.list[[1]][2] ## [1] 2 my.list[2][1] ## [[1]] ## [1] &quot;A&quot; my.list &lt;- list(lel1=c(1,2), lel2=&quot;A&quot;, lel3=T ) my.list ## $lel1 ## [1] 1 2 ## ## $lel2 ## [1] &quot;A&quot; ## ## $lel3 ## [1] TRUE my.list$lel1 ## [1] 1 2 my.list$lel1[2] ## [1] 2 rm(my.list) 4.6 Dataframe: list of vectors/factors #data(&quot;mtcars&quot;) class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 #data(&quot;mpg&quot;) #class(mpg) #str(mpg) my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA)) class(my.df) ## [1] &quot;data.frame&quot; my.df ## first_name second_name age income ## 1 AL Smith 15 100 ## 2 AL Larionov 23 200 ## 3 BS Bob 1 NA str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: Factor w/ 3 levels &quot;Bob&quot;,&quot;Larionov&quot;,..: 3 2 1 ## $ age : num 15 23 1 ## $ income : num 100 200 NA my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA), stringsAsFactors=F) str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: chr &quot;Smith&quot; &quot;Larionov&quot; &quot;Bob&quot; ## $ age : num 15 23 1 ## $ income : num 100 200 NA "],["plotly.html", "Section 5 Plotly 5.1 Reference 5.2 Load library 5.3 Get some data 5.4 Plotly magic", " Section 5 Plotly 5.1 Reference https://plotly-r.com/index.html 5.2 Load library library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 5.3 Get some data # Inbuilt dataset #data(&quot;mtcars&quot;) mtcars &lt;- data.frame(mtcars, make=rownames(mtcars), stringsAsFactors = F) # Explore data dim(mtcars) ## [1] 32 12 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 12 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... ## $ make: chr &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... 5.4 Plotly magic d &lt;- ggplot(data=mtcars, aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point(aes(text=make)) + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## Warning: Ignoring unknown aesthetics: text d ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplotly(d, tooltip=&quot;text&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # A base R plot ... plot(mpg~hp, data=mtcars) "],["distributionns.html", "Section 6 Distributionns 6.1 Function curve() 6.2 Normal distribution 6.3 Gamma 6.4 Beta", " Section 6 Distributionns 6.1 Function curve() curve(sin(x),0,2*pi) curve(cos(x),from=0,to=2*pi,add=T,col=&quot;red&quot;,lty=2) 6.2 Normal distribution dnorm(-1.96, mean=0, sd=1) ## [1] 0.05844094 dnorm(-1.96, mean=10, sd=7) ## [1] 0.01324074 curve(dnorm(x),-3,3) curve(dnorm(x, 10, 3),0,20) curve(pnorm(x, 10, 3),0,20) curve(dnorm(x),-4,4) abline(v=qnorm(0.025), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.025, lower.tail = F), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.005), col=&quot;blue&quot;, lty=2) abline(v=qnorm(0.005, lower.tail = F), col=&quot;blue&quot;, lty=2) curve(qnorm(x)) rnorm(5) ## [1] 0.2512001 2.0635402 0.3118581 -1.0877817 0.3427476 hist(rnorm(1000),freq = F,ylim=c(0,0.5)) curve(dnorm(x),-4,4,add=T,col=&quot;blue&quot;) dh &lt;- c(rnorm(1000,75,50),rnorm(1000,180,15)) hist(dh,100,200) 6.3 Gamma curve(dgamma(x,2,6)) curve(dgamma(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;g(2,6)&quot;,&quot;g(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dgamma(x,3,7),0,2) abline(v=qgamma(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qgamma(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rgamma(1000,3,7),freq = F, add=T, border=&quot;green&quot;) 6.4 Beta curve(dbeta(x,2,6)) curve(dbeta(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;b(2,6)&quot;,&quot;b(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dbeta(x,3,7)) abline(v=qbeta(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qbeta(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rbeta(1000,3,7),freq = F, add=T, border=&quot;green&quot;) "],["package.html", "Section 7 Package 7.1 User-defined fnction 7.2 Recursive functions are allowed 7.3 Lets add some data 7.4 Prepare template for package source folder", " Section 7 Package 7.1 User-defined fnction mff &lt;- function(a,b){ return(a+b) } mff(3,5) ## [1] 8 #mff(3,&quot;Good&quot;) #mff(&quot;a&quot;,&quot;b&quot;) 7.2 Recursive functions are allowed my_sum &lt;- function(n){ result &lt;- 0 for(i in 1:n){ result &lt;- result + i } return(result) } my_sum(10) ## [1] 55 my_rsum &lt;- function(n){ if(n&gt;1){ return(n+my_rsum(n-1)) } if(n==1){ return(n) } } my_rsum(10) ## [1] 55 7.3 Lets add some data my_great_dataset &lt;- data.frame(names=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), values=c(1,2,3,4)) 7.4 Prepare template for package source folder ls() getwd() package.skeleton(name=&quot;ab&quot;,list=ls()) A bush chunk to build: cd /Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021 R CMD build ab R CMD check ab_1.0.tar.gz Follow the instructions for minimal required documentation, i.e.: Titles (&amp; Descriptions?) in each object Rd Examples (in the package Rd) Then install, load and enjoy ! #install.packages(...) library(ab) data(my_great_dataset) "],["data-reading-writing.html", "Section 8 Data reading / writing 8.1 A simple simulated dataset 8.2 Saving and reading", " Section 8 Data reading / writing library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 8.1 A simple simulated dataset x &lt;- runif(1000, 0, 100) hist(x) f &lt;- factor(c(rep(&quot;A&quot;,500),rep(&quot;B&quot;,500))) head(f) ## [1] A A A A A A ## Levels: A B tail(f) ## [1] B B B B B B ## Levels: A B y1 &lt;- 0.5 * x[1:500] + rnorm(500,0,25) y2 &lt;- -0.5 * x[501:1000] + rnorm(500,0,25) y &lt;- c(y1,y2+50) sim_data.df &lt;- data.frame(y,x,f) str(sim_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -4.036 -10.088 -0.953 30.371 21.234 ... ## $ x: num 7.49 6.52 2.84 12.41 46.83 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... plot(y~x, pch=as.integer(f)) ggplot(data=sim_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 8.2 Saving and reading … at last ! 8.2.1 RData save.image(file=&quot;results1.RData&quot;) rm(list=ls()) load(&quot;results1.RData&quot;) save(&quot;sim_data.df&quot;,file=&quot;results2.RData&quot;) rm(list=ls()) load(&quot;results2.RData&quot;) 8.2.2 Text files write.table(sim_data.df,file=&quot;results.tsv&quot;, quote=F,sep=&quot;\\t&quot;,row.names = F) rm(list=ls()) my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -4.036 -10.088 -0.953 30.371 21.234 ... ## $ x: num 7.49 6.52 2.84 12.41 46.83 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-58.764 Min. : 0.04665 A:500 ## 1st Qu.: 3.804 1st Qu.:24.24327 B:500 ## Median : 22.788 Median :49.66040 ## Mean : 23.540 Mean :49.60082 ## 3rd Qu.: 42.603 3rd Qu.:74.29039 ## Max. :107.160 Max. :99.92659 "],["data-exploring-modifying.html", "Section 9 Data exploring / modifying 9.1 Read data 9.2 Apply (summary) functions over margin 9.3 Missed values 9.4 Sorting, filtering and summarising 9.5 Dplyr", " Section 9 Data exploring / modifying library(ggplot2) library(dplyr) 9.1 Read data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -4.036 -10.088 -0.953 30.371 21.234 ... ## $ x: num 7.49 6.52 2.84 12.41 46.83 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-58.764 Min. : 0.04665 A:500 ## 1st Qu.: 3.804 1st Qu.:24.24327 B:500 ## Median : 22.788 Median :49.66040 ## Mean : 23.540 Mean :49.60082 ## 3rd Qu.: 42.603 3rd Qu.:74.29039 ## Max. :107.160 Max. :99.92659 9.2 Apply (summary) functions over margin This is base R data(&quot;mtcars&quot;) dim(mtcars) ## [1] 32 11 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... apply(mtcars,2,mean) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 apply(my_data.df[,c(1,2)],2,mean) ## y x ## 23.53977 49.60082 9.3 Missed values Missed values could be NA or NULL There are special values like Inf and -Inf (e.g. produced by division by zero) that are “Not a Number” or NaN NaN is NOT NA The functions to chack such values include is.na, is.nan, is.finite etc x &lt;- c(1,2,5,NA,NA) mean(x,na.rm=T) ## [1] 2.666667 sum(x,na.rm=T) ## [1] 8 is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE rm(x) # Division by zero is NOT NA 1/0 ## [1] Inf 9.4 Sorting, filtering and summarising 9.4.1 Base R Base R can do everything that dplyr does, but sometime the dplyr does is slightly easier. # Count/Tabulate table(my_data.df$f,useNA=&quot;always&quot;) ## ## A B &lt;NA&gt; ## 500 500 0 # Filtering my_data.df[my_data.df$x &gt;99,] ## y x f ## 135 63.53285 99.60168 A ## 174 35.95206 99.47804 A ## 392 57.16467 99.20543 A ## 417 41.48158 99.92659 A ## 434 62.57526 99.72521 A ## 811 -22.61801 99.43900 B ## 896 -9.09346 99.82257 B # Ordering != sorting sort(my_data.df$x)[1:5] ## [1] 0.04664706 0.10360118 0.11247254 0.17828955 0.41077132 order(my_data.df$x)[1:5] ## [1] 619 964 52 443 83 head(my_data.df[order(my_data.df$x),]) ## y x f ## 619 28.84024 0.04664706 B ## 964 22.69780 0.10360118 B ## 52 13.93909 0.11247254 A ## 443 -17.34737 0.17828955 A ## 83 16.07430 0.41077132 A ## 822 49.52343 0.65499248 B 9.4.2 Tables to tests In base R contingency tables are naturally connected to tests table(mtcars$vs) ## ## 0 1 ## 18 14 table(mtcars$am) ## ## 0 1 ## 19 13 table(mtcars$vs,mtcars$am) ## ## 0 1 ## 0 12 6 ## 1 7 7 fisher.test(table(mtcars$vs,mtcars$am)) # Chi Square etc ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(mtcars$vs, mtcars$am) ## p-value = 0.4727 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3825342 10.5916087 ## sample estimates: ## odds ratio ## 1.956055 x.mx &lt;- matrix(c(37,11,14,24),nrow=2) x.mx ## [,1] [,2] ## [1,] 37 14 ## [2,] 11 24 fisher.test(x.mx) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x.mx ## p-value = 0.0003348 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.047814 16.532095 ## sample estimates: ## odds ratio ## 5.63382 #table(mtcars$vs,mtcars$am,mtcars$gear) 9.5 Dplyr 9.5.1 Tidy Data concept Dplyr, ggplot and some other packages are part of the wider concept of Tidy Data : https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html I personally do not like this concept because I consider it as an additional unnecessary layer above what is already available in base R. On the other hand, I cant deny the convenience of Dplyr and ggplot. In any way, it is not possible to ignore the Tidy Data packages because they became so ubiquitous nowadays. 9.5.2 Traps in TidyData One of the stupidest thing in Tidy Data is that it refuses using Row Names, considering them somehow “untidy.” This means that many dplyr functions just silently drop the row names. Why on Earth row names are less tidy than column names?? Another possible trap is that dplyr functions may change the row order. So, the user needs either explicitly arrange the rows all the time, or, at least, pay attention to the row order when using dplyr. 9.5.3 Dplyr is BIG Here we just mention some functions to illustrate how dplyr may be used. In fact it is MUCH bigger than these small examples. See the cheat-sheet, for the beginning: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf 9.5.4 Piping Piping is a technique borrowed by dplyr from magrittr package: https://uc-r.github.io/pipe https://cran.r-project.org/web/packages/magrittr/index.html Key combination shift + ctr + M x %&gt;% f(y,z,...) is equivalent to f(x,y,z,...) 9.5.5 Dplyr examples # Summarising my_data.df %&gt;% group_by(f) %&gt;% summarise(count=n(), x_mean=mean(x), y_mean=mean(y)) ## # A tibble: 2 x 4 ## f count x_mean y_mean ## * &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 500 47.6 23.9 ## 2 B 500 51.6 23.2 # Sorting my_data.df %&gt;% arrange(x) %&gt;% head ## y x f ## 1 28.84024 0.04664706 B ## 2 22.69780 0.10360118 B ## 3 13.93909 0.11247254 A ## 4 -17.34737 0.17828955 A ## 5 16.07430 0.41077132 A ## 6 49.52343 0.65499248 B # Filtering rows my_data.df %&gt;% filter(x&gt;75) %&gt;% arrange(x) %&gt;% head ## y x f ## 1 51.664667 75.14120 B ## 2 74.877955 75.16536 A ## 3 9.301528 75.21929 B ## 4 24.431406 75.24749 B ## 5 38.428195 75.34197 A ## 6 104.802452 75.36677 A # Selecting columns my_data.df %&gt;% select(y,x) %&gt;% head ## y x ## 1 -4.0355166 7.493717 ## 2 -10.0877060 6.524005 ## 3 -0.9528231 2.841776 ## 4 30.3710849 12.412504 ## 5 21.2335777 46.828968 ## 6 57.6147990 98.162571 # Piping can be done outside of the dplyr too mtcars %&gt;% head ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 "],["regression.html", "Section 10 Regression 10.1 Note 10.2 Load data 10.3 Explore data 10.4 Impute if needed 10.5 Linear modelling 10.6 lm could be evaluated by ANOVA 10.7 Predict 10.8 Update 10.9 Crooked things yet to explore …", " Section 10 Regression 10.1 Note Because of the shoterned time we only discussed examples with lm(); interface to glm() works in a very similar way - look in the references that I e-mailed to you earlier. 10.2 Load data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) 10.3 Explore data dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num -4.036 -10.088 -0.953 30.371 21.234 ... ## $ x: num 7.49 6.52 2.84 12.41 46.83 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; head(my_data.df) ## y x f ## 1 -4.0355166 7.493717 A ## 2 -10.0877060 6.524005 A ## 3 -0.9528231 2.841776 A ## 4 30.3710849 12.412504 A ## 5 21.2335777 46.828968 A ## 6 57.6147990 98.162571 A summary(my_data.df) ## y x f ## Min. :-58.764 Min. : 0.04665 A:500 ## 1st Qu.: 3.804 1st Qu.:24.24327 B:500 ## Median : 22.788 Median :49.66040 ## Mean : 23.540 Mean :49.60082 ## 3rd Qu.: 42.603 3rd Qu.:74.29039 ## Max. :107.160 Max. :99.92659 any(is.na(my_data.df)) ## [1] FALSE ggplot(data=my_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 10.4 Impute if needed x &lt;- c(1,2,5,NA,NA) is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE mean_x &lt;- mean(x, na.rm = T) mean_x -&gt; x[is.na(x)] x ## [1] 1.000000 2.000000 5.000000 2.666667 2.666667 rm(x) 10.5 Linear modelling 10.5.1 Single term model_1 &lt;- lm(y~x, data=my_data.df) summary(model_1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.38 -19.74 -0.73 19.08 83.65 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.434970 1.792145 13.076 &lt;2e-16 *** ## x 0.002113 0.031217 0.068 0.946 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.54 on 998 degrees of freedom ## Multiple R-squared: 4.59e-06, Adjusted R-squared: -0.0009974 ## F-statistic: 0.004581 on 1 and 998 DF, p-value: 0.9461 plot(model_1) plot(y~x,data=my_data.df) abline(model_1, col=&quot;red&quot;) 10.5.2 Multiple terms without interaction model_2 &lt;- lm(y~x+f, data=my_data.df) #equivalent: note using dot for ALL terms model_2_dot &lt;- lm(y~., data=my_data.df) summary(model_2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.067 -19.810 -0.444 19.375 84.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.730813 1.961462 12.099 &lt;2e-16 *** ## x 0.002935 0.031308 0.094 0.925 ## fB -0.673197 1.810114 -0.372 0.710 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.55 on 997 degrees of freedom ## Multiple R-squared: 0.0001433, Adjusted R-squared: -0.001862 ## F-statistic: 0.07145 on 2 and 997 DF, p-value: 0.9311 summary(model_2_dot) ## ## Call: ## lm(formula = y ~ ., data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.067 -19.810 -0.444 19.375 84.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.730813 1.961462 12.099 &lt;2e-16 *** ## x 0.002935 0.031308 0.094 0.925 ## fB -0.673197 1.810114 -0.372 0.710 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.55 on 997 degrees of freedom ## Multiple R-squared: 0.0001433, Adjusted R-squared: -0.001862 ## F-statistic: 0.07145 on 2 and 997 DF, p-value: 0.9311 plot(model_2) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_2, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_2, col = &quot;blue&quot;, lwd = 3): only using the first two of 3 ## regression coefficients 10.5.3 With interaction model_3 &lt;- lm(y~x*f, data=my_data.df) summary(model_3) ## ## Call: ## lm(formula = y ~ x * f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -76.047 -15.768 0.449 15.878 75.606 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.12605 2.13787 0.994 0.32 ## x 0.45719 0.03825 11.953 &lt;2e-16 *** ## fB 46.17059 3.16691 14.579 &lt;2e-16 *** ## x:fB -0.94300 0.05511 -17.111 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.11 on 996 degrees of freedom ## Multiple R-squared: 0.2273, Adjusted R-squared: 0.225 ## F-statistic: 97.66 on 3 and 996 DF, p-value: &lt; 2.2e-16 plot(model_3) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_3, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_3, col = &quot;blue&quot;, lwd = 3): only using the first two of 4 ## regression coefficients 10.6 lm could be evaluated by ANOVA https://stats.stackexchange.com/questions/115304/interpreting-output-from-anova-when-using-lm-as-input Anova output may be more clear when categorical variables have more than 3 levels (we discussed it during the practical session) Note that there are at least two slightly different functions for ANOVA in R: anova() and aov() anova(model_1) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 4 3.73 0.0046 0.9461 ## Residuals 998 812717 814.35 anova(model_2) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 4 3.73 0.0046 0.9461 ## f 1 113 112.73 0.1383 0.7100 ## Residuals 997 812604 815.05 anova(model_3) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 4 4 0.0059 0.9387 ## f 1 113 113 0.1788 0.6725 ## x:f 1 184608 184608 292.7878 &lt;2e-16 *** ## Residuals 996 627996 631 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.1 Anova could be used to compare models This can be used in the following way: First a “null” model could be calculated with all the “confounders”/“covariates” Then a predictor of interest is added to the model and models are compared by ANOVA If the model is improved significantly, then the predictor is important anova(model_1,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 812717 ## 2 996 627996 2 184721 146.48 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model_1,model_2,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x + f ## Model 3: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 812717 ## 2 997 812604 1 113 0.1788 0.6725 ## 3 996 627996 1 184608 292.7878 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.2 A bit of OOP about anova(lm) # What other methods could be applied to lm object? class(model_1) ## [1] &quot;lm&quot; methods(class=&quot;lm&quot;) ## [1] add1 alias anova case.names coerce ## [6] confint cooks.distance deviance dfbeta dfbetas ## [11] drop1 dummy.coef effects extractAIC family ## [16] formula fortify hatvalues influence initialize ## [21] kappa labels logLik model.frame model.matrix ## [26] nobs plot predict print proj ## [31] qqnorm qr residuals rstandard rstudent ## [36] show simulate slotsFromS3 summary variable.names ## [41] vcov ## see &#39;?methods&#39; for accessing help and source code # What other classes coul be evaluated by ANOVA? methods(anova) ## [1] anova.gam* anova.glm* anova.glmlist* anova.gls* anova.lm* ## [6] anova.lme* anova.lmlist* anova.loess* anova.mlm* anova.nls* ## see &#39;?methods&#39; for accessing help and source code #?aov #?anova #?anova.lm 10.7 Predict new_data.df &lt;- data.frame(x=c(10,20), f=factor(c(&quot;A&quot;,&quot;B&quot;))) new_data.df ## x f ## 1 10 A ## 2 20 B predict(model_1,new_data.df) ## 1 2 ## 23.45610 23.47723 predict(model_2,new_data.df) ## 1 2 ## 23.76016 23.11631 10.8 Update It is very rarely used, but occasionally could be seen in help and tutorials Note using dots on the left and right of ~ in the formula m1 = lm(y~x, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.38 -19.74 -0.73 19.08 83.65 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.434970 1.792145 13.076 &lt;2e-16 *** ## x 0.002113 0.031217 0.068 0.946 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.54 on 998 degrees of freedom ## Multiple R-squared: 4.59e-06, Adjusted R-squared: -0.0009974 ## F-statistic: 0.004581 on 1 and 998 DF, p-value: 0.9461 m2 = update(m1,.~.+f, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.067 -19.810 -0.444 19.375 84.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.730813 1.961462 12.099 &lt;2e-16 *** ## x 0.002935 0.031308 0.094 0.925 ## fB -0.673197 1.810114 -0.372 0.710 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.55 on 997 degrees of freedom ## Multiple R-squared: 0.0001433, Adjusted R-squared: -0.001862 ## F-statistic: 0.07145 on 2 and 997 DF, p-value: 0.9311 10.9 Crooked things yet to explore … We could not discuss these because we were short of time (and I have limited understanding of them ?) 10.9.1 Intercept, design matrix See my PowerPoint slides for students with biological background Intercept could be set to zero by two ways: y ~ 0 + x y ~ x - 1 Something strange happens without intercept in the model. This something could be undertood by staring at “design matrix” m1 &lt;- lm(y~x+f, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.067 -19.810 -0.444 19.375 84.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.730813 1.961462 12.099 &lt;2e-16 *** ## x 0.002935 0.031308 0.094 0.925 ## fB -0.673197 1.810114 -0.372 0.710 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.55 on 997 degrees of freedom ## Multiple R-squared: 0.0001433, Adjusted R-squared: -0.001862 ## F-statistic: 0.07145 on 2 and 997 DF, p-value: 0.9311 m2 &lt;- lm(y~x+f-1, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f - 1, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.067 -19.810 -0.444 19.375 84.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.002935 0.031308 0.094 0.925 ## fA 23.730813 1.961462 12.099 &lt;2e-16 *** ## fB 23.057616 2.060112 11.192 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.55 on 997 degrees of freedom ## Multiple R-squared: 0.4055, Adjusted R-squared: 0.4037 ## F-statistic: 226.7 on 3 and 997 DF, p-value: &lt; 2.2e-16 m3 &lt;- lm(y~0+x+f, data=my_data.df) summary(m3) ## ## Call: ## lm(formula = y ~ 0 + x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.067 -19.810 -0.444 19.375 84.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.002935 0.031308 0.094 0.925 ## fA 23.730813 1.961462 12.099 &lt;2e-16 *** ## fB 23.057616 2.060112 11.192 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.55 on 997 degrees of freedom ## Multiple R-squared: 0.4055, Adjusted R-squared: 0.4037 ## F-statistic: 226.7 on 3 and 997 DF, p-value: &lt; 2.2e-16 data(&quot;mtcars&quot;) other_data &lt;- mtcars[1:10,c(&quot;vs&quot;,&quot;am&quot;,&quot;gear&quot;)] other_data ## vs am gear ## Mazda RX4 0 1 4 ## Mazda RX4 Wag 0 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 0 0 3 ## Valiant 1 0 3 ## Duster 360 0 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 model.matrix(~am+gear, data=other_data) ## (Intercept) am gear ## Mazda RX4 1 1 4 ## Mazda RX4 Wag 1 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 1 0 3 ## Valiant 1 0 3 ## Duster 360 1 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 model.matrix(~0+am+gear, data=other_data) ## am gear ## Mazda RX4 1 4 ## Mazda RX4 Wag 1 4 ## Datsun 710 1 4 ## Hornet 4 Drive 0 3 ## Hornet Sportabout 0 3 ## Valiant 0 3 ## Duster 360 0 3 ## Merc 240D 0 4 ## Merc 230 0 4 ## Merc 280 0 4 ## attr(,&quot;assign&quot;) ## [1] 1 2 10.9.2 Contrasts Also, there is some cryptic way allowing to specify what sub-groups to include/exclude in the comparison: this is something to do with “contrasts” https://rcompanion.org/rcompanion/h_01.html https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r contrasts(my_data.df$f) ## B ## A 0 ## B 1 contrasts(as.factor(other_data$gear)) ## 4 ## 3 0 ## 4 1 10.9.3 Type I, II and III sums ANOVA and linear models are, in fact equivalent to each other (even at the level of implementation). Both rely on calculating some sum of squares. There are 3 ways of calculating these sums: Type I, Type II and Type III Google about these … https://rcompanion.org/rcompanion/d_04.html "]]
