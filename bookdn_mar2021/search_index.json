[["index.html", "I forgot the title Section 1 My great Math LaTex 1.1 Rendering to PDF and EPUB 1.2 Sharing on social media", " I forgot the title Of course, its Me 2021-04-21 Section 1 My great Math LaTex This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\[{{a^2 + b^2} \\over { \\sum{y} \\iff \\int{z \\over d^2}}} \\ne c^2\\]. https://www.math.uci.edu/~xiangwen/pdf/LaTeX-Math-Symbols.pdf 1.1 Rendering to PDF and EPUB Although PDF and EPUB rendering is a default feature of bookdown, it is suppressed in this example to illustrate interactive features of Plotly 1.2 Sharing on social media I do not like these media ! I kept it here just to illustrate this feature of bookdown pre{ overflow-x: auto; } pre code{ word-wrap: normal; white-space: pre; } Sys.time() ## [1] &quot;2021-04-21 13:57:20 BST&quot; rm(list=ls()) graphics.off() gc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 444790 23.8 950041 50.8 NA 641574 34.3 ## Vcells 830823 6.4 8388608 64.0 16384 1769399 13.5 "],["r-markdown.html", "Section 2 R-Markdown 2.1 2nd level header 2.2 Not only R … 2.3 Back to R", " Section 2 R-Markdown 2.1 2nd level header 2.1.1 3rd level header etc 2.2 Not only R … 2.2.1 Python Just use chunk with option “python” Requires Reticulate and Python (of course :) DOES NOT WORK with bookdown (and gives a very obscure error mesage !) print(&quot;hello world&quot;) a=5 print(a) 2.2.2 Bash Just use chunk with option “bash” echo BlaBlaBla ## BlaBlaBla 2.3 Back to R 2.3.1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: data(&quot;cars&quot;) summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 2.3.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["a-simple-analysis.html", "Section 3 A simple analysis 3.1 In-built datasets 3.2 Explore data 3.3 T-test 3.4 Liear model", " Section 3 A simple analysis library(ggplot2) 3.1 In-built datasets # Inbuilt dataset data(&quot;mtcars&quot;) # Explore data class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 dim(mtcars) ## [1] 32 11 3.2 Explore data mpg vs hp cosiderig vs as covariate 3.2.1 Base R plottig plot(mpg~hp, data=mtcars, pch=vs, main=&quot;My title&quot;) legend(&quot;topright&quot;, c(&quot;vs=0&quot;,&quot;vs=1&quot;), pch=c(0,1)) 3.2.2 ggplot style ggplot(data=mtcars,aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point() + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.3 T-test plot(mpg~vs, data=mtcars) boxplot(mpg~vs, data=mtcars) t.test(mpg~vs, data=mtcars) ## ## Welch Two Sample t-test ## ## data: mpg by vs ## t = -4.6671, df = 22.716, p-value = 0.0001098 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.462508 -4.418445 ## sample estimates: ## mean in group 0 mean in group 1 ## 16.61667 24.55714 3.4 Liear model 3.4.1 mpg ~ hp my_model_1 &lt;- lm(mpg~hp, data=mtcars) class(my_model_1) ## [1] &quot;lm&quot; summary(my_model_1) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 plot(mpg~hp, data=mtcars) abline(my_model_1) plot(my_model_1) 3.4.2 mpg ~ vs + hp my_model_2 &lt;- lm(mpg ~ vs + hp, data=mtcars) summary(my_model_2) ## ## Call: ## lm(formula = mpg ~ vs + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7131 -2.3336 -0.1332 1.9055 7.9055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.96300 2.89069 9.328 3.13e-10 *** ## vs 2.57622 1.96966 1.308 0.201163 ## hp -0.05453 0.01448 -3.766 0.000752 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.818 on 29 degrees of freedom ## Multiple R-squared: 0.6246, Adjusted R-squared: 0.5987 ## F-statistic: 24.12 on 2 and 29 DF, p-value: 6.768e-07 plot(my_model_2) 3.4.3 mpg ~ vs * hp my_model_3 &lt;- lm(mpg ~ vs * hp, data=mtcars) summary(my_model_3) ## ## Call: ## lm(formula = mpg ~ vs * hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.5821 -1.7710 -0.3612 1.5969 9.2646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.49637 2.73893 8.944 1.07e-09 *** ## vs 14.50418 4.58160 3.166 0.00371 ** ## hp -0.04153 0.01379 -3.011 0.00547 ** ## vs:hp -0.11657 0.04130 -2.822 0.00868 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.428 on 28 degrees of freedom ## Multiple R-squared: 0.7077, Adjusted R-squared: 0.6764 ## F-statistic: 22.6 on 3 and 28 DF, p-value: 1.227e-07 plot(my_model_3) "],["data-types.html", "Section 4 Data types 4.1 Vectors 4.2 Numeric matrices 4.3 Recycling in element-wise operations 4.4 Factors: categorical variale 4.5 Lists 4.6 Dataframe: list of vectors/factors", " Section 4 Data types Be careful with changing folders in bookdown !! setwd(&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021&quot;) 4.1 Vectors 4.1.1 Numeric vectors a &lt;- 1:5 class(a) ## [1] &quot;integer&quot; a ## [1] 1 2 3 4 5 b &lt;- seq(from=10, to=50, by=10) class(b) ## [1] &quot;numeric&quot; b ## [1] 10 20 30 40 50 # Vectorised operation: element-wise summation a + b ## [1] 11 22 33 44 55 4.1.2 Vector elements may have names Names is a property attributes(a) ## NULL names(a) &lt;- c(&quot;e1&quot;,&quot;e2&quot;,&quot;e3&quot;,&quot;e4&quot;,&quot;e5&quot;) a ## e1 e2 e3 e4 e5 ## 1 2 3 4 5 attributes(a) ## $names ## [1] &quot;e1&quot; &quot;e2&quot; &quot;e3&quot; &quot;e4&quot; &quot;e5&quot; a[c(1,2,4)] ## e1 e2 e4 ## 1 2 4 a[c(T,T,F,T,F)] ## e1 e2 e4 ## 1 2 4 a[c(&quot;e1&quot;,&quot;e2&quot;,&quot;e4&quot;)] ## e1 e2 e4 ## 1 2 4 4.2 Numeric matrices m1 &lt;- matrix(1:25, nrow=5) m1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 m2 &lt;- matrix(rep(10,25), nrow=5) m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 10 10 10 10 ## [2,] 10 10 10 10 10 ## [3,] 10 10 10 10 10 ## [4,] 10 10 10 10 10 ## [5,] 10 10 10 10 10 t(m1) # Transposition ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 ## [4,] 16 17 18 19 20 ## [5,] 21 22 23 24 25 m1 + m2 # Element-wise summation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 16 21 26 31 ## [2,] 12 17 22 27 32 ## [3,] 13 18 23 28 33 ## [4,] 14 19 24 29 34 ## [5,] 15 20 25 30 35 m1 * m2 # Element-wise multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10 60 110 160 210 ## [2,] 20 70 120 170 220 ## [3,] 30 80 130 180 230 ## [4,] 40 90 140 190 240 ## [5,] 50 100 150 200 250 m1 %*% m2 # Matrix multiplication ## [,1] [,2] [,3] [,4] [,5] ## [1,] 550 550 550 550 550 ## [2,] 600 600 600 600 600 ## [3,] 650 650 650 650 650 ## [4,] 700 700 700 700 700 ## [5,] 750 750 750 750 750 4.2.1 Matrices may have rownames and colnames rownames(m1) &lt;- c(&quot;s1&quot;,&quot;s2&quot;,&quot;s3&quot;,&quot;s4&quot;,&quot;s5&quot;) colnames(m1) &lt;- c(&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;g5&quot;) m1 ## g1 g2 g3 g4 g5 ## s1 1 6 11 16 21 ## s2 2 7 12 17 22 ## s3 3 8 13 18 23 ## s4 4 9 14 19 24 ## s5 5 10 15 20 25 m1[c(&quot;s2&quot;,&quot;s3&quot;),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 m1[c(F,T,T,F,F),c(2,4)] ## g2 g4 ## s2 7 17 ## s3 8 18 4.3 Recycling in element-wise operations Shorter vector is recycled along the longer oject Matrices are processed as vectors in column-after-column way c &lt;- 1:3 c ## [1] 1 2 3 b + c ## Warning in b + c: longer object length is not a multiple of shorter object length ## [1] 11 22 33 41 52 c + m2 ## Warning in c + m2: longer object length is not a multiple of shorter object length ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 13 12 11 13 ## [2,] 12 11 13 12 11 ## [3,] 13 12 11 13 12 ## [4,] 11 13 12 11 13 ## [5,] 12 11 13 12 11 a + m2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 m2 + a ## [,1] [,2] [,3] [,4] [,5] ## [1,] 11 11 11 11 11 ## [2,] 12 12 12 12 12 ## [3,] 13 13 13 13 13 ## [4,] 14 14 14 14 14 ## [5,] 15 15 15 15 15 rm(a,b,c,m1,m2) 4.4 Factors: categorical variale Categorical != character v &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) class(v) ## [1] &quot;character&quot; v ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; f &lt;- factor(v) class(f) ## [1] &quot;factor&quot; f ## [1] A B C ## Levels: A B C f &lt;- factor(v, levels=c(&quot;C&quot;,&quot;B&quot;,&quot;A&quot;)) f ## [1] A B C ## Levels: C B A f &lt;- factor(v, levels=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;)) f ## [1] &lt;NA&gt; B C ## Levels: B C D f &lt;- factor(v, levels=LETTERS) f ## [1] A B C ## Levels: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z f &lt;- factor(v, levels=LETTERS, ordered = T) f ## [1] A B C ## Levels: A &lt; B &lt; C &lt; D &lt; E &lt; F &lt; G &lt; H &lt; I &lt; J &lt; K &lt; L &lt; M &lt; N &lt; O &lt; P &lt; Q &lt; R &lt; S &lt; T &lt; U &lt; V &lt; W &lt; X &lt; Y &lt; Z # Potential confusion factor(1:5, levels=c(4,5,3,2,1), ordered=T) ## [1] 1 2 3 4 5 ## Levels: 4 &lt; 5 &lt; 3 &lt; 2 &lt; 1 rm(v,f) 4.5 Lists Collection of ojects of different types my.list &lt;- list(c(1,2),&quot;A&quot;,T ) my.list ## [[1]] ## [1] 1 2 ## ## [[2]] ## [1] &quot;A&quot; ## ## [[3]] ## [1] TRUE my.list[1] ## [[1]] ## [1] 1 2 my.list[[1]][1] ## [1] 1 my.list[[1]][2] ## [1] 2 my.list[2][1] ## [[1]] ## [1] &quot;A&quot; my.list &lt;- list(lel1=c(1,2), lel2=&quot;A&quot;, lel3=T ) my.list ## $lel1 ## [1] 1 2 ## ## $lel2 ## [1] &quot;A&quot; ## ## $lel3 ## [1] TRUE my.list$lel1 ## [1] 1 2 my.list$lel1[2] ## [1] 2 rm(my.list) 4.6 Dataframe: list of vectors/factors #data(&quot;mtcars&quot;) class(mtcars) ## [1] &quot;data.frame&quot; str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 #data(&quot;mpg&quot;) #class(mpg) #str(mpg) my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA)) class(my.df) ## [1] &quot;data.frame&quot; my.df ## first_name second_name age income ## 1 AL Smith 15 100 ## 2 AL Larionov 23 200 ## 3 BS Bob 1 NA str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: Factor w/ 3 levels &quot;Bob&quot;,&quot;Larionov&quot;,..: 3 2 1 ## $ age : num 15 23 1 ## $ income : num 100 200 NA my.df &lt;- data.frame(first_name=factor(c(&quot;AL&quot;,&quot;AL&quot;,&quot;BS&quot;)), second_name=c(&quot;Smith&quot;,&quot;Larionov&quot;,&quot;Bob&quot;), age=c(15,23,1), income=c(100,200,NA), stringsAsFactors=F) str(my.df) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ first_name : Factor w/ 2 levels &quot;AL&quot;,&quot;BS&quot;: 1 1 2 ## $ second_name: chr &quot;Smith&quot; &quot;Larionov&quot; &quot;Bob&quot; ## $ age : num 15 23 1 ## $ income : num 100 200 NA "],["plotly.html", "Section 5 Plotly 5.1 Reference 5.2 Load library 5.3 Get some data 5.4 Plotly magic", " Section 5 Plotly 5.1 Reference https://plotly-r.com/index.html 5.2 Load library library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 5.3 Get some data # Inbuilt dataset #data(&quot;mtcars&quot;) mtcars &lt;- data.frame(mtcars, make=rownames(mtcars), stringsAsFactors = F) # Explore data dim(mtcars) ## [1] 32 12 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 12 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... ## $ make: chr &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... 5.4 Plotly magic d &lt;- ggplot(data=mtcars, aes(x=hp,y=mpg,color=as.factor(vs))) + geom_point(aes(text=make)) + geom_smooth(method=lm) + labs(title=&quot;My great ggplot !!!&quot;, color=&quot;vs&quot;, x =&quot;HP&quot;, y=&quot;MPG&quot;) ## Warning: Ignoring unknown aesthetics: text d ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplotly(d, tooltip=&quot;text&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # A base R plot ... plot(mpg~hp, data=mtcars) "],["distributionns.html", "Section 6 Distributionns 6.1 Function curve() 6.2 Normal distribution 6.3 Gamma 6.4 Beta", " Section 6 Distributionns 6.1 Function curve() curve(sin(x),0,2*pi) curve(cos(x),from=0,to=2*pi,add=T,col=&quot;red&quot;,lty=2) 6.2 Normal distribution dnorm(-1.96, mean=0, sd=1) ## [1] 0.05844094 dnorm(-1.96, mean=10, sd=7) ## [1] 0.01324074 curve(dnorm(x),-3,3) curve(dnorm(x, 10, 3),0,20) curve(pnorm(x, 10, 3),0,20) curve(dnorm(x),-4,4) abline(v=qnorm(0.025), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.025, lower.tail = F), col=&quot;red&quot;, lty=2) abline(v=qnorm(0.005), col=&quot;blue&quot;, lty=2) abline(v=qnorm(0.005, lower.tail = F), col=&quot;blue&quot;, lty=2) curve(qnorm(x)) rnorm(5) ## [1] 1.1667779 1.1314982 -0.5103041 1.1842293 0.9792950 hist(rnorm(1000),freq = F,ylim=c(0,0.5)) curve(dnorm(x),-4,4,add=T,col=&quot;blue&quot;) dh &lt;- c(rnorm(1000,75,50),rnorm(1000,180,15)) hist(dh,100,200) 6.3 Gamma curve(dgamma(x,2,6)) curve(dgamma(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;g(2,6)&quot;,&quot;g(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dgamma(x,3,7),0,2) abline(v=qgamma(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qgamma(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rgamma(1000,3,7),freq = F, add=T, border=&quot;green&quot;) 6.4 Beta curve(dbeta(x,2,6)) curve(dbeta(x,3,7),col=&quot;red&quot;,add=T) legend(&quot;topright&quot;, legend=c(&quot;b(2,6)&quot;,&quot;b(3,7)&quot;), lty=1, col=c(&quot;black&quot;,&quot;red&quot;)) curve(dbeta(x,3,7)) abline(v=qbeta(0.05,3,7),col=&quot;red&quot;,lty=2) abline(v=qbeta(0.05,3,7,lower.tail = F),col=&quot;red&quot;,lty=2) hist(rbeta(1000,3,7),freq = F, add=T, border=&quot;green&quot;) "],["package.html", "Section 7 Package 7.1 User-defined fnction 7.2 Recursive functions are allowed 7.3 Lets add some data 7.4 Prepare template for package source folder", " Section 7 Package 7.1 User-defined fnction mff &lt;- function(a,b){ return(a+b) } mff(3,5) ## [1] 8 #mff(3,&quot;Good&quot;) #mff(&quot;a&quot;,&quot;b&quot;) 7.2 Recursive functions are allowed my_sum &lt;- function(n){ result &lt;- 0 for(i in 1:n){ result &lt;- result + i } return(result) } my_sum(10) ## [1] 55 my_rsum &lt;- function(n){ if(n&gt;1){ return(n+my_rsum(n-1)) } if(n==1){ return(n) } } my_rsum(10) ## [1] 55 7.3 Lets add some data my_great_dataset &lt;- data.frame(names=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), values=c(1,2,3,4)) 7.4 Prepare template for package source folder ls() getwd() package.skeleton(name=&quot;ab&quot;,list=ls()) A bush chunk to build: cd /Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021 R CMD build ab R CMD check ab_1.0.tar.gz Follow the instructions for minimal required documentation, i.e.: Titles (&amp; Descriptions?) in each object Rd Examples (in the package Rd) Then install, load and enjoy ! #install.packages(...) library(ab) data(my_great_dataset) "],["data-reading-writing.html", "Section 8 Data reading / writing 8.1 A simple simulated dataset 8.2 Saving and reading", " Section 8 Data reading / writing library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 8.1 A simple simulated dataset x &lt;- runif(1000, 0, 100) hist(x) f &lt;- factor(c(rep(&quot;A&quot;,500),rep(&quot;B&quot;,500))) head(f) ## [1] A A A A A A ## Levels: A B tail(f) ## [1] B B B B B B ## Levels: A B y1 &lt;- 0.5 * x[1:500] + rnorm(500,0,25) y2 &lt;- -0.5 * x[501:1000] + rnorm(500,0,25) y &lt;- c(y1,y2+50) sim_data.df &lt;- data.frame(y,x,f) str(sim_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num 60.1 29.7 -24.2 31.5 14.1 ... ## $ x: num 91.2 79.6 18.9 95.2 97.3 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... plot(y~x, pch=as.integer(f)) ggplot(data=sim_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 8.2 Saving and reading … at last ! 8.2.1 RData save.image(file=&quot;results1.RData&quot;) rm(list=ls()) load(&quot;results1.RData&quot;) save(&quot;sim_data.df&quot;,file=&quot;results2.RData&quot;) rm(list=ls()) load(&quot;results2.RData&quot;) 8.2.2 Text files write.table(sim_data.df,file=&quot;results.tsv&quot;, quote=F,sep=&quot;\\t&quot;,row.names = F) rm(list=ls()) my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num 60.1 29.7 -24.2 31.5 14.1 ... ## $ x: num 91.2 79.6 18.9 95.2 97.3 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-50.797 Min. : 0.03501 A:500 ## 1st Qu.: 6.281 1st Qu.:24.54960 B:500 ## Median : 25.405 Median :48.63585 ## Mean : 26.348 Mean :49.57381 ## 3rd Qu.: 45.679 3rd Qu.:75.13851 ## Max. :124.211 Max. :99.97250 "],["data-exploring-modifying.html", "Section 9 Data exploring / modifying 9.1 Read data 9.2 Apply (summary) functions over margin 9.3 Missed values 9.4 Sorting, filtering and summarising 9.5 Dplyr", " Section 9 Data exploring / modifying library(ggplot2) library(dplyr) 9.1 Read data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num 60.1 29.7 -24.2 31.5 14.1 ... ## $ x: num 91.2 79.6 18.9 95.2 97.3 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; summary(my_data.df) ## y x f ## Min. :-50.797 Min. : 0.03501 A:500 ## 1st Qu.: 6.281 1st Qu.:24.54960 B:500 ## Median : 25.405 Median :48.63585 ## Mean : 26.348 Mean :49.57381 ## 3rd Qu.: 45.679 3rd Qu.:75.13851 ## Max. :124.211 Max. :99.97250 9.2 Apply (summary) functions over margin This is base R data(&quot;mtcars&quot;) dim(mtcars) ## [1] 32 11 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... apply(mtcars,2,mean) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 0.437500 0.406250 3.687500 2.812500 apply(my_data.df[,c(1,2)],2,mean) ## y x ## 26.34824 49.57381 9.3 Missed values Missed values could be NA or NULL There are special values like Inf and -Inf (e.g. produced by division by zero) that are “Not a Number” or NaN NaN is NOT NA The functions to chack such values include is.na, is.nan, is.finite etc x &lt;- c(1,2,5,NA,NA) mean(x,na.rm=T) ## [1] 2.666667 sum(x,na.rm=T) ## [1] 8 is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE rm(x) # Division by zero is NOT NA 1/0 ## [1] Inf 9.4 Sorting, filtering and summarising 9.4.1 Base R Base R can do everything that dplyr does, but sometime the dplyr does is slightly easier. # Count/Tabulate table(my_data.df$f,useNA=&quot;always&quot;) ## ## A B &lt;NA&gt; ## 500 500 0 # Filtering my_data.df[my_data.df$x &gt;99,] ## y x f ## 23 75.8628994 99.49952 A ## 49 39.4988562 99.68580 A ## 130 101.5633037 99.32998 A ## 151 58.2729911 99.10992 A ## 159 32.1896099 99.97250 A ## 231 43.5506325 99.54336 A ## 430 46.4520591 99.45886 A ## 600 -20.8935258 99.28113 B ## 692 20.1214623 99.84227 B ## 703 -0.8803640 99.82776 B ## 775 -25.4546275 99.19841 B ## 836 0.6044361 99.19020 B ## 895 24.4069325 99.11075 B ## 897 15.1902885 99.39741 B # Ordering != sorting sort(my_data.df$x)[1:5] ## [1] 0.03500932 0.09305954 0.18818777 0.47212590 0.54321650 order(my_data.df$x)[1:5] ## [1] 822 741 40 914 642 head(my_data.df[order(my_data.df$x),]) ## y x f ## 822 43.94244 0.03500932 B ## 741 35.34163 0.09305954 B ## 40 -42.92886 0.18818777 A ## 914 39.48882 0.47212590 B ## 642 79.48188 0.54321650 B ## 590 70.58160 0.67339509 B 9.4.2 Tables to tests In base R contingency tables are naturally connected to tests table(mtcars$vs) ## ## 0 1 ## 18 14 table(mtcars$am) ## ## 0 1 ## 19 13 table(mtcars$vs,mtcars$am) ## ## 0 1 ## 0 12 6 ## 1 7 7 fisher.test(table(mtcars$vs,mtcars$am)) # Chi Square etc ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(mtcars$vs, mtcars$am) ## p-value = 0.4727 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3825342 10.5916087 ## sample estimates: ## odds ratio ## 1.956055 x.mx &lt;- matrix(c(37,11,14,24),nrow=2) x.mx ## [,1] [,2] ## [1,] 37 14 ## [2,] 11 24 fisher.test(x.mx) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x.mx ## p-value = 0.0003348 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.047814 16.532095 ## sample estimates: ## odds ratio ## 5.63382 #table(mtcars$vs,mtcars$am,mtcars$gear) 9.5 Dplyr 9.5.1 Tidy Data concept Dplyr, ggplot and some other packages are part of the wider concept of Tidy Data : https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html I personally do not like this concept because I consider it as an additional unnecessary layer above what is already available in base R. On the other hand, I cant deny the convenience of Dplyr and ggplot. In any way, it is not possible to ignore the Tidy Data packages because they became so ubiquitous nowadays. 9.5.2 Traps in TidyData One of the stupidest thing in Tidy Data is that it refuses using Row Names, considering them somehow “untidy.” This means that many dplyr functions just silently drop the row names. Why on Earth row names are less tidy than column names?? Another possible trap is that dplyr functions may change the row order. So, the user needs either explicitly arrange the rows all the time, or, at least, pay attention to the row order when using dplyr. 9.5.3 Dplyr is BIG Here we just mention some functions to illustrate how dplyr may be used. In fact it is MUCH bigger than these small examples. See the cheat-sheet, for the beginning: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf 9.5.4 Piping Piping is a technique borrowed by dplyr from magrittr package: https://uc-r.github.io/pipe https://cran.r-project.org/web/packages/magrittr/index.html Key combination shift + ctr + M x %&gt;% f(y,z,...) is equivalent to f(x,y,z,...) 9.5.5 Dplyr examples # Summarising my_data.df %&gt;% group_by(f) %&gt;% summarise(count=n(), x_mean=mean(x), y_mean=mean(y)) ## # A tibble: 2 x 4 ## f count x_mean y_mean ## * &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 500 49.8 25.3 ## 2 B 500 49.3 27.4 # Sorting my_data.df %&gt;% arrange(x) %&gt;% head ## y x f ## 1 43.94244 0.03500932 B ## 2 35.34163 0.09305954 B ## 3 -42.92886 0.18818777 A ## 4 39.48882 0.47212590 B ## 5 79.48188 0.54321650 B ## 6 70.58160 0.67339509 B # Filtering rows my_data.df %&gt;% filter(x&gt;75) %&gt;% arrange(x) %&gt;% head ## y x f ## 1 73.10901 75.04623 A ## 2 3.23488 75.12585 B ## 3 42.80709 75.17649 B ## 4 23.26798 75.33979 B ## 5 24.40846 75.38906 B ## 6 -42.16674 75.48921 B # Selecting columns my_data.df %&gt;% select(y,x) %&gt;% head ## y x ## 1 60.057989 91.16850 ## 2 29.683027 79.64650 ## 3 -24.160063 18.91743 ## 4 31.488361 95.18982 ## 5 14.122619 97.29713 ## 6 5.311347 41.99818 # Piping can be done outside of the dplyr too mtcars %&gt;% head ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 "],["regression.html", "Section 10 Regression 10.1 Note 10.2 Load data 10.3 Explore data 10.4 Impute if needed 10.5 Linear modelling 10.6 lm could be evaluated by ANOVA 10.7 Predict 10.8 Update 10.9 Crooked things yet to explore …", " Section 10 Regression 10.1 Note Because of the shoterned time we only discussed examples with lm(); interface to glm() works in a very similar way - look in the references that I e-mailed to you earlier. 10.2 Load data my_data.df &lt;- read.table(&quot;results.tsv&quot;, sep=&quot;\\t&quot;,quote=&quot;&quot;, header=T) 10.3 Explore data dim(my_data.df) ## [1] 1000 3 str(my_data.df) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ y: num 60.1 29.7 -24.2 31.5 14.1 ... ## $ x: num 91.2 79.6 18.9 95.2 97.3 ... ## $ f: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... colnames(my_data.df) ## [1] &quot;y&quot; &quot;x&quot; &quot;f&quot; head(my_data.df) ## y x f ## 1 60.057989 91.16850 A ## 2 29.683027 79.64650 A ## 3 -24.160063 18.91743 A ## 4 31.488361 95.18982 A ## 5 14.122619 97.29713 A ## 6 5.311347 41.99818 A summary(my_data.df) ## y x f ## Min. :-50.797 Min. : 0.03501 A:500 ## 1st Qu.: 6.281 1st Qu.:24.54960 B:500 ## Median : 25.405 Median :48.63585 ## Mean : 26.348 Mean :49.57381 ## 3rd Qu.: 45.679 3rd Qu.:75.13851 ## Max. :124.211 Max. :99.97250 any(is.na(my_data.df)) ## [1] FALSE ggplot(data=my_data.df,aes(x=x,y=y,color=f)) + geom_point() + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; 10.4 Impute if needed x &lt;- c(1,2,5,NA,NA) is.na(x) ## [1] FALSE FALSE FALSE TRUE TRUE sum(is.na(x)) ## [1] 2 sum(is.na(x))/length(x) ## [1] 0.4 any(is.na(x)) ## [1] TRUE mean_x &lt;- mean(x, na.rm = T) mean_x -&gt; x[is.na(x)] x ## [1] 1.000000 2.000000 5.000000 2.666667 2.666667 rm(x) 10.5 Linear modelling 10.5.1 Single term model_1 &lt;- lm(y~x, data=my_data.df) summary(model_1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.561 -20.235 -0.924 19.229 97.282 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.62736 1.82712 14.026 &lt;2e-16 *** ## x 0.01454 0.03185 0.457 0.648 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.07 on 998 degrees of freedom ## Multiple R-squared: 0.0002088, Adjusted R-squared: -0.000793 ## F-statistic: 0.2084 on 1 and 998 DF, p-value: 0.6481 plot(model_1) plot(y~x,data=my_data.df) abline(model_1, col=&quot;red&quot;) 10.5.2 Multiple terms without interaction model_2 &lt;- lm(y~x+f, data=my_data.df) #equivalent: note using dot for ALL terms model_2_dot &lt;- lm(y~., data=my_data.df) summary(model_2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.589 -19.866 -0.893 19.028 98.291 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.59362 2.05071 11.993 &lt;2e-16 *** ## x 0.01482 0.03185 0.465 0.642 ## fB 2.03987 1.83825 1.110 0.267 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.06 on 997 degrees of freedom ## Multiple R-squared: 0.001442, Adjusted R-squared: -0.000561 ## F-statistic: 0.7199 on 2 and 997 DF, p-value: 0.487 summary(model_2_dot) ## ## Call: ## lm(formula = y ~ ., data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.589 -19.866 -0.893 19.028 98.291 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.59362 2.05071 11.993 &lt;2e-16 *** ## x 0.01482 0.03185 0.465 0.642 ## fB 2.03987 1.83825 1.110 0.267 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.06 on 997 degrees of freedom ## Multiple R-squared: 0.001442, Adjusted R-squared: -0.000561 ## F-statistic: 0.7199 on 2 and 997 DF, p-value: 0.487 plot(model_2) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_2, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_2, col = &quot;blue&quot;, lwd = 3): only using the first two of 3 regression coefficients 10.5.3 With interaction model_3 &lt;- lm(y~x*f, data=my_data.df) summary(model_3) ## ## Call: ## lm(formula = y ~ x * f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65.584 -16.738 -0.392 15.799 80.534 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.71731 2.28254 0.314 0.753 ## x 0.49425 0.03966 12.461 &lt;2e-16 *** ## fB 49.49979 3.21511 15.396 &lt;2e-16 *** ## x:fB -0.95735 0.05605 -17.080 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.57 on 996 degrees of freedom ## Multiple R-squared: 0.2277, Adjusted R-squared: 0.2253 ## F-statistic: 97.87 on 3 and 996 DF, p-value: &lt; 2.2e-16 plot(model_3) plot(y~x,col=as.integer(f),data=my_data.df) abline(model_3, col=&quot;blue&quot;, lwd=3) ## Warning in abline(model_3, col = &quot;blue&quot;, lwd = 3): only using the first two of 4 regression coefficients 10.6 lm could be evaluated by ANOVA https://stats.stackexchange.com/questions/115304/interpreting-output-from-anova-when-using-lm-as-input Anova output may be more clear when categorical variables have more than 3 levels (we discussed it during the practical session) Note that there are at least two slightly different functions for ANOVA in R: anova() and aov() anova(model_1) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 176 176.10 0.2084 0.6481 ## Residuals 998 843244 844.93 anova(model_2) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 176 176.10 0.2085 0.6481 ## f 1 1040 1040.20 1.2314 0.2674 ## Residuals 997 842204 844.74 anova(model_3) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 176 176 0.2693 0.6039 ## f 1 1040 1040 1.5905 0.2076 ## x:f 1 190803 190803 291.7397 &lt;2e-16 *** ## Residuals 996 651401 654 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.1 Anova could be used to compare models This can be used in the following way: First a “null” model could be calculated with all the “confounders”/“covariates” Then a predictor of interest is added to the model and models are compared by ANOVA If the model is improved significantly, then the predictor is important anova(model_1,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 843244 ## 2 996 651401 2 191843 146.67 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model_1,model_2,model_3) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ x + f ## Model 3: y ~ x * f ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 998 843244 ## 2 997 842204 1 1040 1.5905 0.2076 ## 3 996 651401 1 190803 291.7397 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.6.2 A bit of OOP about anova(lm) # What other methods could be applied to lm object? class(model_1) ## [1] &quot;lm&quot; methods(class=&quot;lm&quot;) ## [1] add1 alias anova case.names coerce confint cooks.distance deviance dfbeta dfbetas drop1 dummy.coef effects extractAIC family formula fortify hatvalues influence initialize kappa labels logLik model.frame model.matrix nobs plot predict print proj qqnorm qr residuals rstandard rstudent show simulate slotsFromS3 summary variable.names vcov ## see &#39;?methods&#39; for accessing help and source code # What other classes coul be evaluated by ANOVA? methods(anova) ## [1] anova.gam* anova.glm* anova.glmlist* anova.gls* anova.lm* anova.lme* anova.lmlist* anova.loess* anova.mlm* anova.nls* ## see &#39;?methods&#39; for accessing help and source code #?aov #?anova #?anova.lm 10.7 Predict new_data.df &lt;- data.frame(x=c(10,20), f=factor(c(&quot;A&quot;,&quot;B&quot;))) new_data.df ## x f ## 1 10 A ## 2 20 B predict(model_1,new_data.df) ## 1 2 ## 25.77277 25.91819 predict(model_2,new_data.df) ## 1 2 ## 24.74182 26.92989 10.8 Update It is very rarely used, but occasionally could be seen in help and tutorials Note using dots on the left and right of ~ in the formula m1 = lm(y~x, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.561 -20.235 -0.924 19.229 97.282 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.62736 1.82712 14.026 &lt;2e-16 *** ## x 0.01454 0.03185 0.457 0.648 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.07 on 998 degrees of freedom ## Multiple R-squared: 0.0002088, Adjusted R-squared: -0.000793 ## F-statistic: 0.2084 on 1 and 998 DF, p-value: 0.6481 m2 = update(m1,.~.+f, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.589 -19.866 -0.893 19.028 98.291 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.59362 2.05071 11.993 &lt;2e-16 *** ## x 0.01482 0.03185 0.465 0.642 ## fB 2.03987 1.83825 1.110 0.267 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.06 on 997 degrees of freedom ## Multiple R-squared: 0.001442, Adjusted R-squared: -0.000561 ## F-statistic: 0.7199 on 2 and 997 DF, p-value: 0.487 10.9 Crooked things yet to explore … We could not discuss these because we were short of time (and I have limited understanding of them ?) 10.9.1 Intercept, design matrix See my PowerPoint slides for students with biological background Intercept could be set to zero by two ways: y ~ 0 + x y ~ x - 1 Something strange happens without intercept in the model. This something could be undertood by staring at “design matrix” m1 &lt;- lm(y~x+f, data=my_data.df) summary(m1) ## ## Call: ## lm(formula = y ~ x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.589 -19.866 -0.893 19.028 98.291 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.59362 2.05071 11.993 &lt;2e-16 *** ## x 0.01482 0.03185 0.465 0.642 ## fB 2.03987 1.83825 1.110 0.267 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.06 on 997 degrees of freedom ## Multiple R-squared: 0.001442, Adjusted R-squared: -0.000561 ## F-statistic: 0.7199 on 2 and 997 DF, p-value: 0.487 m2 &lt;- lm(y~x+f-1, data=my_data.df) summary(m2) ## ## Call: ## lm(formula = y ~ x + f - 1, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.589 -19.866 -0.893 19.028 98.291 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.01482 0.03185 0.465 0.642 ## fA 24.59362 2.05071 11.993 &lt;2e-16 *** ## fB 26.63348 2.03953 13.059 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.06 on 997 degrees of freedom ## Multiple R-squared: 0.4523, Adjusted R-squared: 0.4506 ## F-statistic: 274.4 on 3 and 997 DF, p-value: &lt; 2.2e-16 m3 &lt;- lm(y~0+x+f, data=my_data.df) summary(m3) ## ## Call: ## lm(formula = y ~ 0 + x + f, data = my_data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.589 -19.866 -0.893 19.028 98.291 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 0.01482 0.03185 0.465 0.642 ## fA 24.59362 2.05071 11.993 &lt;2e-16 *** ## fB 26.63348 2.03953 13.059 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.06 on 997 degrees of freedom ## Multiple R-squared: 0.4523, Adjusted R-squared: 0.4506 ## F-statistic: 274.4 on 3 and 997 DF, p-value: &lt; 2.2e-16 data(&quot;mtcars&quot;) other_data &lt;- mtcars[1:10,c(&quot;vs&quot;,&quot;am&quot;,&quot;gear&quot;)] other_data ## vs am gear ## Mazda RX4 0 1 4 ## Mazda RX4 Wag 0 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 0 0 3 ## Valiant 1 0 3 ## Duster 360 0 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 model.matrix(~am+gear, data=other_data) ## (Intercept) am gear ## Mazda RX4 1 1 4 ## Mazda RX4 Wag 1 1 4 ## Datsun 710 1 1 4 ## Hornet 4 Drive 1 0 3 ## Hornet Sportabout 1 0 3 ## Valiant 1 0 3 ## Duster 360 1 0 3 ## Merc 240D 1 0 4 ## Merc 230 1 0 4 ## Merc 280 1 0 4 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 model.matrix(~0+am+gear, data=other_data) ## am gear ## Mazda RX4 1 4 ## Mazda RX4 Wag 1 4 ## Datsun 710 1 4 ## Hornet 4 Drive 0 3 ## Hornet Sportabout 0 3 ## Valiant 0 3 ## Duster 360 0 3 ## Merc 240D 0 4 ## Merc 230 0 4 ## Merc 280 0 4 ## attr(,&quot;assign&quot;) ## [1] 1 2 10.9.2 Contrasts Also, there is some cryptic way allowing to specify what sub-groups to include/exclude in the comparison: this is something to do with “contrasts” https://rcompanion.org/rcompanion/h_01.html https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r contrasts(my_data.df$f) ## B ## A 0 ## B 1 contrasts(as.factor(other_data$gear)) ## 4 ## 3 0 ## 4 1 10.9.3 Type I, II and III sums ANOVA and linear models are, in fact equivalent to each other (even at the level of implementation). Both rely on calculating some sum of squares. There are 3 ways of calculating these sums: Type I, Type II and Type III Google about these … https://rcompanion.org/rcompanion/d_04.html "],["real-life-lm-example.html", "Section 11 Real life lm() example 11.1 Summary 11.2 Start section 11.3 Read data 11.4 Explore and prepare data 11.5 Explore in a univariate manner 11.6 Model with all variables 11.7 Drop1 11.8 Optimal model 11.9 Predict", " Section 11 Real life lm() example 11.1 Summary This is only the first orientation with the dataset. This example: Reads data Explores missingnes and does simplistic imputation Looks at univariate regression/anova for each variable: all look highly significant Multivariate model with all variables: suggests dropping owners drop1 suggests keeping only make, engine.size and mileage Importantly, the example does NOT analyze the normality of residuals in the models etc. It does NOT considers interactions. Further directions may also include: transformation of variables using generalized models using add1 estimating accuracy of prediction etc. 11.2 Start section 11.3 Read data data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) rm(data_file) 11.4 Explore and prepare data Data clean-up and preparation is a separate large realm in data science. Here we just check that data look well structured, check the data size, types of variables, missingness. 11.4.1 Overall size and structure dim(data.df) ## [1] 3780 6 str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... 11.4.2 Missingness count.na.udf &lt;- function(x)(sum(is.na(x))) count_na &lt;- apply(data.df,2,count.na.udf) count_na ## mileage manufactured.year engine.size owners price make ## 0 0 2 1968 0 0 count_na/nrow(data.df) ## mileage manufactured.year engine.size owners price make ## 0.0000000000 0.0000000000 0.0005291005 0.5206349206 0.0000000000 0.0000000000 rm(count.na.udf,count_na) 11.4.3 Impute missed Its a simplistic procedure, with its pros and contras. Strictly speaking, it may not be necessary if lm() has its own imputation in place. However, it may not be imediately clear what exactly this default implicit procedure is … So, it may be useful to deal with missed data explicitly to be sure how they were treated. On a technical point: I recollect that there could be a specialized function for applying mean in place of missed data. mean_engine_size &lt;- mean(data.df$engine.size, na.rm=T) mean_engine_size -&gt; data.df$engine.size[is.na(data.df$engine.size)] mean_owners &lt;- mean(data.df$owners, na.rm=T) mean_owners -&gt; data.df$owners[is.na(data.df$owners)] any(is.na(data.df)) ## [1] FALSE rm(mean_owners, mean_engine_size) 11.4.4 Calculate age Our previous knowledge in the field suggests that age may be a better representation for manufactured.year Should manufactured.year be removed from analysis after making age? May including both variables confuse the model (because of collinearuty). data.df &lt;- data.frame(data.df,age=2021-data.df$manufactured.year) str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 7 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : num 2.46 3 6 2.46 1 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... ## $ age : num 6 9 20 11 8 17 18 9 8 9 ... 11.5 Explore in a univariate manner 11.5.1 Anova for categorical variable makes data.df %&gt;% group_by(make) %&gt;% summarise(count=n(), mean_price=mean(price), mean_mileage=mean(mileage), mean_age=mean(age), mean_price=mean(price), mean_owners=mean(owners), mean_engine=mean(engine.size)) %&gt;% arrange(desc(count)) ## # A tibble: 43 x 7 ## make count mean_price mean_mileage mean_age mean_owners mean_engine ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vauxhall 715 6188. 44018. 6.09 2.36 1.42 ## 2 Ford 286 6502 52934. 7.21 2.46 1.34 ## 3 Nissan 269 8673. 39307. 5.90 2.31 1.33 ## 4 Hyundai 250 8521 31668. 4.35 2.44 1.30 ## 5 Volkswagen 208 5318. 74174. 10.4 3.03 1.51 ## 6 Toyota 199 6305. 48149. 8.34 2.10 1.55 ## 7 Peugeot 193 6488. 43657. 6.38 2.41 1.39 ## 8 Citroen 165 4882. 53608. 8.18 2.40 1.44 ## 9 Renault 164 6506. 41006. 6.68 2.37 1.34 ## 10 SEAT 156 6342. 53444. 6.63 2.25 1.40 ## # … with 33 more rows boxplot(price~make, data=data.df) summary(aov(price~make, data=data.df)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## make 42 1.171e+10 278725988 21.38 &lt;2e-16 *** ## Residuals 3737 4.871e+10 13035655 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.5.2 Univariate lm() for numeric variables There is a way of doing this in one step … 11.5.2.1 mileage mileage.lm &lt;- lm(price~mileage, data=data.df) summary(mileage.lm) ## ## Call: ## lm(formula = price ~ mileage, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6878.8 -1971.3 -661.8 1175.8 29729.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.780e+03 9.264e+01 105.57 &lt;2e-16 *** ## mileage -5.909e-02 1.469e-03 -40.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3346 on 3778 degrees of freedom ## Multiple R-squared: 0.2999, Adjusted R-squared: 0.2997 ## F-statistic: 1618 on 1 and 3778 DF, p-value: &lt; 2.2e-16 mileage.lm.p=coefficients(summary(mileage.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~mileage, data=data.df, main=paste(&quot;Univariate mileage p =&quot;,round(mileage.lm.p,5))) abline(mileage.lm, col=&quot;red&quot;, lwd=3) rm(mileage.lm,mileage.lm.p) 11.5.2.2 age age.lm &lt;- lm(price~age, data=data.df) summary(age.lm) ## ## Call: ## lm(formula = price ~ age, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4784 -1868 -717 898 36767 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11038.23 95.84 115.18 &lt;2e-16 *** ## age -584.32 11.21 -52.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3050 on 3778 degrees of freedom ## Multiple R-squared: 0.4183, Adjusted R-squared: 0.4182 ## F-statistic: 2717 on 1 and 3778 DF, p-value: &lt; 2.2e-16 age.lm.p=coefficients(summary(age.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~age, data=data.df, main=paste(&quot;Univariate age p =&quot;,round(age.lm.p,5))) abline(age.lm, col=&quot;red&quot;, lwd=3) rm(age.lm,age.lm.p) 11.5.2.3 owners owners.lm &lt;- lm(price~owners, data=data.df) summary(owners.lm) ## ## Call: ## lm(formula = price ~ owners, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7583.5 -2683.5 -343.3 1862.9 31164.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9651.66 161.15 59.89 &lt;2e-16 *** ## owners -1173.18 60.43 -19.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3813 on 3778 degrees of freedom ## Multiple R-squared: 0.09071, Adjusted R-squared: 0.09047 ## F-statistic: 376.9 on 1 and 3778 DF, p-value: &lt; 2.2e-16 owners.lm.p=coefficients(summary(owners.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~owners, data=data.df, main=paste(&quot;Univariate owners p =&quot;,round(owners.lm.p,5))) abline(owners.lm, col=&quot;red&quot;, lwd=3) rm(owners.lm,owners.lm.p) 11.5.2.4 engine.size engine.size.lm &lt;- lm(price~engine.size, data=data.df) summary(engine.size.lm) ## ## Call: ## lm(formula = price ~ engine.size, data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7582.8 -2853.8 -75.1 2179.3 26531.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4682.0 231.5 20.224 &lt;2e-16 *** ## engine.size 1393.1 148.8 9.362 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3954 on 3778 degrees of freedom ## Multiple R-squared: 0.02267, Adjusted R-squared: 0.02241 ## F-statistic: 87.65 on 1 and 3778 DF, p-value: &lt; 2.2e-16 engine.size.lm.p=coefficients(summary(engine.size.lm))[2,&quot;Pr(&gt;|t|)&quot;] plot(price~engine.size, data=data.df, main=paste(&quot;Univariate engine.size p =&quot;,round(engine.size.lm.p,5))) abline(engine.size.lm, col=&quot;red&quot;, lwd=3) rm(engine.size.lm,engine.size.lm.p) 11.6 Model with all variables It seems that owners are not very informative: could it be because we imputed half of them full.m &lt;- lm(price~., data=data.df) #summary(full.m) anova(full.m) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.8708 &lt;2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.3510 &lt;2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.5751 &lt;2e-16 *** ## owners 1 1.0695e+06 1.0695e+06 0.2467 0.6195 ## make 42 6.9638e+09 1.6580e+08 38.2404 &lt;2e-16 *** ## Residuals 3733 1.6186e+10 4.3359e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.7 Drop1 I think that drop1 suggests to keep only make, engine.size, mileage https://stats.stackexchange.com/questions/4639/interpreting-the-drop1-output-in-r drop1.m &lt;- drop1(full.m, test=&quot;F&quot;) #drop1.m drop1.m[order(drop1.m$`Pr(&gt;F)`),] ## Single term deletions ## ## Model: ## price ~ mileage + manufactured.year + engine.size + owners + ## make + age ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## make 42 6963803785 2.3150e+10 59083 38.2404 &lt;2e-16 *** ## engine.size 1 3549033541 1.9735e+10 58562 818.5316 &lt;2e-16 *** ## mileage 1 1900987496 1.8087e+10 58232 438.4344 &lt;2e-16 *** ## owners 1 4803073 1.6191e+10 57813 1.1078 0.2926 ## &lt;none&gt; 1.6186e+10 57814 ## manufactured.year 0 0 1.6186e+10 57814 ## age 0 0 1.6186e+10 57814 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 rm(full.m,drop1.m) 11.8 Optimal model optimal.m &lt;- lm(price~make + engine.size + mileage, data=data.df) anova(optimal.m) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## make 42 1.1706e+10 2.7873e+08 37.793 &lt; 2.2e-16 *** ## engine.size 1 1.1660e+08 1.1660e+08 15.810 7.136e-05 *** ## mileage 1 2.1052e+10 2.1052e+10 2854.492 &lt; 2.2e-16 *** ## Residuals 3735 2.7546e+10 7.3750e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.9 Predict new_data.df &lt;- list(make=c(&quot;Citroen&quot;,&quot;Audi&quot;,&quot;Mercedes-Benz&quot;), engine.size=c(1,1.5,3), mileage=c(70000,50000,10000)) predicted_price &lt;- predict(optimal.m,newdata = new_data.df) new_data.df &lt;- data.frame(new_data.df, predicted_price) new_data.df ## make engine.size mileage predicted_price ## 1 Citroen 1.0 70000 2653.858 ## 2 Audi 1.5 50000 10086.041 ## 3 Mercedes-Benz 3.0 10000 15371.881 rm(predicted_price) "],["stepwise-selection-of-predictors-in-lm.html", "Section 12 Stepwise selection of predictors in lm() 12.1 Prepare data 12.2 The full model 12.3 Stepwise selection 12.4 A sidetrack about seed", " Section 12 Stepwise selection of predictors in lm() 12.1 Prepare data 12.1.1 Load data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) rm(data_file) str(data.df) ## &#39;data.frame&#39;: 3780 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... head(data.df) ## mileage manufactured.year engine.size owners price make ## 1 32000 2015 1.6 NA 7495 Citroen ## 2 69000 2012 2.0 3 7000 Audi ## 3 95300 2001 3.2 6 8000 Mercedes-Benz ## 4 85962 2010 2.7 NA 7490 Audi ## 5 64000 2013 1.8 1 7000 Audi ## 6 47053 2004 3.0 4 7995 BMW dim(data.df) ## [1] 3780 6 12.1.2 Impute missed count.na.udf &lt;- function(x)(sum(is.na(x))) count_na &lt;- apply(data.df,2,count.na.udf) count_na ## mileage manufactured.year engine.size owners price make ## 0 0 2 1968 0 0 count_na/nrow(data.df) ## mileage manufactured.year engine.size owners price make ## 0.0000000000 0.0000000000 0.0005291005 0.5206349206 0.0000000000 0.0000000000 mean_engine_size &lt;- mean(data.df$engine.size, na.rm=T) mean_engine_size -&gt; data.df$engine.size[is.na(data.df$engine.size)] mean_owners &lt;- mean(data.df$owners, na.rm=T) mean_owners -&gt; data.df$owners[is.na(data.df$owners)] any(is.na(data.df)) ## [1] FALSE rm(count.na.udf,count_na,mean_owners, mean_engine_size) 12.2 The full model This is a full model : it uses all the data lm.full &lt;- lm(price~.,data=data.df) summary(lm.full) ## ## Call: ## lm(formula = price ~ ., data = data.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11040.4 -1175.1 -309.0 748.6 20590.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.147e+06 2.352e+04 -48.737 &lt; 2e-16 *** ## mileage -2.808e-02 1.341e-03 -20.939 &lt; 2e-16 *** ## manufactured.year 5.717e+02 1.163e+01 49.176 &lt; 2e-16 *** ## engine.size 3.004e+03 1.050e+02 28.610 &lt; 2e-16 *** ## owners -4.082e+01 3.878e+01 -1.053 0.292638 ## makeAlfa Romeo -2.015e+03 8.290e+02 -2.430 0.015129 * ## makeAston Martin 2.576e+04 1.652e+03 15.590 &lt; 2e-16 *** ## makeAudi 1.853e+03 7.364e+02 2.517 0.011887 * ## makeBMW 1.434e+03 7.283e+02 1.969 0.049021 * ## makeChevrolet -2.511e+03 9.055e+02 -2.773 0.005576 ** ## makeChrysler -3.748e+03 9.886e+02 -3.791 0.000152 *** ## makeCitroen -2.097e+03 7.151e+02 -2.933 0.003380 ** ## makeDacia -2.873e+03 7.613e+02 -3.774 0.000163 *** ## makeDaihatsu -7.264e+01 2.197e+03 -0.033 0.973627 ## makeDodge -1.493e+03 1.257e+03 -1.188 0.234965 ## makeDS AUTOMOBILES -2.429e+03 9.203e+02 -2.639 0.008341 ** ## makeFiat -1.868e+03 7.163e+02 -2.608 0.009156 ** ## makeFord -7.337e+02 7.067e+02 -1.038 0.299247 ## makeHonda -5.571e+01 7.642e+02 -0.073 0.941890 ## makeHyundai -8.206e+02 7.082e+02 -1.159 0.246658 ## makeInfiniti 5.677e+02 1.390e+03 0.408 0.683041 ## makeIsuzu -1.261e+03 2.202e+03 -0.573 0.566998 ## makeJaguar -1.572e+03 8.844e+02 -1.778 0.075508 . ## makeJeep -1.057e+03 1.260e+03 -0.839 0.401694 ## makeKia -9.860e+02 7.296e+02 -1.351 0.176644 ## makeLand Rover 4.365e+02 1.258e+03 0.347 0.728734 ## makeLexus 2.837e+03 8.404e+02 3.376 0.000744 *** ## makeMazda -1.161e+02 7.339e+02 -0.158 0.874260 ## makeMercedes-Benz 1.450e+03 7.356e+02 1.971 0.048833 * ## makeMG -1.746e+03 9.395e+02 -1.859 0.063108 . ## makeMINI 1.445e+02 7.190e+02 0.201 0.840741 ## makeMitsubishi -1.081e+03 8.086e+02 -1.337 0.181427 ## makeNissan 3.130e+02 7.077e+02 0.442 0.658330 ## makePeugeot -1.647e+03 7.119e+02 -2.313 0.020761 * ## makePorsche 1.289e+04 9.951e+02 12.948 &lt; 2e-16 *** ## makeRenault -1.382e+03 7.150e+02 -1.933 0.053264 . ## makeSaab -2.424e+03 9.070e+02 -2.673 0.007547 ** ## makeSEAT -1.419e+03 7.160e+02 -1.982 0.047509 * ## makeSKODA -1.344e+03 7.607e+02 -1.767 0.077371 . ## makeSmart -1.129e+03 8.612e+02 -1.311 0.190009 ## makeSsangYong -3.613e+03 2.200e+03 -1.643 0.100560 ## makeSubaru -2.250e+03 1.018e+03 -2.210 0.027170 * ## makeSuzuki -1.701e+03 7.513e+02 -2.264 0.023656 * ## makeToyota -1.070e+03 7.134e+02 -1.500 0.133588 ## makeVauxhall -2.188e+03 7.004e+02 -3.124 0.001796 ** ## makeVolkswagen -1.189e+01 7.106e+02 -0.017 0.986651 ## makeVolvo -7.560e+02 7.654e+02 -0.988 0.323307 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2082 on 3733 degrees of freedom ## Multiple R-squared: 0.7321, Adjusted R-squared: 0.7288 ## F-statistic: 221.8 on 46 and 3733 DF, p-value: &lt; 2.2e-16 #coefficients(summary(lm.full)) anova(lm.full) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.8708 &lt;2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.3510 &lt;2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.5751 &lt;2e-16 *** ## owners 1 1.0695e+06 1.0695e+06 0.2467 0.6195 ## make 42 6.9638e+09 1.6580e+08 38.2404 &lt;2e-16 *** ## Residuals 3733 1.6186e+10 4.3359e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 formula(lm.full) ## price ~ mileage + manufactured.year + engine.size + owners + ## make 12.3 Stepwise selection base R drop1, add1, step http://rstudio-pubs-static.s3.amazonaws.com/2899_a9129debf6bd47d2a0501de9c0dc583d.html “advanced” solutions MASS::stepAIC leaps package rms::fastbw olsrr package 12.3.1 backward step.bk &lt;- step(lm.full, direction = &quot;backward&quot;, trace = 1) ## Start: AIC=57814.29 ## price ~ mileage + manufactured.year + engine.size + owners + ## make ## ## Df Sum of Sq RSS AIC ## - owners 1 4.8031e+06 1.6191e+10 57813 ## &lt;none&gt; 1.6186e+10 57814 ## - mileage 1 1.9010e+09 1.8087e+10 58232 ## - engine.size 1 3.5490e+09 1.9735e+10 58562 ## - make 42 6.9638e+09 2.3150e+10 59083 ## - manufactured.year 1 1.0485e+10 2.6671e+10 59700 ## ## Step: AIC=57813.41 ## price ~ mileage + manufactured.year + engine.size + make ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.6191e+10 57813 ## - mileage 1 1.9644e+09 1.8155e+10 58244 ## - engine.size 1 3.5511e+09 1.9742e+10 58561 ## - make 42 6.9601e+09 2.3151e+10 59081 ## - manufactured.year 1 1.1355e+10 2.7546e+10 59820 formula(step.bk) ## price ~ mileage + manufactured.year + engine.size + make anova(step.bk) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mileage 1 1.8119e+10 1.8119e+10 4178.750 &lt; 2.2e-16 *** ## manufactured.year 1 8.2483e+09 8.2483e+09 1902.296 &lt; 2.2e-16 *** ## engine.size 1 1.0903e+10 1.0903e+10 2514.503 &lt; 2.2e-16 *** ## make 42 6.9601e+09 1.6572e+08 38.219 &lt; 2.2e-16 *** ## Residuals 3734 1.6191e+10 4.3360e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 step.bk$anova ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 3733 16185742445 57814.29 ## 2 - owners 1 4803073 3734 16190545517 57813.41 12.3.2 forward Requires null model: for data and intercept scope: for predictors to add lm.null &lt;- lm(price~1,data=data.df) step.fw &lt;- step(lm.null, scope= ~owners+mileage+engine.size+make+manufactured.year, direction = &quot;forward&quot;, trace = 1) ## Start: AIC=62701.31 ## price ~ 1 ## ## Df Sum of Sq RSS AIC ## + manufactured.year 1 2.5275e+10 3.5146e+10 60655 ## + mileage 1 1.8119e+10 4.2302e+10 61356 ## + make 42 1.1706e+10 4.8714e+10 61971 ## + owners 1 5.4810e+09 5.4940e+10 62344 ## + engine.size 1 1.3700e+09 5.9051e+10 62617 ## &lt;none&gt; 6.0421e+10 62701 ## ## Step: AIC=60655.17 ## price ~ manufactured.year ## ## Df Sum of Sq RSS AIC ## + make 42 1.4026e+10 2.1120e+10 58814 ## + engine.size 1 9.6948e+09 2.5451e+10 59437 ## + mileage 1 1.0921e+09 3.4053e+10 60538 ## &lt;none&gt; 3.5146e+10 60655 ## + owners 1 1.4655e+07 3.5131e+10 60656 ## ## Step: AIC=58814.04 ## price ~ manufactured.year + make ## ## Df Sum of Sq RSS AIC ## + engine.size 1 2964711644 1.8155e+10 58244 ## + mileage 1 1378008193 1.9742e+10 58561 ## + owners 1 59227052 2.1060e+10 58805 ## &lt;none&gt; 2.1120e+10 58814 ## ## Step: AIC=58244.27 ## price ~ manufactured.year + make + engine.size ## ## Df Sum of Sq RSS AIC ## + mileage 1 1964368341 1.6191e+10 57813 ## + owners 1 68183917 1.8087e+10 58232 ## &lt;none&gt; 1.8155e+10 58244 ## ## Step: AIC=57813.41 ## price ~ manufactured.year + make + engine.size + mileage ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1.6191e+10 57813 ## + owners 1 4803073 1.6186e+10 57814 formula(step.fw) ## price ~ manufactured.year + make + engine.size + mileage anova(step.fw) ## Analysis of Variance Table ## ## Response: price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## manufactured.year 1 2.5275e+10 2.5275e+10 5829.185 &lt; 2.2e-16 *** ## make 42 1.4026e+10 3.3395e+08 77.018 &lt; 2.2e-16 *** ## engine.size 1 2.9647e+09 2.9647e+09 683.747 &lt; 2.2e-16 *** ## mileage 1 1.9644e+09 1.9644e+09 453.039 &lt; 2.2e-16 *** ## Residuals 3734 1.6191e+10 4.3360e+06 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 step.bk$anova ## Step Df Deviance Resid. Df Resid. Dev AIC ## 1 NA NA 3733 16185742445 57814.29 ## 2 - owners 1 4803073 3734 16190545517 57813.41 12.4 A sidetrack about seed will be used in caret package x &lt;- 1:100 set.seed(42) sample(x,5) ## [1] 49 65 25 74 18 rm(x) "],["regression-trees.html", "Section 13 Regression trees 13.1 Summary 13.2 Start section 13.3 Prepare data 13.4 Simple tree model 13.5 Full tree without pruning 13.6 A model with manual settings control 13.7 Selecting optimal hyperparameters through enumeration 13.8 Bootstrap aggregating (bagging) 13.9 Bagging with caret 13.10 Confusion plots", " Section 13 Regression trees 13.1 Summary http://uc-r.github.io/regression_trees Also you may see for general R intro example: http://uc-r.github.io/ 13.2 Start section Sys.time() ## [1] &quot;2021-04-21 13:57:39 BST&quot; rm(list=ls()) graphics.off() gc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 2327936 124.4 4344478 232.1 NA 3332585 178 ## Vcells 4268223 32.6 10146329 77.5 16384 8387291 64 #install.packages(&quot;rpart.plot&quot;) #install.packages(&quot;AmesHousing&quot;) library(rsample) # data splitting library(dplyr) # data wrangling library(rpart) # performing regression trees library(rpart.plot) # plotting regression trees library(ipred) # bagging library(caret) # bagging ## Loading required package: lattice library(AmesHousing) # data to play with set.seed(123) 13.3 Prepare data 13.3.1 Data data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) data.df &lt;- unique(data.df) str(data.df) ## &#39;data.frame&#39;: 1600 obs. of 6 variables: ## $ mileage : num 32000 69000 95300 85962 64000 ... ## $ manufactured.year: int 2015 2012 2001 2010 2013 2004 2003 2012 2013 2012 ... ## $ engine.size : num 1.6 2 3.2 2.7 1.8 3 5 2.1 1.8 2 ... ## $ owners : int NA 3 6 NA 1 4 3 3 3 2 ... ## $ price : num 7495 7000 8000 7490 7000 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 8 4 25 4 4 5 25 25 24 5 ... head(data.df) ## mileage manufactured.year engine.size owners price make ## 1 32000 2015 1.6 NA 7495 Citroen ## 2 69000 2012 2.0 3 7000 Audi ## 3 95300 2001 3.2 6 8000 Mercedes-Benz ## 4 85962 2010 2.7 NA 7490 Audi ## 5 64000 2013 1.8 1 7000 Audi ## 6 47053 2004 3.0 4 7995 BMW dim(data.df) ## [1] 1600 6 rm(data_file) 13.3.2 Split data_split &lt;- initial_split(data.df) names(data_split) ## [1] &quot;data&quot; &quot;in_id&quot; &quot;out_id&quot; &quot;id&quot; data_train &lt;- training(data_split) data_test &lt;- testing(data_split) 13.4 Simple tree model Uses default pruning settings m1 &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot; ) m1 ## n= 1200 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 1200 25233610000 7217.132 ## 2) manufactured.year&lt; 2012.5 360 4569798000 3340.072 ## 4) engine.size&lt; 3.1 350 1440377000 3008.423 ## 8) manufactured.year&lt; 2010.5 261 719566900 2484.115 * ## 9) manufactured.year&gt;=2010.5 89 438652500 4546.000 * ## 5) engine.size&gt;=3.1 10 1743531000 14947.800 * ## 3) manufactured.year&gt;=2012.5 840 12933280000 8878.729 ## 6) engine.size&lt; 2.45 829 8580714000 8635.245 ## 12) manufactured.year&lt; 2016.5 390 3016570000 7224.836 ## 24) make=Abarth,Alfa Romeo,Chevrolet,Chrysler,Citroen,Dacia,DS AUTOMOBILES,Fiat,Ford,Honda,Hyundai,Infiniti,Kia,Mazda,MINI,Mitsubishi,Nissan,Peugeot,Renault,SEAT,SKODA,Smart,Suzuki,Toyota,Vauxhall,Volkswagen,Volvo 356 1369669000 6784.680 ## 48) make=Alfa Romeo,Chevrolet,Chrysler,Citroen,Dacia,DS AUTOMOBILES,Fiat,Ford,Hyundai,Kia,Mitsubishi,Peugeot,Renault,SEAT,SKODA,Smart,Suzuki,Toyota,Vauxhall 259 631373900 6244.714 * ## 49) make=Abarth,Honda,Infiniti,Mazda,MINI,Nissan,Volkswagen,Volvo 97 461148500 8226.443 * ## 25) make=Audi,BMW,Jaguar,Lexus,Mercedes-Benz,Porsche 34 855767900 11833.530 ## 50) mileage&gt;=40446.5 27 193347700 10402.590 * ## 51) mileage&lt; 40446.5 7 393894700 17352.860 * ## 13) manufactured.year&gt;=2016.5 439 4099121000 9888.228 ## 26) engine.size&lt; 1.45 262 1154730000 8583.332 * ## 27) engine.size&gt;=1.45 177 1837909000 11819.770 ## 54) make=Citroen,Dacia,DS AUTOMOBILES,Fiat,MG,MINI,Peugeot,Renault,Suzuki,Vauxhall,Volvo 71 307640100 9776.577 * ## 55) make=Audi,BMW,Ford,Honda,Hyundai,Infiniti,Jeep,Kia,Lexus,Mazda,Mercedes-Benz,Nissan,SEAT,Toyota,Volkswagen 106 1035339000 13188.320 * ## 7) engine.size&gt;=2.45 11 599548200 27228.550 * rpart.plot(m1) plotcp(m1) 13.5 Full tree without pruning m2 &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(cp = 0, xval = 10) ) plotcp(m2) abline(v = 7, lty = &quot;dashed&quot;) abline(v = 20, lty = &quot;dashed&quot;, col=&quot;red&quot;) 13.6 A model with manual settings control m3 &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(minsplit = 10, maxdepth = 12, xval = 10) ) m3$cptable ## CP nsplit rel error xerror xstd ## 1 0.30635851 0 1.0000000 1.0026040 0.09546510 ## 2 0.14873099 1 0.6936415 0.7131700 0.10101577 ## 3 0.05805843 2 0.5449105 0.5948951 0.08586096 ## 4 0.05492239 3 0.4868521 0.5677906 0.07959552 ## 5 0.04384951 4 0.4319297 0.5439939 0.07924383 ## 6 0.03135232 5 0.3880802 0.4631415 0.06097607 ## 7 0.01961391 6 0.3567278 0.4104493 0.04707860 ## 8 0.01118181 7 0.3371139 0.3870748 0.04676690 ## 9 0.01098324 8 0.3259321 0.3843668 0.04663158 ## 10 0.01064158 9 0.3149489 0.3802017 0.04662678 ## 11 0.01000000 10 0.3043073 0.3786965 0.04662779 plotcp(m3) 13.7 Selecting optimal hyperparameters through enumeration 13.7.1 Make the hyperparameters grid hyper_grid &lt;- expand.grid( minsplit = seq(5, 20, 1), maxdepth = seq(8, 15, 1) ) head(hyper_grid) ## minsplit maxdepth ## 1 5 8 ## 2 6 8 ## 3 7 8 ## 4 8 8 ## 5 9 8 ## 6 10 8 tail(hyper_grid) ## minsplit maxdepth ## 123 15 15 ## 124 16 15 ## 125 17 15 ## 126 18 15 ## 127 19 15 ## 128 20 15 dim(hyper_grid) ## [1] 128 2 13.7.2 # Calculate models for all combinations in the hyperparameters grid models &lt;- list() for (i in 1:nrow(hyper_grid)) { # get minsplit, maxdepth values at row i minsplit &lt;- hyper_grid$minsplit[i] maxdepth &lt;- hyper_grid$maxdepth[i] # train a model and store in the list models[[i]] &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(minsplit = minsplit, maxdepth = maxdepth) ) } 13.7.3 Select hyperparameters with lowest x-validation error # function to get optimal cp get_cp &lt;- function(x) { min &lt;- which.min(x$cptable[, &quot;xerror&quot;]) cp &lt;- x$cptable[min, &quot;CP&quot;] } # function to get minimum error get_min_error &lt;- function(x) { min &lt;- which.min(x$cptable[, &quot;xerror&quot;]) xerror &lt;- x$cptable[min, &quot;xerror&quot;] } # Add cp and error to hyperparameters grid hyper_grid %&gt;% mutate( cp = purrr::map_dbl(models, get_cp), error = purrr::map_dbl(models, get_min_error) ) %&gt;% arrange(error) %&gt;% top_n(-5, wt = error) ## minsplit maxdepth cp error ## 1 12 9 0.01 0.3630854 ## 2 6 8 0.01 0.3651868 ## 3 9 8 0.01 0.3665252 ## 4 9 14 0.01 0.3668898 ## 5 16 8 0.01 0.3671934 13.7.4 The optimal tree optimal_tree &lt;- rpart( formula = price ~ ., data = data_train, method = &quot;anova&quot;, control = list(minsplit = 12, maxdepth = 9, cp = 0.01) ) rpart.plot(optimal_tree) 13.7.5 Prediction RMSE is the error estimate = how (on average) the predicted value is different from the actual value in the test set pred &lt;- predict(optimal_tree, newdata = data_test) RMSE(pred = pred, obs = data_test$price) ## [1] 2524.683 hist(data.df$price) quantile(data.df$price) ## 0% 25% 50% 75% 100% ## 250 3999 6850 9200 34999 13.8 Bootstrap aggregating (bagging) 13.8.1 bagged trees model with default settings Note Out-Of-Bag x-validation RMSE in the model summary # make bootstrapping reproducible set.seed(123) # train bagged model bagged_m1 &lt;- bagging( formula = price ~ ., data = data_train, coob = TRUE ) bagged_m1 ## ## Bagging regression trees with 25 bootstrap replications ## ## Call: bagging.data.frame(formula = price ~ ., data = data_train, coob = TRUE) ## ## Out-of-bag estimate of root mean squared error: 3251.254 13.8.2 Selecting optimal bag size # assess 10-50 bagged trees ntree &lt;- 10:150 # create empty vector to store OOB RMSE values rmse &lt;- vector(mode = &quot;numeric&quot;, length = length(ntree)) x &lt;- Sys.time() for (i in seq_along(ntree)) { # reproducibility set.seed(123) # perform bagged model model &lt;- bagging( formula = price ~ ., data = data_train, coob = TRUE, nbagg = ntree[i]) # get OOB error rmse[i] &lt;- model$err } y &lt;- Sys.time() y-x ## Time difference of 48.01311 secs plot(ntree, rmse, type = &#39;l&#39;, lwd = 2) abline(v = 60, col = &quot;red&quot;, lty = &quot;dashed&quot;) 13.9 Bagging with caret https://topepo.github.io/caret https://topepo.github.io/caret/available-models.html to be done: requires # Specify 10-fold cross validation ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10) # CV bagged model x &lt;- Sys.time() bagged_cv &lt;- train( price ~ ., data = na.omit(data_train), method = &quot;treebag&quot;, trControl = ctrl, importance = TRUE, ) y &lt;- Sys.time() y-x ## Time difference of 4.814894 secs bagged_cv ## Bagged CART ## ## 561 samples ## 5 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 504, 506, 505, 505, 505, 505, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3044.858 0.658359 1909.488 # plot most important variables plot(varImp(bagged_cv), 20) pred &lt;- predict(bagged_cv, data_test) RMSE(pred, data_test$price) ## Warning in pred - obs: longer object length is not a multiple of shorter object length ## [1] 5330.005 13.10 Confusion plots plot(na.omit(data_train)$price, predict(bagged_cv, na.omit(data_train)), main=&quot;Training set&quot;) abline(a=0,b=1,col=&quot;red&quot;,lty=2) R2 &lt;- cor(na.omit(data_train)$price, predict(bagged_cv, na.omit(data_train)))^2 legend(&quot;bottomright&quot;, legend=paste(&quot;R^2 =&quot;,round(R2,2)), bty=&quot;n&quot;) plot(na.omit(data_test)$price, predict(bagged_cv, na.omit(data_test)), main=&quot;Test set&quot;) abline(a=0,b=1,col=&quot;red&quot;,lty=2) R2 &lt;- cor(na.omit(data_test)$price, predict(bagged_cv, na.omit(data_test)))^2 legend(&quot;bottomright&quot;, legend=paste(&quot;R^2 =&quot;,round(R2,2)), bty=&quot;n&quot;) "],["ranger-in-the-caret.html", "Section 14 Ranger in the Caret 14.1 Summary 14.2 Start section 14.3 Prepare data 14.4 Split to training and test sets 14.5 Explore split data 14.6 Set parallel processing 14.7 Build model(s) 14.8 Evaluate models on the test set 14.9 Manual selection of the best model 14.10 Final section", " Section 14 Ranger in the Caret 14.1 Summary https://topepo.github.io/caret/ https://topepo.github.io/caret/available-models.html http://zevross.com/blog/2017/09/19/predictive-modeling-and-machine-learning-in-r-with-the-caret-package/ https://uc-r.github.io/random_forests 14.2 Start section Sys.time() ## [1] &quot;2021-04-21 13:58:47 BST&quot; rm(list=ls()) graphics.off() gc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 2767568 147.9 4344478 232.1 NA 4344478 232.1 ## Vcells 5013100 38.3 14815979 113.1 16384 14815979 113.1 # Load libraries library(caret) library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel library(corrplot) ## corrplot 0.84 loaded library(dplyr) library(ranger) # Reproducibility (it may be complicated in caret !) #set.seed(42) 14.3 Prepare data 14.3.1 Load raw data data_file=&quot;/Users/alexey/OneDrive/Documents/Home/Alex/stat_in_r_Mar2021/example/structure_data.csv&quot; data.df &lt;- read.csv(data_file) dim(data.df) ## [1] 3780 6 # Clean-up rm(data_file) 14.3.2 Remove duplictes data.df &lt;- unique(data.df) dim(data.df) ## [1] 1600 6 14.3.3 Remove records with missed data Could imputation be used for Random Forest? # Count missed data per field count.na.udf &lt;- function(x)(sum(is.na(x))) count_na &lt;- apply(data.df,2,count.na.udf) count_na ## mileage manufactured.year engine.size owners price make ## 0 0 2 830 0 0 count_na/nrow(data.df) ## mileage manufactured.year engine.size owners price make ## 0.00000 0.00000 0.00125 0.51875 0.00000 0.00000 # Remove missed data data.df &lt;- na.omit(data.df) dim(data.df) ## [1] 769 6 str(data.df) ## &#39;data.frame&#39;: 769 obs. of 6 variables: ## $ mileage : num 69000 95300 64000 47053 38655 ... ## $ manufactured.year: int 2012 2001 2013 2004 2003 2012 2013 2012 2009 2017 ... ## $ engine.size : num 2 3.2 1.8 3 5 2.1 1.8 2 1.8 1 ... ## $ owners : int 3 6 1 4 3 3 3 2 3 1 ... ## $ price : num 7000 8000 7000 7995 7999 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 4 25 4 5 25 25 24 5 25 30 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int 1 4 12 18 19 21 25 26 27 28 ... ## ..- attr(*, &quot;names&quot;)= chr &quot;1&quot; &quot;4&quot; &quot;12&quot; &quot;19&quot; ... # Clean-up rm(count.na.udf, count_na) 14.3.4 Exclude rare makes ad-hoc processing - based on previous exploration Excluding rare values is a difficult question: on one hand, we have little data about them, so it may not be prudent to include them to the model on the other hand, for numeric parameters (e.g. the oldest age, the highest price) the extreme values could be the rare ones and, sometime, an influential predictor / the most interesting response make_counts &lt;- sort(summary(data.df$make),decreasing = T) make_counts ## Vauxhall Nissan Volkswagen Ford BMW Mercedes-Benz Toyota Renault Fiat Peugeot MINI Audi Citroen Mazda SEAT Kia Honda Hyundai SKODA Volvo Suzuki Alfa Romeo Mitsubishi Lexus Dacia Jaguar MG Abarth Chrysler Porsche Smart Aston Martin Chevrolet DS AUTOMOBILES Saab Subaru Daihatsu Dodge Infiniti Isuzu Jeep SsangYong Land Rover ## 91 60 55 54 41 41 39 38 36 32 30 29 29 29 21 18 17 15 10 10 9 8 8 7 5 5 4 3 3 3 3 2 2 2 2 2 1 1 1 1 1 1 0 barplot(make_counts,las=2) abline(v=16.9, col=&quot;red&quot;, lty=2) rare_makes &lt;- names(make_counts[make_counts&lt;25]) rare_makes ## [1] &quot;SEAT&quot; &quot;Kia&quot; &quot;Honda&quot; &quot;Hyundai&quot; &quot;SKODA&quot; &quot;Volvo&quot; &quot;Suzuki&quot; &quot;Alfa Romeo&quot; &quot;Mitsubishi&quot; &quot;Lexus&quot; &quot;Dacia&quot; &quot;Jaguar&quot; &quot;MG&quot; &quot;Abarth&quot; &quot;Chrysler&quot; &quot;Porsche&quot; &quot;Smart&quot; &quot;Aston Martin&quot; &quot;Chevrolet&quot; &quot;DS AUTOMOBILES&quot; &quot;Saab&quot; &quot;Subaru&quot; &quot;Daihatsu&quot; &quot;Dodge&quot; &quot;Infiniti&quot; &quot;Isuzu&quot; &quot;Jeep&quot; &quot;SsangYong&quot; &quot;Land Rover&quot; str(data.df) ## &#39;data.frame&#39;: 769 obs. of 6 variables: ## $ mileage : num 69000 95300 64000 47053 38655 ... ## $ manufactured.year: int 2012 2001 2013 2004 2003 2012 2013 2012 2009 2017 ... ## $ engine.size : num 2 3.2 1.8 3 5 2.1 1.8 2 1.8 1 ... ## $ owners : int 3 6 1 4 3 3 3 2 3 1 ... ## $ price : num 7000 8000 7000 7995 7999 ... ## $ make : Factor w/ 43 levels &quot;Abarth&quot;,&quot;Alfa Romeo&quot;,..: 4 25 4 5 25 25 24 5 25 30 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int 1 4 12 18 19 21 25 26 27 28 ... ## ..- attr(*, &quot;names&quot;)= chr &quot;1&quot; &quot;4&quot; &quot;12&quot; &quot;19&quot; ... data.df &lt;- data.df[!as.vector(data.df$make) %in% rare_makes,] data.df$make &lt;- droplevels(data.df$make) str(data.df) ## &#39;data.frame&#39;: 604 obs. of 6 variables: ## $ mileage : num 69000 95300 64000 47053 38655 ... ## $ manufactured.year: int 2012 2001 2013 2004 2003 2012 2013 2012 2009 2017 ... ## $ engine.size : num 2 3.2 1.8 3 5 2.1 1.8 2 1.8 1 ... ## $ owners : int 3 6 1 4 3 3 3 2 3 1 ... ## $ price : num 7000 8000 7000 7995 7999 ... ## $ make : Factor w/ 14 levels &quot;Audi&quot;,&quot;BMW&quot;,&quot;Citroen&quot;,..: 1 7 1 2 7 7 6 2 7 10 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int 1 4 12 18 19 21 25 26 27 28 ... ## ..- attr(*, &quot;names&quot;)= chr &quot;1&quot; &quot;4&quot; &quot;12&quot; &quot;19&quot; ... rm(rare_makes, make_counts) 14.3.5 Transformation(s) Use age instead of manufacture year Other things, like PC-s or log-transformations, could also be calculated at this (?) stage data.df &lt;- data.df %&gt;% mutate(age=2021-manufactured.year) %&gt;% select(-manufactured.year) str(data.df) ## &#39;data.frame&#39;: 604 obs. of 6 variables: ## $ mileage : num 69000 95300 64000 47053 38655 ... ## $ engine.size: num 2 3.2 1.8 3 5 2.1 1.8 2 1.8 1 ... ## $ owners : int 3 6 1 4 3 3 3 2 3 1 ... ## $ price : num 7000 8000 7000 7995 7999 ... ## $ make : Factor w/ 14 levels &quot;Audi&quot;,&quot;BMW&quot;,&quot;Citroen&quot;,..: 1 7 1 2 7 7 6 2 7 10 ... ## $ age : num 9 20 8 17 18 9 8 9 12 4 ... 14.4 Split to training and test sets Use caret function createDataPartition trainIndex &lt;- createDataPartition(data.df$price, times=1, p=0.7, list=F) trainIndex[1:5] ## [1] 2 9 10 11 13 length(trainIndex) ## [1] 424 train.df &lt;- data.df[trainIndex,] test.df &lt;- data.df[-trainIndex,] training_flag &lt;- as.factor(1:nrow(data.df) %in% trainIndex) # for plots rm(trainIndex) 14.5 Explore split data 14.5.1 Response distribution quantile(train.df$price) ## 0% 25% 50% 75% 100% ## 475 3000 5997 8995 34950 quantile(test.df$price) ## 0% 25% 50% 75% 100% ## 250.00 3051.75 5997.50 8995.00 34994.00 ggplot(data.df) + aes(price, fill=training_flag) + geom_density(colour=&quot;black&quot;, alpha=0.3) + ggtitle(&quot;Price&quot;) 14.5.2 Predictors distribution 14.5.2.1 mileage quantile(train.df$mileage) ## 0% 25% 50% 75% 100% ## 931.00 31332.75 53540.50 81000.00 496867.00 quantile(test.df$mileage) ## 0% 25% 50% 75% 100% ## 565.00 29902.25 52000.00 78000.00 179844.00 ggplot(data.df) + aes(mileage, fill=training_flag) + geom_density(colour=&quot;black&quot;, alpha=0.3) + geom_vline(xintercept=3e5, linetype=&quot;dashed&quot;, color = &quot;red&quot;) + labs(title = &quot;Mileage&quot;, subtitle = &quot;Should high mileage be removed from data ??&quot;) 14.5.2.2 engine.size quantile(train.df$engine.size) ## 0% 25% 50% 75% 100% ## 0.9 1.2 1.5 1.9 5.5 quantile(test.df$engine.size) ## 0% 25% 50% 75% 100% ## 0.900 1.200 1.500 1.825 5.000 ggplot(data.df) + aes(engine.size, fill=training_flag) + geom_density(colour=&quot;black&quot;, alpha=0.3) + ggtitle(&quot;Engine size&quot;) 14.5.2.3 Age quantile(train.df$age) ## 0% 25% 50% 75% 100% ## 1 5 7 11 37 quantile(test.df$age) ## 0% 25% 50% 75% 100% ## 1 5 8 12 23 ggplot(data.df) + aes(age, fill=training_flag) + geom_density(colour=&quot;black&quot;, alpha=0.3) + ggtitle(&quot;Age&quot;) 14.5.2.4 Owners table(data.df$owners,training_flag) ## training_flag ## FALSE TRUE ## 1 52 117 ## 2 57 137 ## 3 32 91 ## 4 18 39 ## 5 13 28 ## 6 4 8 ## 7 2 0 ## 8 2 2 ## 10 0 1 ## 12 0 1 ggplot(data.df) + aes(owners, fill=training_flag) + geom_density(colour=&quot;black&quot;, alpha=0.3) + ggtitle(&quot;Owners&quot;) ggplot(data.df) + aes(owners, fill=training_flag) + geom_histogram(colour=&quot;black&quot;, alpha=0.2, binwidth = 1) + labs(title = &quot;Owners&quot;, subtitle = &quot;stacked histogram&quot;, caption = &quot;Processed data&quot;) ggplot(data.df) + aes(owners, fill=training_flag) + geom_histogram(colour=&quot;black&quot;, alpha=0.2, binwidth = 1, position=&quot;identity&quot;) + labs(title = &quot;Owners&quot;, subtitle = &quot;overlaid histogram&quot;, caption = &quot;Processed data&quot;) 14.5.2.5 Make x &lt;- table(data.df$make,training_flag) x ## training_flag ## FALSE TRUE ## Audi 9 20 ## BMW 11 30 ## Citroen 9 20 ## Fiat 9 27 ## Ford 16 38 ## Mazda 9 20 ## Mercedes-Benz 13 28 ## MINI 9 21 ## Nissan 22 38 ## Peugeot 8 24 ## Renault 13 25 ## Toyota 13 26 ## Vauxhall 24 67 ## Volkswagen 15 40 y &lt;- x[order(rowSums(x), decreasing=T),] y ## training_flag ## FALSE TRUE ## Vauxhall 24 67 ## Nissan 22 38 ## Volkswagen 15 40 ## Ford 16 38 ## BMW 11 30 ## Mercedes-Benz 13 28 ## Toyota 13 26 ## Renault 13 25 ## Fiat 9 27 ## Peugeot 8 24 ## MINI 9 21 ## Audi 9 20 ## Citroen 9 20 ## Mazda 9 20 barplot(t(y), main=&quot;Make (pre-porcessed &amp; split data)&quot;, las=2) # Clean-up rm(x,y,training_flag) 14.5.3 Checks not used for this model 14.5.3.1 Predictors variability in training data numeric_predictors &lt;- c(&quot;mileage&quot;,&quot;engine.size&quot;,&quot;owners&quot;,&quot;age&quot;) nearZeroVar(train.df[,numeric_predictors], saveMetrics = T) ## freqRatio percentUnique zeroVar nzv ## mileage 1.000000 85.377358 FALSE FALSE ## engine.size 1.083333 5.424528 FALSE FALSE ## owners 1.170940 2.122642 FALSE FALSE ## age 1.000000 6.132075 FALSE FALSE x &lt;- data.frame(train.df,bad_predictor=1) str(x) ## &#39;data.frame&#39;: 424 obs. of 7 variables: ## $ mileage : num 95300 74500 22255 38926 35035 ... ## $ engine.size : num 3.2 1.8 1 1.5 1.2 1 1.2 1.4 1.4 1.4 ... ## $ owners : int 6 3 1 2 2 5 4 3 2 2 ... ## $ price : num 8000 7600 8995 9990 6495 ... ## $ make : Factor w/ 14 levels &quot;Audi&quot;,&quot;BMW&quot;,&quot;Citroen&quot;,..: 7 7 10 8 13 10 4 4 5 10 ... ## $ age : num 20 12 4 5 4 12 9 11 16 13 ... ## $ bad_predictor: num 1 1 1 1 1 1 1 1 1 1 ... numeric_predictors &lt;- c(&quot;mileage&quot;,&quot;engine.size&quot;,&quot;owners&quot;,&quot;age&quot;,&quot;bad_predictor&quot;) nearZeroVar(x[,numeric_predictors], saveMetrics = T) ## freqRatio percentUnique zeroVar nzv ## mileage 1.000000 85.3773585 FALSE FALSE ## engine.size 1.083333 5.4245283 FALSE FALSE ## owners 1.170940 2.1226415 FALSE FALSE ## age 1.000000 6.1320755 FALSE FALSE ## bad_predictor 0.000000 0.2358491 TRUE TRUE numeric_predictors &lt;- c(&quot;mileage&quot;,&quot;engine.size&quot;,&quot;owners&quot;,&quot;age&quot;) rm(x) 14.5.3.2 Scales of numeric predictors Centering and scaling may be important for some models, but not for Random Forest Equivalent for categorical predictors ?? featurePlot(x = train.df[,numeric_predictors], y = train.df$price, plot = &quot;scatter&quot;, layout = c(2,3)) # To do: Add regression lines 14.5.3.3 Correlation of numeric predictors Technical: use findCorrelation from caret Conceptual: How is it important for Random Forest ? What about categorical predictors (chi sq for contingency table) ? # Calculate correlations corMat &lt;- cor(train.df[,numeric_predictors]) corMat ## mileage engine.size owners age ## mileage 1.0000000 0.2066522 0.5397023 0.6020196 ## engine.size 0.2066522 1.0000000 0.2166323 0.2501745 ## owners 0.5397023 0.2166323 1.0000000 0.6505029 ## age 0.6020196 0.2501745 0.6505029 1.0000000 # Plot (is it better than heatmap?) corrplot(corMat, order=&quot;hclust&quot;, tl.cex=1) # Look at hightly correlated predictors highCorr &lt;- findCorrelation(corMat, cutoff=0.75) length(highCorr) ## [1] 0 #names(train.df[,numeric_predictors])[highCorr] # Clean-up rm(corMat,highCorr,numeric_predictors) 14.6 Set parallel processing Obviously, the cross-validation is naturally parallelisable. caret is an example of the new generation of R packages that may recognize and use multiple cores (using help from doMC package). registerDoMC(detectCores()) getDoParWorkers() ## [1] 4 14.7 Build model(s) 14.7.1 Configure resampling the small number of CV partitions is on purpose: to avoid issues with lack of data in branches/leaves during cross-validation # Faster, less robust (less prone to over-fitting ??) train_ctrl_1 &lt;- trainControl(method=&quot;cv&quot;, number = 5) # Slower, more robust (more prone to over-fitting ??) train_ctrl_2 &lt;- trainControl(method=&quot;repeatedcv&quot;, number = 5, repeats = 10) 14.7.2 Default model Use default grid of hyper-parameters set for ranger by caret The importance of varianbles could be evaluated by Gini or Permutation: https://alexisperrier.com/datascience/2015/08/27/feature-importance-random-forests-gini-accuracy.html # Train x &lt;- Sys.time() rangerFit1 &lt;- train(price ~ ., data = train.df, method = &quot;ranger&quot;, trControl = train_ctrl_2, importance = &quot;permutation&quot;, verbose = FALSE) Sys.time() - x ## Time difference of 47.68218 secs # Check result rangerFit1 ## Random Forest ## ## 424 samples ## 5 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 339, 339, 340, 339, 339, 338, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 2666.235 0.7624565 1778.258 ## 2 extratrees 3121.548 0.6977838 2150.264 ## 9 variance 2247.079 0.7892767 1464.055 ## 9 extratrees 2209.335 0.8017228 1455.794 ## 17 variance 2316.893 0.7711552 1499.514 ## 17 extratrees 2172.865 0.8019147 1433.934 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 17, splitrule = extratrees and min.node.size = 5. rangerFit1$finalModel ## Ranger result ## ## Call: ## ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x, mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size, splitrule = as.character(param$splitrule), write.forest = TRUE, probability = classProbs, ...) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 424 ## Number of independent variables: 17 ## Mtry: 17 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: extratrees ## Number of random splits: 1 ## OOB prediction error (MSE): 4236339 ## R squared (OOB): 0.8119659 plot(varImp(rangerFit1), main=&quot;Feature importance\\nranger in caret with default hyper-parameters grid&quot;) # Clean-up rm(x ) 14.7.3 Model with custom grid Use expand.grid function from base Making a custom grid of hyper-parameters requires some knowledge of them … # Getting know the hyper-parameters # https://topepo.github.io/caret/available-models.html -&gt; ranger # getModelInfo(&quot;ranger&quot;) # mtry = 1:5 # number of predictors to pick at random ??? # min.node.size: 1:20 # Self-explanatory # if (is.factor(y)): c(&quot;gini&quot;, &quot;extratrees&quot;) # else: c(&quot;variance&quot;, &quot;extratrees&quot;, &quot;maxstat&quot;) # str(data.df) # Make the custom grid of hyper-parameters rangerGrid &lt;- expand.grid( mtry = seq(1,18,2), splitrule = c(&quot;variance&quot;, &quot;extratrees&quot;, &quot;maxstat&quot;), min.node.size = seq(5,15,2)) nrow(rangerGrid) ## [1] 162 head(rangerGrid) ## mtry splitrule min.node.size ## 1 1 variance 5 ## 2 3 variance 5 ## 3 5 variance 5 ## 4 7 variance 5 ## 5 9 variance 5 ## 6 11 variance 5 tail(rangerGrid) ## mtry splitrule min.node.size ## 157 7 maxstat 15 ## 158 9 maxstat 15 ## 159 11 maxstat 15 ## 160 13 maxstat 15 ## 161 15 maxstat 15 ## 162 17 maxstat 15 # Train with the custom grid of hyper-parameters x &lt;- Sys.time() rangerFit2 &lt;- train(price ~ ., data = train.df, method = &quot;ranger&quot;, trControl = train_ctrl_2, tuneGrid = rangerGrid, importance = &quot;permutation&quot;, verbose = FALSE) Sys.time() - x ## Time difference of 19.29393 mins # Check results rangerFit2 ## Random Forest ## ## 424 samples ## 5 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 338, 339, 340, 339, 340, 339, ... ## Resampling results across tuning parameters: ## ## mtry splitrule min.node.size RMSE Rsquared MAE ## 1 variance 5 3480.708 0.7090287 2472.049 ## 1 variance 7 3485.074 0.7092895 2475.316 ## 1 variance 9 3489.622 0.7078086 2478.340 ## 1 variance 11 3488.937 0.7074563 2478.689 ## 1 variance 13 3495.424 0.7069768 2482.410 ## 1 variance 15 3497.762 0.7063670 2485.192 ## 1 extratrees 5 3930.052 0.6124170 2879.577 ## 1 extratrees 7 3939.023 0.6071972 2887.027 ## 1 extratrees 9 3932.380 0.6104707 2882.411 ## 1 extratrees 11 3942.351 0.6080585 2891.884 ## 1 extratrees 13 3943.907 0.6059577 2890.984 ## 1 extratrees 15 3948.833 0.6026171 2895.591 ## 1 maxstat 5 4361.519 0.5766463 3253.215 ## 1 maxstat 7 4358.142 0.5797896 3250.234 ## 1 maxstat 9 4361.568 0.5780818 3253.265 ## 1 maxstat 11 4368.732 0.5735337 3259.457 ## 1 maxstat 13 4357.273 0.5765763 3248.064 ## 1 maxstat 15 4359.366 0.5677793 3250.118 ## 3 variance 5 2352.078 0.7933851 1563.604 ## 3 variance 7 2370.127 0.7897887 1578.139 ## 3 variance 9 2393.975 0.7861035 1593.235 ## 3 variance 11 2414.148 0.7821976 1604.235 ## 3 variance 13 2435.040 0.7790921 1618.012 ## 3 variance 15 2455.284 0.7766778 1630.401 ## 3 extratrees 5 2656.494 0.7538935 1813.204 ## 3 extratrees 7 2680.668 0.7492453 1827.349 ## 3 extratrees 9 2700.494 0.7460945 1838.420 ## 3 extratrees 11 2730.644 0.7394906 1858.045 ## 3 extratrees 13 2753.124 0.7360851 1873.314 ## 3 extratrees 15 2777.481 0.7312582 1889.330 ## 3 maxstat 5 3460.214 0.6723615 2442.387 ## 3 maxstat 7 3466.623 0.6702104 2447.492 ## 3 maxstat 9 3466.208 0.6688062 2444.038 ## 3 maxstat 11 3475.988 0.6666921 2453.815 ## 3 maxstat 13 3477.919 0.6655827 2451.536 ## 3 maxstat 15 3488.781 0.6597247 2459.926 ## 5 variance 5 2218.117 0.8026400 1468.044 ## 5 variance 7 2240.590 0.7986390 1481.434 ## 5 variance 9 2267.743 0.7939873 1498.788 ## 5 variance 11 2291.741 0.7906138 1517.405 ## 5 variance 13 2316.331 0.7859540 1532.410 ## 5 variance 15 2339.008 0.7827188 1549.234 ## 5 extratrees 5 2312.319 0.7963695 1550.998 ## 5 extratrees 7 2348.701 0.7911407 1571.691 ## 5 extratrees 9 2396.896 0.7826806 1600.222 ## 5 extratrees 11 2432.837 0.7768959 1621.867 ## 5 extratrees 13 2461.349 0.7727988 1641.644 ## 5 extratrees 15 2497.813 0.7669131 1664.536 ## 5 maxstat 5 2800.925 0.7285356 1894.520 ## 5 maxstat 7 2802.588 0.7260980 1892.335 ## 5 maxstat 9 2817.653 0.7219827 1896.834 ## 5 maxstat 11 2822.011 0.7219941 1902.842 ## 5 maxstat 13 2832.474 0.7176074 1906.936 ## 5 maxstat 15 2840.442 0.7165951 1910.929 ## 7 variance 5 2195.422 0.8013149 1447.152 ## 7 variance 7 2220.482 0.7973964 1461.390 ## 7 variance 9 2241.051 0.7938248 1478.572 ## 7 variance 11 2271.667 0.7884257 1497.196 ## 7 variance 13 2286.093 0.7864966 1509.103 ## 7 variance 15 2310.910 0.7822363 1525.065 ## 7 extratrees 5 2186.410 0.8106278 1461.609 ## 7 extratrees 7 2235.131 0.8031455 1489.085 ## 7 extratrees 9 2272.641 0.7983737 1512.364 ## 7 extratrees 11 2311.592 0.7921418 1535.056 ## 7 extratrees 13 2353.660 0.7851490 1560.921 ## 7 extratrees 15 2386.628 0.7804443 1578.947 ## 7 maxstat 5 2517.904 0.7503736 1672.767 ## 7 maxstat 7 2536.425 0.7462856 1684.164 ## 7 maxstat 9 2551.768 0.7426973 1689.678 ## 7 maxstat 11 2564.673 0.7395181 1698.901 ## 7 maxstat 13 2579.951 0.7363549 1706.882 ## 7 maxstat 15 2587.853 0.7346267 1710.734 ## 9 variance 5 2198.506 0.7981117 1443.302 ## 9 variance 7 2217.049 0.7954140 1456.605 ## 9 variance 9 2235.018 0.7922762 1471.817 ## 9 variance 11 2262.145 0.7873965 1488.140 ## 9 variance 13 2281.747 0.7842274 1500.049 ## 9 variance 15 2299.361 0.7810090 1513.504 ## 9 extratrees 5 2135.504 0.8154829 1427.330 ## 9 extratrees 7 2183.622 0.8079939 1454.198 ## 9 extratrees 9 2220.237 0.8028832 1474.301 ## 9 extratrees 11 2251.764 0.7987958 1494.092 ## 9 extratrees 13 2296.372 0.7915954 1517.576 ## 9 extratrees 15 2329.346 0.7861168 1539.122 ## 9 maxstat 5 2435.714 0.7556420 1611.726 ## 9 maxstat 7 2448.661 0.7527975 1614.753 ## 9 maxstat 9 2468.462 0.7485984 1626.206 ## 9 maxstat 11 2481.306 0.7458888 1633.324 ## 9 maxstat 13 2504.445 0.7407622 1645.711 ## 9 maxstat 15 2517.991 0.7382049 1655.127 ## 11 variance 5 2208.628 0.7948292 1448.881 ## 11 variance 7 2227.374 0.7917467 1462.570 ## 11 variance 9 2242.845 0.7890146 1474.912 ## 11 variance 11 2262.336 0.7855255 1487.473 ## 11 variance 13 2281.811 0.7822043 1498.660 ## 11 variance 15 2300.801 0.7786993 1512.806 ## 11 extratrees 5 2121.980 0.8146371 1413.736 ## 11 extratrees 7 2149.499 0.8109904 1433.529 ## 11 extratrees 9 2199.405 0.8036176 1460.098 ## 11 extratrees 11 2234.409 0.7984726 1480.336 ## 11 extratrees 13 2268.096 0.7934579 1498.733 ## 11 extratrees 15 2294.026 0.7897046 1515.105 ## 11 maxstat 5 2408.593 0.7569455 1590.234 ## 11 maxstat 7 2422.702 0.7540510 1597.424 ## 11 maxstat 9 2443.785 0.7496759 1607.299 ## 11 maxstat 11 2462.165 0.7453431 1617.305 ## 11 maxstat 13 2478.143 0.7425341 1628.410 ## 11 maxstat 15 2495.729 0.7389388 1638.798 ## 13 variance 5 2224.776 0.7904001 1460.655 ## 13 variance 7 2236.880 0.7886909 1468.034 ## 13 variance 9 2255.269 0.7856946 1478.074 ## 13 variance 11 2269.926 0.7825451 1490.774 ## 13 variance 13 2290.953 0.7788466 1504.965 ## 13 variance 15 2303.106 0.7767364 1513.477 ## 13 extratrees 5 2108.888 0.8150672 1406.900 ## 13 extratrees 7 2143.699 0.8104801 1429.810 ## 13 extratrees 9 2187.200 0.8038312 1449.629 ## 13 extratrees 11 2213.848 0.8002418 1467.741 ## 13 extratrees 13 2249.545 0.7947662 1486.598 ## 13 extratrees 15 2272.520 0.7914293 1502.166 ## 13 maxstat 5 2409.709 0.7547325 1591.273 ## 13 maxstat 7 2421.683 0.7522924 1596.497 ## 13 maxstat 9 2443.232 0.7478132 1606.913 ## 13 maxstat 11 2455.971 0.7451920 1615.406 ## 13 maxstat 13 2474.318 0.7412053 1626.891 ## 13 maxstat 15 2490.213 0.7379786 1637.103 ## 15 variance 5 2245.458 0.7857367 1468.871 ## 15 variance 7 2257.614 0.7838378 1476.913 ## 15 variance 9 2268.819 0.7818964 1486.727 ## 15 variance 11 2290.682 0.7777409 1499.959 ## 15 variance 13 2303.803 0.7754090 1508.128 ## 15 variance 15 2314.439 0.7737001 1516.707 ## 15 extratrees 5 2109.192 0.8141949 1403.830 ## 15 extratrees 7 2140.721 0.8097701 1424.623 ## 15 extratrees 9 2171.226 0.8054118 1443.804 ## 15 extratrees 11 2205.492 0.8001438 1463.391 ## 15 extratrees 13 2233.059 0.7962069 1476.409 ## 15 extratrees 15 2261.578 0.7918521 1494.142 ## 15 maxstat 5 2413.008 0.7527492 1596.522 ## 15 maxstat 7 2431.269 0.7487393 1605.224 ## 15 maxstat 9 2444.403 0.7461677 1611.300 ## 15 maxstat 11 2459.750 0.7428750 1621.435 ## 15 maxstat 13 2483.502 0.7382048 1633.102 ## 15 maxstat 15 2503.034 0.7341556 1646.722 ## 17 variance 5 2271.064 0.7804503 1481.357 ## 17 variance 7 2279.668 0.7791901 1487.993 ## 17 variance 9 2291.893 0.7769432 1498.494 ## 17 variance 11 2309.112 0.7734374 1510.798 ## 17 variance 13 2326.162 0.7705538 1520.142 ## 17 variance 15 2334.920 0.7690559 1526.685 ## 17 extratrees 5 2099.762 0.8146892 1399.288 ## 17 extratrees 7 2133.956 0.8099013 1421.661 ## 17 extratrees 9 2164.972 0.8049848 1441.025 ## 17 extratrees 11 2197.491 0.8004547 1457.724 ## 17 extratrees 13 2228.198 0.7953733 1475.359 ## 17 extratrees 15 2245.420 0.7932948 1487.364 ## 17 maxstat 5 2428.483 0.7484978 1610.536 ## 17 maxstat 7 2440.865 0.7458872 1617.972 ## 17 maxstat 9 2461.315 0.7413833 1627.144 ## 17 maxstat 11 2476.466 0.7383731 1634.856 ## 17 maxstat 13 2489.963 0.7355372 1644.713 ## 17 maxstat 15 2513.213 0.7306427 1659.341 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 17, splitrule = extratrees and min.node.size = 5. rangerFit2$finalModel ## Ranger result ## ## Call: ## ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x, mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size, splitrule = as.character(param$splitrule), write.forest = TRUE, probability = classProbs, ...) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 424 ## Number of independent variables: 17 ## Mtry: 17 ## Target node size: 5 ## Variable importance mode: permutation ## Splitrule: extratrees ## Number of random splits: 1 ## OOB prediction error (MSE): 4352161 ## R squared (OOB): 0.8068251 plot(varImp(rangerFit1), main=&quot;Feature importance\\nranger in caret with custom hyper-parameters grid&quot;) # De-clutter rm(x,train_ctrl_1,train_ctrl_2) 14.8 Evaluate models on the test set 14.8.1 Caret’s default hyper-parameters grid test_pred_rangerFit1 &lt;- predict(rangerFit1, test.df) RMSE_test_rangerFit1 &lt;- RMSE(test_pred_rangerFit1, test.df$price) Rsq_test_rangerFit1 &lt;- cor(test_pred_rangerFit1, test.df$price)^2 RMSE_test_rangerFit1 ## [1] 1878.018 Rsq_test_rangerFit1 ## [1] 0.8267155 plot(test.df$price, test_pred_rangerFit1, main=&quot;Performance of ranger in caret model on test set\\ndefault hyper-parameters grid&quot;) abline(0,1,lwd=3, col=&quot;red&quot;) abline(lm(test_pred_rangerFit1~test.df$price), lty=2, col=&quot;red&quot;) legend(&quot;topleft&quot;, legend=c(&quot;lm fitted to ranger prediction&quot;,&quot;diagonal&quot;), lwd=c(1,3),lty=c(2,1), col=&quot;red&quot;,bty=&quot;n&quot;) legend_text &lt;- paste( &quot;R sq: &quot;,round(Rsq_test_rangerFit1,2),&quot;\\n&quot;, &quot;RMSE: &quot;,round(RMSE_test_rangerFit1,0),sep=&quot;&quot;) legend(&quot;bottomright&quot;, legend=legend_text, bty=&quot;n&quot;) # Clean-up rm(legend_text,Rsq_test_rangerFit1,RMSE_test_rangerFit1,test_pred_rangerFit1) 14.8.2 Custom hyper-parameters grid test_pred_rangerFit2 &lt;- predict(rangerFit2, test.df) RMSE_test_rangerFit2 &lt;- RMSE(test_pred_rangerFit2, test.df$price) Rsq_test_rangerFit2 &lt;- cor(test_pred_rangerFit2, test.df$price)^2 RMSE_test_rangerFit2 ## [1] 1914.545 Rsq_test_rangerFit2 ## [1] 0.8201145 plot(test.df$price, test_pred_rangerFit2, main=&quot;Performance of ranger in caret model on test set\\ncustom hyper-parameters grid&quot;) abline(0,1,lwd=3, col=&quot;red&quot;) abline(lm(test_pred_rangerFit2~test.df$price), lty=2, col=&quot;red&quot;) legend(&quot;topleft&quot;, legend=c(&quot;lm fitted to ranger prediction&quot;,&quot;diagonal&quot;), lwd=c(1,3),lty=c(2,1), col=&quot;red&quot;,bty=&quot;n&quot;) legend_text &lt;- paste( &quot;R sq: &quot;,round(Rsq_test_rangerFit2,2),&quot;\\n&quot;, &quot;RMSE: &quot;,round(RMSE_test_rangerFit2,0),sep=&quot;&quot;) legend(&quot;bottomright&quot;, legend=legend_text, bty=&quot;n&quot;) # Clean-up rm(legend_text,Rsq_test_rangerFit2,RMSE_test_rangerFit2,test_pred_rangerFit2) 14.9 Manual selection of the best model We use the finalModel suggested by caret, but there is a way of selecting it manually using different criteria … ?tolerance - is it sorted from the least to the most complex model ? 14.9.0.1 Caret’s default hyper-parameter grid RMSE_tol2pct_rangerFit1 &lt;- tolerance(rangerFit1$results, metric = &quot;RMSE&quot;, tol = 2, maximize = TRUE) rangerFit1$results[RMSE_tol2pct_rangerFit1,] ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 2 2 5 extratrees 3121.548 0.6977838 2150.264 524.667 0.05123444 169.8643 Rsq_tol2pct_rangerFit1 &lt;- tolerance(rangerFit1$results, metric = &quot;Rsquared&quot;, tol = 2, maximize = TRUE) rangerFit1$results[Rsq_tol2pct_rangerFit1,] ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 3 9 5 variance 2247.079 0.7892767 1464.055 546.1479 0.05460494 184.0897 # Clean-up rm(RMSE_tol2pct_rangerFit1,Rsq_tol2pct_rangerFit1) 14.9.0.2 Custom hyper-parameter grid rangerFit2$results ## mtry splitrule min.node.size RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 1 variance 5 3480.708 0.7090287 2472.049 653.8809 0.05739400 251.8085 ## 2 1 variance 7 3485.074 0.7092895 2475.316 642.7905 0.05646284 246.8308 ## 3 1 variance 9 3489.622 0.7078086 2478.340 651.9453 0.05813802 248.8386 ## 4 1 variance 11 3488.937 0.7074563 2478.689 647.4680 0.05753141 246.4589 ## 5 1 variance 13 3495.424 0.7069768 2482.410 648.0846 0.05784794 245.9483 ## 6 1 variance 15 3497.762 0.7063670 2485.192 646.5400 0.05790954 247.0167 ## 7 1 extratrees 5 3930.052 0.6124170 2879.577 628.2805 0.07380893 243.6790 ## 8 1 extratrees 7 3939.023 0.6071972 2887.027 635.0366 0.06780230 247.2786 ## 9 1 extratrees 9 3932.380 0.6104707 2882.411 637.2757 0.06910053 248.6003 ## 10 1 extratrees 11 3942.351 0.6080585 2891.884 635.7954 0.07049508 249.0843 ## 11 1 extratrees 13 3943.907 0.6059577 2890.984 634.9859 0.06656599 245.5603 ## 12 1 extratrees 15 3948.833 0.6026171 2895.591 635.3994 0.06749773 245.1560 ## 13 1 maxstat 5 4361.519 0.5766463 3253.215 654.6685 0.06860905 246.9378 ## 14 1 maxstat 7 4358.142 0.5797896 3250.234 657.6254 0.07212058 251.5503 ## 15 1 maxstat 9 4361.568 0.5780818 3253.265 653.4601 0.07150346 245.4741 ## 16 1 maxstat 11 4368.732 0.5735337 3259.457 651.3930 0.07221044 243.2442 ## 17 1 maxstat 13 4357.273 0.5765763 3248.064 642.6443 0.06957168 235.0368 ## 18 1 maxstat 15 4359.366 0.5677793 3250.118 651.9950 0.07240528 242.3737 ## 19 3 variance 5 2352.078 0.7933851 1563.604 559.0479 0.04944228 194.9500 ## 20 3 variance 7 2370.127 0.7897887 1578.139 568.0398 0.05084529 199.8765 ## 21 3 variance 9 2393.975 0.7861035 1593.235 575.7855 0.05084433 204.9602 ## 22 3 variance 11 2414.148 0.7821976 1604.235 584.8731 0.05115010 207.0534 ## 23 3 variance 13 2435.040 0.7790921 1618.012 577.9749 0.05111998 205.5794 ## 24 3 variance 15 2455.284 0.7766778 1630.401 582.0422 0.05211189 204.1361 ## 25 3 extratrees 5 2656.494 0.7538935 1813.204 561.3127 0.05085592 211.5217 ## 26 3 extratrees 7 2680.668 0.7492453 1827.349 570.5052 0.05134098 215.5524 ## 27 3 extratrees 9 2700.494 0.7460945 1838.420 577.8251 0.05214093 219.0072 ## 28 3 extratrees 11 2730.644 0.7394906 1858.045 575.2133 0.05201573 215.9733 ## 29 3 extratrees 13 2753.124 0.7360851 1873.314 586.4585 0.05443066 225.9861 ## 30 3 extratrees 15 2777.481 0.7312582 1889.330 585.4652 0.05436938 223.0845 ## 31 3 maxstat 5 3460.214 0.6723615 2442.387 673.7528 0.06515483 247.0978 ## 32 3 maxstat 7 3466.623 0.6702104 2447.492 679.7960 0.06731181 247.6231 ## 33 3 maxstat 9 3466.208 0.6688062 2444.038 673.6115 0.06794990 240.0847 ## 34 3 maxstat 11 3475.988 0.6666921 2453.815 682.3185 0.06920364 245.3531 ## 35 3 maxstat 13 3477.919 0.6655827 2451.536 682.2451 0.06834995 244.8525 ## 36 3 maxstat 15 3488.781 0.6597247 2459.926 687.8721 0.07109304 247.4998 ## 37 5 variance 5 2218.117 0.8026400 1468.044 528.2924 0.04770455 182.7764 ## 38 5 variance 7 2240.590 0.7986390 1481.434 530.8554 0.04784610 182.7778 ## 39 5 variance 9 2267.743 0.7939873 1498.788 546.1234 0.04953236 187.5525 ## 40 5 variance 11 2291.741 0.7906138 1517.405 549.5934 0.05016140 189.2706 ## 41 5 variance 13 2316.331 0.7859540 1532.410 555.5916 0.05036941 192.5030 ## 42 5 variance 15 2339.008 0.7827188 1549.234 559.1909 0.05043419 193.4935 ## 43 5 extratrees 5 2312.319 0.7963695 1550.998 517.4454 0.04645457 189.2701 ## 44 5 extratrees 7 2348.701 0.7911407 1571.691 531.5292 0.04583267 196.2668 ## 45 5 extratrees 9 2396.896 0.7826806 1600.222 545.4848 0.04876185 199.3240 ## 46 5 extratrees 11 2432.837 0.7768959 1621.867 552.9158 0.04764272 198.9975 ## 47 5 extratrees 13 2461.349 0.7727988 1641.644 556.7975 0.04919700 204.5521 ## 48 5 extratrees 15 2497.813 0.7669131 1664.536 562.3792 0.04833071 206.0860 ## 49 5 maxstat 5 2800.925 0.7285356 1894.520 651.4455 0.06009226 227.9112 ## 50 5 maxstat 7 2802.588 0.7260980 1892.335 654.8942 0.06228871 230.0327 ## 51 5 maxstat 9 2817.653 0.7219827 1896.834 666.3828 0.06398817 235.6859 ## 52 5 maxstat 11 2822.011 0.7219941 1902.842 653.1908 0.06237274 226.7886 ## 53 5 maxstat 13 2832.474 0.7176074 1906.936 662.8699 0.06407045 229.8546 ## 54 5 maxstat 15 2840.442 0.7165951 1910.929 665.3258 0.06506307 230.7233 ## 55 7 variance 5 2195.422 0.8013149 1447.152 508.8013 0.04719530 178.3215 ## 56 7 variance 7 2220.482 0.7973964 1461.390 514.1453 0.04695249 177.8916 ## 57 7 variance 9 2241.051 0.7938248 1478.572 515.3166 0.04662153 179.9194 ## 58 7 variance 11 2271.667 0.7884257 1497.196 529.4821 0.04792323 185.0141 ## 59 7 variance 13 2286.093 0.7864966 1509.103 531.1973 0.04811141 185.9506 ## 60 7 variance 15 2310.910 0.7822363 1525.065 533.2558 0.04906881 184.4948 ## 61 7 extratrees 5 2186.410 0.8106278 1461.609 484.5753 0.04297896 172.9852 ## 62 7 extratrees 7 2235.131 0.8031455 1489.085 502.0878 0.04356946 178.3344 ## 63 7 extratrees 9 2272.641 0.7983737 1512.364 513.6440 0.04438049 183.5718 ## 64 7 extratrees 11 2311.592 0.7921418 1535.056 531.6477 0.04621409 191.2128 ## 65 7 extratrees 13 2353.660 0.7851490 1560.921 542.3451 0.04800692 193.3412 ## 66 7 extratrees 15 2386.628 0.7804443 1578.947 549.6777 0.04671849 196.4946 ## 67 7 maxstat 5 2517.904 0.7503736 1672.767 600.4154 0.05637362 203.9183 ## 68 7 maxstat 7 2536.425 0.7462856 1684.164 612.1842 0.05895742 205.9984 ## 69 7 maxstat 9 2551.768 0.7426973 1689.678 619.1666 0.05826909 208.1121 ## 70 7 maxstat 11 2564.673 0.7395181 1698.901 623.7440 0.06120910 206.5662 ## 71 7 maxstat 13 2579.951 0.7363549 1706.882 622.7693 0.06034849 206.8083 ## 72 7 maxstat 15 2587.853 0.7346267 1710.734 626.2172 0.06168224 209.5004 ## 73 9 variance 5 2198.506 0.7981117 1443.302 500.8555 0.04688194 176.7930 ## 74 9 variance 7 2217.049 0.7954140 1456.605 500.9089 0.04590373 176.1508 ## 75 9 variance 9 2235.018 0.7922762 1471.817 504.0781 0.04640002 176.7751 ## 76 9 variance 11 2262.145 0.7873965 1488.140 513.7616 0.04771615 181.1194 ## 77 9 variance 13 2281.747 0.7842274 1500.049 522.7910 0.04886353 182.9050 ## 78 9 variance 15 2299.361 0.7810090 1513.504 527.2283 0.05003504 186.6265 ## 79 9 extratrees 5 2135.504 0.8154829 1427.330 470.2106 0.04334879 167.3108 ## 80 9 extratrees 7 2183.622 0.8079939 1454.198 490.0744 0.04423407 173.4569 ## 81 9 extratrees 9 2220.237 0.8028832 1474.301 506.0734 0.04494859 178.2459 ## 82 9 extratrees 11 2251.764 0.7987958 1494.092 512.7106 0.04455416 182.7378 ## 83 9 extratrees 13 2296.372 0.7915954 1517.576 524.6530 0.04605976 186.3926 ## 84 9 extratrees 15 2329.346 0.7861168 1539.122 530.2303 0.04687606 187.7759 ## 85 9 maxstat 5 2435.714 0.7556420 1611.726 570.8161 0.05552823 193.9666 ## 86 9 maxstat 7 2448.661 0.7527975 1614.753 582.4167 0.05704015 196.2106 ## 87 9 maxstat 9 2468.462 0.7485984 1626.206 588.6516 0.05843964 198.2046 ## 88 9 maxstat 11 2481.306 0.7458888 1633.324 591.9210 0.05918793 200.4703 ## 89 9 maxstat 13 2504.445 0.7407622 1645.711 601.9711 0.06058538 202.2753 ## 90 9 maxstat 15 2517.991 0.7382049 1655.127 603.7236 0.06024504 203.1684 ## 91 11 variance 5 2208.628 0.7948292 1448.881 495.6737 0.04750471 177.0882 ## 92 11 variance 7 2227.374 0.7917467 1462.570 496.2797 0.04691160 178.8564 ## 93 11 variance 9 2242.845 0.7890146 1474.912 502.2390 0.04772099 181.0526 ## 94 11 variance 11 2262.336 0.7855255 1487.473 508.4716 0.04804097 180.2007 ## 95 11 variance 13 2281.811 0.7822043 1498.660 505.3399 0.04812315 182.0668 ## 96 11 variance 15 2300.801 0.7786993 1512.806 505.1617 0.04836799 179.9858 ## 97 11 extratrees 5 2121.980 0.8146371 1413.736 475.6018 0.04499907 167.4452 ## 98 11 extratrees 7 2149.499 0.8109904 1433.529 475.2556 0.04403727 167.9377 ## 99 11 extratrees 9 2199.405 0.8036176 1460.098 492.5168 0.04384786 172.6403 ## 100 11 extratrees 11 2234.409 0.7984726 1480.336 506.0456 0.04585271 180.0340 ## 101 11 extratrees 13 2268.096 0.7934579 1498.733 516.2363 0.04546465 178.6396 ## 102 11 extratrees 15 2294.026 0.7897046 1515.105 524.0581 0.04756092 183.3612 ## 103 11 maxstat 5 2408.593 0.7569455 1590.234 560.5663 0.05533539 194.7188 ## 104 11 maxstat 7 2422.702 0.7540510 1597.424 567.9169 0.05675566 197.3623 ## 105 11 maxstat 9 2443.785 0.7496759 1607.299 571.2541 0.05781119 196.2274 ## 106 11 maxstat 11 2462.165 0.7453431 1617.305 577.5012 0.05850388 197.1825 ## 107 11 maxstat 13 2478.143 0.7425341 1628.410 586.0297 0.05959070 198.7964 ## 108 11 maxstat 15 2495.729 0.7389388 1638.798 588.7539 0.06042413 202.7002 ## 109 13 variance 5 2224.776 0.7904001 1460.655 481.0254 0.04828689 174.8134 ## 110 13 variance 7 2236.880 0.7886909 1468.034 481.2127 0.04658402 173.8269 ## 111 13 variance 9 2255.269 0.7856946 1478.074 487.9459 0.04638150 177.4697 ## 112 13 variance 11 2269.926 0.7825451 1490.774 492.0347 0.04819167 178.1355 ## 113 13 variance 13 2290.953 0.7788466 1504.965 494.0271 0.04800836 179.2905 ## 114 13 variance 15 2303.106 0.7767364 1513.477 499.6572 0.04914277 181.8960 ## 115 13 extratrees 5 2108.888 0.8150672 1406.900 460.8956 0.04501725 161.5215 ## 116 13 extratrees 7 2143.699 0.8104801 1429.810 476.2635 0.04561886 168.4591 ## 117 13 extratrees 9 2187.200 0.8038312 1449.629 486.7118 0.04439793 167.7202 ## 118 13 extratrees 11 2213.848 0.8002418 1467.741 490.9791 0.04431379 171.4975 ## 119 13 extratrees 13 2249.545 0.7947662 1486.598 506.6294 0.04598211 175.4891 ## 120 13 extratrees 15 2272.520 0.7914293 1502.166 513.0152 0.04518420 181.1301 ## 121 13 maxstat 5 2409.709 0.7547325 1591.273 555.4300 0.05631515 196.3642 ## 122 13 maxstat 7 2421.683 0.7522924 1596.497 556.0821 0.05712486 195.3323 ## 123 13 maxstat 9 2443.232 0.7478132 1606.913 563.5372 0.05619844 196.4202 ## 124 13 maxstat 11 2455.971 0.7451920 1615.406 567.7536 0.05785194 197.7244 ## 125 13 maxstat 13 2474.318 0.7412053 1626.891 578.0401 0.05939632 201.3558 ## 126 13 maxstat 15 2490.213 0.7379786 1637.103 582.6910 0.06067540 201.3697 ## 127 15 variance 5 2245.458 0.7857367 1468.871 476.0919 0.04819886 175.8929 ## 128 15 variance 7 2257.614 0.7838378 1476.913 470.7052 0.04630882 173.0441 ## 129 15 variance 9 2268.819 0.7818964 1486.727 476.6829 0.04741934 174.7385 ## 130 15 variance 11 2290.682 0.7777409 1499.959 481.8747 0.04649771 177.0355 ## 131 15 variance 13 2303.803 0.7754090 1508.128 484.9908 0.04855337 179.3236 ## 132 15 variance 15 2314.439 0.7737001 1516.707 487.8661 0.04869217 180.2305 ## 133 15 extratrees 5 2109.192 0.8141949 1403.830 464.4881 0.04351751 160.3821 ## 134 15 extratrees 7 2140.721 0.8097701 1424.623 475.1010 0.04491077 163.4446 ## 135 15 extratrees 9 2171.226 0.8054118 1443.804 482.2199 0.04456001 168.2173 ## 136 15 extratrees 11 2205.492 0.8001438 1463.391 491.5246 0.04569526 172.6320 ## 137 15 extratrees 13 2233.059 0.7962069 1476.409 506.4821 0.04678655 175.4366 ## 138 15 extratrees 15 2261.578 0.7918521 1494.142 509.9930 0.04662749 176.3383 ## 139 15 maxstat 5 2413.008 0.7527492 1596.522 547.0849 0.05650205 196.6912 ## 140 15 maxstat 7 2431.269 0.7487393 1605.224 553.8724 0.05755344 196.4431 ## 141 15 maxstat 9 2444.403 0.7461677 1611.300 560.0403 0.05797248 198.6144 ## 142 15 maxstat 11 2459.750 0.7428750 1621.435 568.4793 0.05904108 200.3133 ## 143 15 maxstat 13 2483.502 0.7382048 1633.102 572.4336 0.05974318 202.1199 ## 144 15 maxstat 15 2503.034 0.7341556 1646.722 579.0257 0.06058044 202.6827 ## 145 17 variance 5 2271.064 0.7804503 1481.357 460.6714 0.04679949 172.1437 ## 146 17 variance 7 2279.668 0.7791901 1487.993 460.2556 0.04607237 171.2131 ## 147 17 variance 9 2291.893 0.7769432 1498.494 463.9629 0.04584838 173.3495 ## 148 17 variance 11 2309.112 0.7734374 1510.798 468.6667 0.04614428 177.4674 ## 149 17 variance 13 2326.162 0.7705538 1520.142 475.9210 0.04764976 179.4543 ## 150 17 variance 15 2334.920 0.7690559 1526.685 478.6440 0.04813462 181.1187 ## 151 17 extratrees 5 2099.762 0.8146892 1399.288 458.1752 0.04468388 163.6685 ## 152 17 extratrees 7 2133.956 0.8099013 1421.661 463.7166 0.04322355 160.6645 ## 153 17 extratrees 9 2164.972 0.8049848 1441.025 471.2761 0.04543728 164.8484 ## 154 17 extratrees 11 2197.491 0.8004547 1457.724 488.6085 0.04542805 169.6087 ## 155 17 extratrees 13 2228.198 0.7953733 1475.359 497.2423 0.04690350 174.4546 ## 156 17 extratrees 15 2245.420 0.7932948 1487.364 500.8902 0.04681644 175.1624 ## 157 17 maxstat 5 2428.483 0.7484978 1610.536 539.9379 0.05751022 196.5421 ## 158 17 maxstat 7 2440.865 0.7458872 1617.972 548.3167 0.05721811 198.5786 ## 159 17 maxstat 9 2461.315 0.7413833 1627.144 555.1827 0.05888327 201.2240 ## 160 17 maxstat 11 2476.466 0.7383731 1634.856 565.5207 0.06037233 203.5473 ## 161 17 maxstat 13 2489.963 0.7355372 1644.713 568.5059 0.06029050 206.1184 ## 162 17 maxstat 15 2513.213 0.7306427 1659.341 576.4425 0.06253614 208.6268 RMSE_tol2pct_rangerFit2 &lt;- tolerance(rangerFit2$results, metric = &quot;RMSE&quot;, tol = 2, maximize = TRUE) rangerFit2$results[ RMSE_tol2pct_rangerFit2,] ## mtry splitrule min.node.size RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 13 1 maxstat 5 4361.519 0.5766463 3253.215 654.6685 0.06860905 246.9378 Rsq_tol2pct_rangerFit2 &lt;- tolerance(rangerFit2$results, metric = &quot;Rsquared&quot;, tol = 2, maximize = TRUE) rangerFit2$results[Rsq_tol2pct_rangerFit2,] ## mtry splitrule min.node.size RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 37 5 variance 5 2218.117 0.80264 1468.044 528.2924 0.04770455 182.7764 # Clean-up rm(RMSE_tol2pct_rangerFit2,Rsq_tol2pct_rangerFit2) 14.10 Final section ls ## function (name, pos = -1L, envir = as.environment(pos), all.names = FALSE, ## pattern, sorted = TRUE) ## { ## if (!missing(name)) { ## pos &lt;- tryCatch(name, error = function(e) e) ## if (inherits(pos, &quot;error&quot;)) { ## name &lt;- substitute(name) ## if (!is.character(name)) ## name &lt;- deparse(name) ## warning(gettextf(&quot;%s converted to character string&quot;, ## sQuote(name)), domain = NA) ## pos &lt;- name ## } ## } ## all.names &lt;- .Internal(ls(envir, all.names, sorted)) ## if (!missing(pattern)) { ## if ((ll &lt;- length(grep(&quot;[&quot;, pattern, fixed = TRUE))) &amp;&amp; ## ll != length(grep(&quot;]&quot;, pattern, fixed = TRUE))) { ## if (pattern == &quot;[&quot;) { ## pattern &lt;- &quot;\\\\[&quot; ## warning(&quot;replaced regular expression pattern &#39;[&#39; by &#39;\\\\\\\\[&#39;&quot;) ## } ## else if (length(grep(&quot;[^\\\\\\\\]\\\\[&lt;-&quot;, pattern))) { ## pattern &lt;- sub(&quot;\\\\[&lt;-&quot;, &quot;\\\\\\\\\\\\[&lt;-&quot;, pattern) ## warning(&quot;replaced &#39;[&lt;-&#39; by &#39;\\\\\\\\[&lt;-&#39; in regular expression pattern&quot;) ## } ## } ## grep(pattern, all.names, value = TRUE) ## } ## else all.names ## } ## &lt;bytecode: 0x7f8ebbb3eb60&gt; ## &lt;environment: namespace:base&gt; sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ranger_0.12.1 corrplot_0.84 doMC_1.3.7 iterators_1.0.13 foreach_1.5.1 AmesHousing_0.0.4 caret_6.0-86 lattice_0.20-41 ipred_0.9-11 rpart.plot_3.0.9 rpart_4.1-15 rsample_0.0.9 dplyr_1.0.4 plotly_4.9.3 ggplot2_3.3.3 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 tidyr_1.1.2 jsonlite_1.7.2 viridisLite_0.3.0 splines_3.6.2 prodlim_2019.11.13 assertthat_0.2.1 stats4_3.6.2 highr_0.8 yaml_2.2.1 globals_0.14.0 pillar_1.4.7 glue_1.4.2 reticulate_1.18 pROC_1.17.0.1 digest_0.6.27 colorspace_2.0-0 recipes_0.1.15 htmltools_0.5.1.1 Matrix_1.3-2 plyr_1.8.6 timeDate_3043.102 pkgconfig_2.0.3 listenv_0.8.0 bookdown_0.21 purrr_0.3.4 scales_1.1.1 gower_0.2.2 lava_1.6.9 tibble_3.0.6 mgcv_1.8-33 generics_0.1.0 farver_2.0.3 ellipsis_0.3.1 withr_2.4.1 furrr_0.2.2 nnet_7.3-15 lazyeval_0.2.2 cli_2.3.0 survival_3.2-7 magrittr_2.0.1 crayon_1.4.1 evaluate_0.14 future_1.21.0 fansi_0.4.2 parallelly_1.23.0 nlme_3.1-152 ## [48] MASS_7.3-53.1 class_7.3-18 tools_3.6.2 data.table_1.13.6 lifecycle_1.0.0 stringr_1.4.0 munsell_0.5.0 e1071_1.7-4 compiler_3.6.2 rlang_0.4.10 grid_3.6.2 rstudioapi_0.13 htmlwidgets_1.5.3 crosstalk_1.1.1 labeling_0.4.2 rmarkdown_2.6 ModelMetrics_1.2.2.2 gtable_0.3.0 codetools_0.2-18 DBI_1.1.1 reshape2_1.4.4 R6_2.5.0 lubridate_1.7.9.2 knitr_1.31 utf8_1.1.4 stringi_1.5.3 Rcpp_1.0.6 vctrs_0.3.6 tidyselect_1.1.0 xfun_0.21 Sys.time() ## [1] &quot;2021-04-21 14:19:01 BST&quot; gc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 2838379 151.6 4344478 232.1 NA 4344478 232.1 ## Vcells 6347678 48.5 14815979 113.1 16384 14815979 113.1 "]]
